{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dc125f8a",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/iu5git/Deep-learning/blob/main/notebooks/Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "\n",
        "\n",
        "# \u0627\u0644\u0639\u0645\u0644 \u0627\u0644\u0645\u062e\u0628\u0631\u064a \u0631\u0642\u0645 1\n",
        "\n",
        "\n",
        "\n",
        "## \u0627\u0644\u0645\u0647\u0645\u0629\n",
        "\n",
        "\n",
        "\n",
        "\u064a\u062c\u0628 \u0627\u0644\u062a\u0639\u0631\u0641 \u0639\u0644\u0649 \u0625\u0637\u0627\u0631 \u0639\u0645\u0644 \u0627\u0644\u062a\u0639\u0644\u0645 \u0627\u0644\u0622\u0644\u064a PyTorch \u0648\u062a\u0646\u0641\u064a\u0630 \u062b\u0644\u0627\u062b \u0645\u0647\u0627\u0645:\n",
        "\n",
        "1. \u0627\u0644\u0627\u0646\u062d\u062f\u0627\u0631 \u0648\u0641\u0642\u064b\u0627 \u0644\u0646\u0638\u0631\u064a\u0629 \u0627\u0644\u062a\u0642\u0631\u064a\u0628 \u0627\u0644\u0634\u0627\u0645\u0644\u060c \u0627\u0644\u062a\u0641\u0627\u0636\u0644 \u0627\u0644\u064a\u062f\u0648\u064a\n",
        "\n",
        "2. \u0627\u0644\u062a\u0635\u0646\u064a\u0641 \u0627\u0644\u062b\u0646\u0627\u0626\u064a \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0627\u0644\u062a\u0641\u0627\u0636\u0644 \u0627\u0644\u062a\u0644\u0642\u0627\u0626\u064a \u0641\u064a PyTorch\n",
        "\n",
        "3. \u062a\u062f\u0631\u064a\u0628 \u0634\u0628\u0643\u0629 \u0639\u0635\u0628\u064a\u0629 \u0643\u0627\u0645\u0644\u0629 \u0627\u0644\u0627\u062a\u0635\u0627\u0644 \u0644\u062a\u0635\u0646\u064a\u0641 3 \u0641\u0626\u0627\u062a \u0645\u0646 \u0627\u0644\u0635\u0648\u0631 \u0645\u0646 \u0645\u062c\u0645\u0648\u0639\u0629 \u0628\u064a\u0627\u0646\u0627\u062a CIFAR100 \u0648\u0641\u0642\u064b\u0627 \u0644\u0644\u0645\u062b\u0627\u0644 \u0627\u0644\u0645\u062d\u062f\u062f \u062b\u0645 \u062a\u062d\u0633\u064a\u0646 \u0627\u0644\u062f\u0642\u0629 \u0639\u0644\u0649 \u0645\u062c\u0645\u0648\u0639\u0629 \u0627\u0644\u0627\u062e\u062a\u0628\u0627\u0631.\n",
        "\n",
        "\n",
        "\n",
        "\u0644\u0644\u0645\u0647\u0645\u0629 3 \u064a\u062c\u0628 \u062a\u0634\u0643\u064a\u0644 \u0645\u062c\u0645\u0648\u0639\u0629 \u0641\u0631\u0639\u064a\u0629 \u062e\u0627\u0635\u0629 \u0628\u0643 \u0648\u0641\u0642\u064b\u0627 \u0644\u0644\u062e\u064a\u0627\u0631 \u0627\u0644\u0645\u062d\u062f\u062f \u0645\u0646 CIFAR100 \u0648\u0641\u0642\u064b\u0627 \u0644\u0644\u062e\u064a\u0627\u0631 \u0627\u0644\u0645\u062d\u062f\u062f. \n",
        "\n",
        "\n",
        "\n",
        "\u064a\u062a\u0645 \u062a\u0646\u0641\u064a\u0630 \u0627\u0644\u0645\u062e\u062a\u0628\u0631\u0627\u062a \u0639\u0644\u0649 \u0645\u0646\u0635\u0629 Google Colab \u0645\u0627 \u0639\u0644\u064a\u0643 \u0633\u0648\u0649 \u0627\u0644\u0627\u0646\u062a\u0642\u0627\u0644 \u0625\u0644\u0649 \u0627\u0644\u0631\u0627\u0628\u0637 \u0641\u064a \u0628\u062f\u0627\u064a\u0629 \u0627\u0644 Notebook . \u0643\u0645\u0627 \u064a\u0645\u0643\u0646 \u0627\u0644\u0639\u0645\u0644 \u0645\u0639 Notebook \u0627\u0644\u0645\u062e\u062a\u0628\u0631 \u0645\u062d\u0644\u064a\u064b\u0627 .\n",
        "\n",
        "\n",
        "\n",
        "### \u064a\u062c\u0628 \u0623\u0646 \u064a\u062d\u062a\u0648\u064a \u0627\u0644\u062a\u0642\u0631\u064a\u0631 \u0639\u0644\u0649:\n",
        "\n",
        "- \u0635\u0641\u062d\u0629 \u0627\u0644\u0639\u0646\u0648\u0627\u0646\n",
        "\n",
        "- \u0627\u0644\u0645\u0647\u0645\u0629 \u0645\u0639 \u0627\u0644\u062e\u064a\u0627\u0631 \u0627\u0644\u0645\u062d\u062f\u062f\n",
        "\n",
        "- \u0644\u0642\u0637\u0627\u062a \u0634\u0627\u0634\u0629 \u0648\u062a\u0641\u0633\u064a\u0631\u0627\u062a \u0642\u0635\u064a\u0631\u0629 \u0644\u0643\u0644 \u0645\u0631\u062d\u0644\u0629 \u0645\u0646 \u0645\u0631\u0627\u062d\u0644 \u0627\u0644\u0639\u0645\u0644 \u0627\u0644\u0645\u062e\u0628\u0631\u064a\n",
        "\n",
        "- \u062c\u062f\u0648\u0644 \u0646\u0647\u0627\u0626\u064a \u0628\u0627\u0644\u0646\u062a\u0627\u0626\u062c \u0644\u062c\u0645\u064a\u0639 \u062e\u064a\u0627\u0631\u0627\u062a \u0627\u0644\u062a\u062f\u0631\u064a\u0628\n",
        "\n",
        "\n",
        "\n",
        "# \u0627\u0644\u062e\u064a\u0627\u0631\u0627\u062a \u0644\u0644\u0645\u0647\u0645\u0629 3\n",
        "\n",
        "\n",
        "\n",
        "\u064a\u062c\u0628 \u0639\u0644\u064a\u0643 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0627\u0644\u0641\u0626\u0627\u062a \u0627\u0644\u062a\u0627\u0644\u064a\u0629 \u0645\u0646 CIFAR100:\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "    \n",
        "\n",
        "1. \u0631\u0642\u0645 \u0627\u0644\u0645\u062c\u0645\u0648\u0639\u0629 + 15\n",
        "\n",
        "2. \u0631\u0642\u0645 \u0627\u0644\u062e\u064a\u0627\u0631 + 56\n",
        "\n",
        "  3. \u0418\u04235 (\u0631\u0642\u0645 \u0627\u0644\u062e\u064a\u0627\u0631 + 21)\u061b \u0413\u0423\u0418\u041c\u0426 (80) \u061b  (\u0627\u0644\u0623\u062c\u0627\u0646\u0628(90)  \n",
        "\n",
        "<div>\n",
        "\n",
        "\n",
        "\n",
        "# \u0645\u0647\u0627\u0645 \u0644\u0644\u0639\u0645\u0644 \u0627\u0644\u0630\u0627\u062a\u064a\n",
        "\n",
        "\n",
        "\n",
        "1. \u0642\u0645 \u0628\u062a\u062d\u0644\u064a\u0644 \u0646\u062a\u0627\u0626\u062c \u062a\u062f\u0631\u064a\u0628 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0627\u0644\u062e\u0627\u0635 \u0628\u0643. \u0645\u0627\u0630\u0627 \u062a\u062e\u0628\u0631\u0646\u0627 \u062f\u0642\u0629 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0639\u0644\u0649 \u0645\u062c\u0645\u0648\u0639\u0629 \u0627\u0644\u062a\u062f\u0631\u064a\u0628 \u0648\u0627\u062e\u062a\u0628\u0627\u0631 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a\u061f \u0645\u0639 \u0623\u064a \u0627\u0644\u0641\u0626\u0627\u062a \u064a\u062a\u0639\u0627\u0645\u0644 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0628\u0634\u0643\u0644 \u0623\u0641\u0636\u0644 \u0648\u0644\u0645\u0627\u0630\u0627\u061f\n",
        "\n",
        "2. \u0642\u0645 \u0628\u062a\u062d\u0644\u064a\u0644 \u0646\u062a\u0627\u0626\u062c \u0627\u0644\u062a\u062f\u0631\u064a\u0628. \u0647\u0644 \u062a\u062d\u062f\u062b \u0632\u064a\u0627\u062f\u0629 \u0641\u064a \u062a\u0639\u0644\u0645 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0627\u0644\u062e\u0627\u0635 \u0628\u0643\u061f \u0645\u0627\u0630\u0627 \u064a\u062c\u0628 \u0623\u0646 \u062a\u0641\u0639\u0644 \u0644\u062a\u0642\u0644\u064a\u0644 \u0630\u0644\u0643 (\u062f\u0648\u0646 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0627\u0644\u0627\u0646\u062a\u0638\u0627\u0645)\u061f\n",
        "\n",
        "3. \u0642\u0645 \u0628\u062a\u063a\u064a\u064a\u0631 \u062d\u062c\u0645 \u0627\u0644\u062f\u0641\u0639\u0629\u060c \u0648\u0644\u0643\u0646 \u062d\u0627\u0641\u0638 \u0639\u0644\u0649 \u0627\u0644\u0639\u062f\u062f \u0627\u0644\u0625\u062c\u0645\u0627\u0644\u064a \u0644\u0644\u062a\u0643\u0631\u0627\u0631\u0627\u062a. \u0642\u0645 \u0628\u062a\u062d\u0644\u064a\u0644 \u0646\u062a\u0627\u0626\u062c \u0627\u0644\u062a\u062f\u0631\u064a\u0628 \u0645\u0639 \u0627\u0644\u0645\u0639\u0644\u0645\u0627\u062a \u0627\u0644\u062c\u062f\u064a\u062f\u0629. \u0645\u0627\u0630\u0627 \u062a\u063a\u064a\u0631 \u0648\u0644\u0645\u0627\u0630\u0627\u061f\n",
        "\n",
        "4. \u0642\u0644\u0644 \u0645\u0646 \u0645\u0639\u062f\u0644 \u0627\u0644\u062a\u0639\u0644\u0645 \u0648\u0632\u062f \u0645\u0646 \u0627\u0644\u0639\u062f\u062f \u0627\u0644\u0625\u062c\u0645\u0627\u0644\u064a \u0644\u0644\u062a\u0643\u0631\u0627\u0631\u0627\u062a \u0644\u0632\u064a\u0627\u062f\u0629 \u062f\u0642\u0629 \u0627\u0644\u0646\u0645\u0648\u0630\u062c.\n",
        "\n",
        "5. \u0642\u0645 \u0628\u062a\u063a\u064a\u064a\u0631 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0627\u0644\u062e\u0627\u0635 \u0628\u0643 - \u063a\u064a\u0651\u0631 \u0639\u062f\u062f \u0627\u0644\u062e\u0644\u0627\u064a\u0627 \u0627\u0644\u0639\u0635\u0628\u064a\u0629 \u0648\u0627\u0644\u0637\u0628\u0642\u0627\u062a. \u0642\u0645 \u0628\u062a\u062d\u0644\u064a\u0644 \u0646\u062a\u0627\u0626\u062c \u062a\u062f\u0631\u064a\u0628 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0627\u0644\u062c\u062f\u064a\u062f. \u0627\u0628\u062d\u062b \u0639\u0646 \u0623\u0641\u0636\u0644 \u0627\u0644\u0645\u0639\u0644\u0645\u0627\u062a \u0644\u0647\u0630\u0627 \u0627\u0644\u0646\u0645\u0648\u0630\u062c.\n",
        "\n",
        "6. \u062d\u062f\u062f \u0627\u0644\u0625\u062c\u0631\u0627\u0621\u0627\u062a \u0627\u0644\u062a\u064a \u0633\u0627\u0639\u062f\u062a \u0641\u064a \u0632\u064a\u0627\u062f\u0629 \u062f\u0642\u0629 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0627\u0644\u062e\u0627\u0635 \u0628\u0643 \u0648\u0641\u0633\u0631 \u0627\u0644\u0633\u0628\u0628.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# \u062a\u0639\u0644\u064a\u0642\u0627\u062a \u0639\u0644\u0649 \u0627\u0644\u0639\u0645\u0644 \u0627\u0644\u0630\u0627\u062a\u064a\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "- \u064a\u062a\u0645 \u062a\u063a\u064a\u064a\u0631 \u0645\u0639\u062f\u0644 \u0627\u0644\u062a\u0639\u0644\u0645 \u0648\u062d\u062c\u0645 \u0627\u0644\u062f\u0641\u0639\u0629 \u0641\u0642\u0637 \u0628\u0639\u062f \u062a\u062d\u062f\u064a\u062f \u0644\u062d\u0638\u0629 \u0627\u0644\u0632\u064a\u0627\u062f\u0629 \u0641\u064a \u0627\u0644\u062a\u0639\u0644\u0645 - \u0627\u0644\u062d\u0642\u0628\u0629 \u0627\u0644\u062a\u064a \u062a\u0643\u0648\u0646 \u0641\u064a\u0647\u0627 \u0627\u0644\u062e\u0637\u0623 \u0639\u0644\u0649 \u0645\u062c\u0645\u0648\u0639\u0629 \u0627\u0644\u0627\u062e\u062a\u0628\u0627\u0631 \u0647\u0648 \u0627\u0644\u0623\u062f\u0646\u0649. \u062e\u0644\u0627\u0641 \u0630\u0644\u0643\u060c \u0642\u062f \u064a\u0624\u062f\u064a \u062a\u062d\u0633\u064a\u0646 \u0627\u0644\u062e\u0637\u0648\u0627\u062a \u0625\u0644\u0649 \u062a\u0639\u0632\u064a\u0632 \u0632\u064a\u0627\u062f\u0629 \u0627\u0644\u062a\u0639\u0644\u0645 (\u062a\u062d\u0633\u0646 \u0623\u062b\u0646\u0627\u0621 \u0627\u0644\u062a\u062f\u0631\u064a\u0628\u060c \u0648\u062a\u062f\u0647\u0648\u0631 \u0641\u064a \u0627\u0644\u0627\u062e\u062a\u0628\u0627\u0631).\n",
        "\n",
        "</div>\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "- \u064a\u062c\u0628 \u0623\u0646 \u064a\u0624\u062f\u064a \u0632\u064a\u0627\u062f\u0629 \u062d\u062c\u0645 \u0627\u0644\u062f\u0641\u0639\u0629 \u0645\u0639 \u0646\u0641\u0633 \u0639\u062f\u062f \u0627\u0644\u062a\u0643\u0631\u0627\u0631\u0627\u062a \u0625\u0644\u0649 \u062a\u062d\u0633\u064a\u0646 \u0627\u0644\u062a\u0639\u0644\u0645 \u0642\u0644\u064a\u0644\u0627\u064b (\u0641\u064a \u0627\u0644\u0645\u062a\u0648\u0633\u0637). \u0644\u0623\u0646\u0646\u0627 \u0646\u0642\u0648\u0645 \u0628\u0646\u0641\u0633 \u0627\u0644\u0639\u062f\u062f\u060c \u0648\u0644\u0643\u0646 \u0628\u062e\u0637\u0648\u0627\u062a \u0623\u0641\u0636\u0644. \u0642\u062f \u0644\u0627 \u064a\u062a\u0648\u0627\u0641\u0642 \u0630\u0644\u0643 \u0645\u0639 \u0627\u0644\u062a\u062c\u0631\u0628\u0629 \u0644\u0623\u0633\u0628\u0627\u0628 \u0645\u062e\u062a\u0644\u0641\u0629\u060c \u0648\u0644\u0643\u0646 \u0645\u0646 \u0627\u0644\u0646\u0638\u0631\u064a\u0629 \u0646\u062a\u0648\u0642\u0639 \u062d\u062f\u0648\u062b \u062a\u062d\u0633\u064a\u0646 \u0645\u0627.\n",
        "\n",
        "</div>\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "- \u0639\u062f\u062f \u0627\u0644\u0645\u0631\u0627\u062a \u0627\u0644\u062a\u064a \u062a\u0642\u0648\u0645 \u0641\u064a\u0647\u0627 \u0628\u062a\u063a\u064a\u064a\u0631 \u0645\u0639\u062f\u0644 \u0627\u0644\u062a\u0639\u0644\u0645 \u0623\u0648 \u062d\u062c\u0645 \u0627\u0644\u062f\u0641\u0639\u0629 \u064a\u062c\u0628 \u0623\u0646 \u064a\u0633\u0627\u0648\u064a \u0639\u062f\u062f \u0645\u0631\u0627\u062a \u062a\u063a\u064a\u064a\u0631 \u0639\u062f\u062f \u0627\u0644\u0639\u0635\u0648\u0631 \u0644\u0644\u062d\u0641\u0627\u0638 \u0639\u0644\u0649 \u0627\u0644\u062a\u0643\u0631\u0627\u0631\u0627\u062a. \u0641\u064a \u0627\u0644\u0645\u0645\u0627\u0631\u0633\u0629 \u0627\u0644\u0639\u0645\u0644\u064a\u0629\u060c \u0647\u0630\u0647 \u0627\u0644\u0634\u0631\u0648\u0637 \u0644\u064a\u0633\u062a \u0636\u0631\u0648\u0631\u064a\u0629\u060c \u0644\u0623\u0646\u0646\u0627 \u0639\u0646\u062f \u0625\u0639\u0627\u062f\u0629 \u0627\u0644\u062a\u062f\u0631\u064a\u0628 \u0646\u063a\u064a\u0631 \u0627\u0644\u0639\u062f\u064a\u062f \u0645\u0646 \u0627\u0644\u0645\u0639\u0644\u0645\u0627\u062a \u0641\u064a \u0646\u0641\u0633 \u0627\u0644\u0648\u0642\u062a. \u0644\u0643\u0646\u0646\u0627 \u0646\u062a\u0639\u0644\u0645\u060c \u0644\u0630\u0644\u0643 \u0646\u062f\u0631\u0633 \u0633\u0644\u0648\u0643 \u0643\u0644 \u0645\u0639\u0644\u0645\u0629 \u0639\u0644\u0649 \u062d\u062f\u0629: \u0644\u0627 \u064a\u0645\u0643\u0646\u0646\u0627 \u0645\u0642\u0627\u0631\u0646\u0629 5 \u062e\u0637\u0648\u0627\u062a \u0643\u0628\u064a\u0631\u0629 \u06487 \u062e\u0637\u0648\u0627\u062a \u0635\u063a\u064a\u0631\u0629\u060c \u0644\u0643\u0646 \u064a\u0645\u0643\u0646\u0646\u0627 \u0645\u0642\u0627\u0631\u0646\u0629 5 \u062e\u0637\u0648\u0627\u062a \u0643\u0628\u064a\u0631\u0629 \u06485 \u062e\u0637\u0648\u0627\u062a \u0635\u063a\u064a\u0631\u0629.\n",
        "\n",
        "</div>\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "- \u0639\u0646\u062f \u062a\u063a\u064a\u064a\u0631 \u0627\u0644\u0646\u0645\u0648\u0630\u062c\u060c \u064a\u062c\u0628 \u0627\u0644\u0628\u062d\u062b \u0639\u0646 \u0627\u0644\u0645\u0639\u0644\u0645\u0627\u062a \u0627\u0644\u062c\u062f\u064a\u062f\u0629 \u0645\u0631\u0629 \u0623\u062e\u0631\u0649 - \u0647\u0630\u0627 \u0646\u0645\u0648\u0630\u062c \u062c\u062f\u064a\u062f. \u0644\u064a\u0633 \u0645\u0646 \u0627\u0644\u0636\u0631\u0648\u0631\u064a \u0625\u0639\u0627\u062f\u0629 \u0634\u0631\u062d \u062c\u0645\u064a\u0639 \u0627\u0644\u062e\u0637\u0648\u0627\u062a\u060c \u0641\u0642\u0637 \u0642\u0645 \u0628\u0630\u0644\u0643 \u0628\u0646\u0641\u0633\u0643 - \u0627\u0628\u062d\u062b \u0639\u0646 \u0632\u064a\u0627\u062f\u0629 \u0627\u0644\u062a\u0639\u0644\u0645 \u0648\u0627\u0628\u062d\u062b \u0639\u0646 \u0645\u0639\u062f\u0644 \u0648\u062d\u062c\u0645 \u062f\u0641\u0639\u0629 \u0633\u064a\u0643\u0648\u0646\u0627\u0646 \u0623\u0641\u0636\u0644.\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "# \u0623\u0633\u0626\u0644\u0629 \u0627\u0644\u0645\u0631\u0627\u0642\u0628\u0629 \u0644\u0644\u062f\u0641\u0627\u0639\n",
        "\n",
        "\n",
        "\n",
        "1. \u0634\u0628\u0643\u0629 \u0639\u0635\u0628\u064a\u0629 \u0643\u0627\u0645\u0644\u0629 \u0627\u0644\u0627\u062a\u0635\u0627\u0644\u060c \u0627\u0634\u0631\u062d \u0627\u0644\u0647\u064a\u0643\u0644 \u0648\u0627\u0644\u062d\u0633\u0627\u0628\u0627\u062a \u0648\u0627\u0644\u063a\u0631\u0636 \u0645\u0646 \u0627\u0644\u0637\u0628\u0642\u0627\u062a \u0648\u0645\u0643\u0648\u0646\u0627\u062a \u0627\u0644\u062e\u0644\u0627\u064a\u0627 \u0627\u0644\u0639\u0635\u0628\u064a\u0629.\n",
        "\n",
        "2. \u0627\u0630\u0643\u0631 \u0639\u062f\u062f \u0627\u0644\u062e\u0644\u0627\u064a\u0627 \u0627\u0644\u0639\u0635\u0628\u064a\u0629 \u0648\u0627\u0644\u0627\u062a\u0635\u0627\u0644\u0627\u062a \u0648\u0627\u0644\u0623\u0648\u0632\u0627\u0646 \u0641\u064a \u0634\u0628\u0643\u0629 \u0639\u0635\u0628\u064a\u0629 \u0643\u0627\u0645\u0644\u0629 \u0627\u0644\u0627\u062a\u0635\u0627\u0644.\n",
        "\n",
        "3. \u0648\u0635\u0641 \u0645\u0647\u0627\u0645 \u0627\u0644\u0627\u0646\u062d\u062f\u0627\u0631 \u0648\u0627\u0644\u062a\u0635\u0646\u064a\u0641. \u0645\u0627 \u0647\u064a \u062f\u0648\u0627\u0644 \u0627\u0644\u062e\u0633\u0627\u0631\u0629 \u0627\u0644\u0645\u0633\u062a\u062e\u062f\u0645\u0629 \u0641\u064a \u0647\u0630\u0647 \u0627\u0644\u0645\u0647\u0627\u0645\u061f\n",
        "\n",
        "4. \u0648\u0635\u0641 \u0647\u064a\u0643\u0644 \u0645\u062c\u0645\u0648\u0639\u0629 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a\u060c \u0648\u0627\u0644\u063a\u0631\u0636 \u0645\u0646 \u0623\u062c\u0632\u0627\u0626\u0647\u0627.\n",
        "\n",
        "5. \u0648\u0635\u0641 \u062e\u0648\u0627\u0631\u0632\u0645\u064a\u0629 \u0627\u0644\u0627\u0646\u062d\u062f\u0627\u0631 \u0627\u0644\u0639\u0634\u0648\u0627\u0626\u064a. \u0627\u0630\u0643\u0631 \u063a\u0631\u0636 \u0627\u0644\u0645\u0639\u0644\u0645\u0627\u062a. \u0645\u0627 \u0627\u0644\u0641\u0631\u0642 \u0628\u064a\u0646 \u0627\u0644\u0627\u0646\u062d\u062f\u0627\u0631 \u0627\u0644\u0639\u0634\u0648\u0627\u0626\u064a \u0648\u0627\u0644\u0627\u0646\u062d\u062f\u0627\u0631 \u0639\u0644\u0649 \u062f\u0641\u0639\u0627\u062a\u061f\n",
        "\n",
        "6. \u0645\u0627 \u0647\u064a \u0627\u0644\u062d\u0642\u0628\u0629\u060c \u0648\u0627\u0644\u062a\u0643\u0631\u0627\u0631\u060c \u0648\u062d\u062c\u0645 \u0627\u0644\u062f\u0641\u0639\u0629 \u0641\u064a \u0627\u0644\u062a\u062f\u0631\u064a\u0628. \u0643\u064a\u0641 \u062a\u0631\u062a\u0628\u0637 \u0628\u0628\u0639\u0636\u0647\u0627\u061f\n",
        "\n",
        "7. \u0645\u0627 \u0647\u0648 \u0627\u0644\u062a\u0639\u0644\u0645 \u062a\u062d\u062a \u0627\u0644\u0625\u0634\u0631\u0627\u0641\u060c \u0648\u0627\u0644\u062a\u0639\u0644\u0645 \u0628\u062f\u0648\u0646 \u0625\u0634\u0631\u0627\u0641\u060c \u0648\u0627\u0644\u062a\u0639\u0644\u0645 \u0627\u0644\u0645\u0639\u0632\u0632\u061f \u0642\u062f\u0645 \u0623\u0645\u062b\u0644\u0629 \u0639\u0644\u0649 \u0627\u0644\u0623\u0633\u0627\u0644\u064a\u0628 \u0648\u0627\u0644\u0645\u0647\u0627\u0645 \u0644\u0643\u0644 \u0646\u0648\u0639 \u0645\u0646 \u0623\u0646\u0648\u0627\u0639 \u0627\u0644\u062a\u0639\u0644\u0645.\n",
        "\n",
        "---\n",
        "\n",
        "# \u0648\u0635\u0641 \u0627\u0644\u062c\u062f\u0648\u0644 \u0627\u0644\u0646\u0647\u0627\u0626\u064a\n",
        "\n",
        "\n",
        "\n",
        "|  \u062a\u0643\u0648\u064a\u0646 \u0627\u0644\u0634\u0628\u0643\u0629 \u0627\u0644\u0639\u0635\u0628\u064a\u0629   |  \u0627\u0644\u0645\u0639\u0644\u0645\u0627\u062a \u0627\u0644\u0641\u0627\u0626\u0642\u0629  |  \u0627\u0644\u062f\u0642\u0629  |  \u0627\u0644\u062a\u0639\u0644\u064a\u0642  |\n",
        "\n",
        "|----------|----------|----------|----------|\n",
        "\n",
        "|  FC(10), FC(3)   | lr = 0.003, batch_size = 128, epochs = 100  |  \u0627\u062e\u062a\u0628\u0627\u0631 = 70%\u060c \u062a\u062f\u0631\u064a\u0628 = 98%  |  \u0627\u0644\u062e\u064a\u0627\u0631 \u0627\u0644\u0623\u0633\u0627\u0633\u064a  |\n",
        "\n",
        "|  FC(10), FC(3)   | lr = 0.001, batch_size = 128, epochs = 300  |  \u0627\u062e\u062a\u0628\u0627\u0631 = 72%\u060c \u062a\u062f\u0631\u064a\u0628 = 99%  |  \u062a\u0645 \u062a\u0642\u0644\u064a\u0644 \u0645\u0639\u062f\u0644 \u0627\u0644\u062a\u0639\u0644\u0645 \u0628\u0645\u0642\u062f\u0627\u0631 3 \u0645\u0631\u0627\u062a\u060c \u0644\u062a\u0639\u0648\u064a\u0636 \u0630\u0644\u0643 \u0632\u0627\u062f \u0639\u062f\u062f \u0627\u0644\u0639\u0635\u0648\u0631 \u0628\u0646\u0641\u0633 \u0627\u0644\u0645\u0642\u062f\u0627\u0631  |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### \u0637\u0631\u0642 \u0648\u062f\u0648\u0627\u0644 PyTorch\n",
        "\n",
        "<div dir = \"rtl\">\n",
        "\n",
        "(Documentation: https://pytorch.org/docs/stable/index.html)\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## \u0627\u0644\u0645\u0643\u062a\u0628\u0627\u062a:\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">* __np__ - \u0645\u0643\u062a\u0628\u0629 NumPy \u0644\u0644\u0639\u0645\u0644 \u0645\u0639 \u0627\u0644\u0645\u0635\u0641\u0648\u0627\u062a \u0645\u062a\u0639\u062f\u062f\u0629 \u0627\u0644\u0623\u0628\u0639\u0627\u062f \u0644\u0644\u0628\u064a\u0627\u0646\u0627\u062a</div>  \n",
        "\n",
        "<div dir=\"rtl\">* __pickle__ - \u0645\u0643\u062a\u0628\u0629 Pickle \u0644\u062a\u0633\u0644\u0633\u0644 \u0648\u062a\u0641\u0643\u064a\u0643 \u0647\u064a\u0627\u0643\u0644 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0641\u064a \u0644\u063a\u0629 Python</div>  \n",
        "\n",
        "<div dir=\"rtl\">* __sklearn__ - \u0645\u0643\u062a\u0628\u0629 \u062a\u0646\u0641\u0630 \u0641\u064a \u0627\u0644\u0623\u0633\u0627\u0633 \u0637\u0631\u0642 \u0627\u0644\u062a\u0639\u0644\u0645 \u0627\u0644\u0622\u0644\u064a \u0627\u0644\u0643\u0644\u0627\u0633\u064a\u0643\u064a \u0648\u0623\u062f\u0648\u0627\u062a \u0644\u0644\u0639\u0645\u0644 \u0645\u0639\u0647\u0627</div>  \n",
        "\n",
        "<div dir=\"rtl\">* __PIL__ - \u0645\u0643\u062a\u0628\u0629 \u062e\u0641\u064a\u0641\u0629 \u0627\u0644\u0648\u0632\u0646 Pillow \u0644\u0644\u0639\u0645\u0644 \u0645\u0639 \u0627\u0644\u0635\u0648\u0631 \u0648\u0639\u0631\u0636 \u0627\u0644\u0639\u0646\u0627\u0635\u0631 \u0627\u0644\u0631\u0633\u0648\u0645\u064a\u0629 \u0645\u0628\u0627\u0634\u0631\u0629 \u0641\u064a Jupyter Notebook</div>  \n",
        "\n",
        "<div dir=\"rtl\">* __matplotlib__ - \u0645\u0643\u062a\u0628\u0629 \u0644\u0631\u0633\u0645 \u0627\u0644\u0631\u0633\u0648\u0645 \u0627\u0644\u0628\u064a\u0627\u0646\u064a\u0629\u060c \u062a\u062a\u0643\u0631\u0631 \u0641\u064a \u0627\u0644\u063a\u0627\u0644\u0628 \u0648\u0627\u062c\u0647\u0629 \u0628\u0631\u0645\u062c\u0629 \u0627\u0644\u062a\u0637\u0628\u064a\u0642\u0627\u062a \u0627\u0644\u062e\u0627\u0635\u0629 \u0628\u0640 Matlab</div>  \n",
        "\n",
        "<div dir=\"rtl\">* __torch__ - \u0645\u0643\u062a\u0628\u0629 PyTorch \u0644\u0644\u062a\u0639\u0644\u0645 \u0627\u0644\u0639\u0645\u064a\u0642 \u0644\u0644\u0634\u0628\u0643\u0627\u062a \u0627\u0644\u0639\u0635\u0628\u064a\u0629</div>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "<div dir=\"rtl\"><h2>\u0627\u0644\u0627\u062e\u062a\u0635\u0627\u0631\u0627\u062a \u0627\u0644\u0645\u0639\u062a\u0645\u062f\u0629:</h2></div>  \n",
        "\n",
        "<div dir=\"rtl\">* torch.nn - nn</div>  \n",
        "\n",
        "<div dir=\"rtl\">* torch.nn.functional - F</div>  \n",
        "\n",
        "<div dir=\"rtl\">* torch.optim - optim</div>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\"><h1>\u0627\u0644\u0637\u0631\u0642</h1></div>  \n",
        "\n",
        "<div dir=\"rtl\">* __torch.Tensor__ - \u064a\u0646\u0634\u0626 \u062a\u064a\u0646\u0633\u0648\u0631 \u0645\u0646 \u0645\u0635\u0641\u0648\u0641\u0629 Numpy \u0645\u062a\u0639\u062f\u062f\u0629 \u0627\u0644\u0623\u0628\u0639\u0627\u062f \u0648\u064a\u0631\u062b \u0646\u0648\u0639 \u0628\u064a\u0627\u0646\u0627\u062a\u0647. \u0628\u0634\u0643\u0644 \u0627\u0641\u062a\u0631\u0627\u0636\u064a\u060c \u064a\u062a\u0645 \u062a\u062e\u0635\u064a\u0635 \u0627\u0644\u0630\u0627\u0643\u0631\u0629 \u0644\u0644\u062a\u064a\u0646\u0633\u0648\u0631\u0627\u062a \u0639\u0644\u0649 \u0648\u062d\u062f\u0629 \u0627\u0644\u0645\u0639\u0627\u0644\u062c\u0629 \u0627\u0644\u0645\u0631\u0643\u0632\u064a\u0629 (CPU). \u0639\u0646\u062f \u062a\u0639\u064a\u064a\u0646 \u0639\u0644\u0627\u0645\u0629 __requires_grad__\u060c \u062a\u062a\u0639\u0642\u0628 \u062a\u0644\u0642\u0627\u0626\u064a\u064b\u0627 \u0627\u0644\u062a\u062f\u0631\u062c\u0627\u062a \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0645\u062d\u0631\u0643 autograd\u060c \u0627\u0644\u0630\u064a \u064a\u0628\u0646\u064a \u0631\u0633\u0645\u064b\u0627 \u0628\u064a\u0627\u0646\u064a\u064b\u0627 \u062f\u064a\u0646\u0627\u0645\u064a\u0643\u064a\u064b\u0627 \u0644\u0644\u062d\u0633\u0627\u0628. \u064a\u0645\u0643\u0646 \u0623\u064a\u0636\u064b\u0627 \u062a\u0645\u0643\u064a\u0646 \u062a\u062a\u0628\u0639 \u0627\u0644\u062a\u064a\u0646\u0633\u0648\u0631 __t__ \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0637\u0631\u064a\u0642\u0629 __t.requires_grad_(True)__ . \u0641\u064a \u0647\u0630\u0647 \u0627\u0644\u062d\u0627\u0644\u0629\u060c \u0628\u0639\u062f \u0627\u0633\u062a\u062f\u0639\u0627\u0621 \u0637\u0631\u064a\u0642\u0629 __backward__\u060c \u0633\u064a\u062a\u0645 \u062a\u0633\u062c\u064a\u0644 \u0627\u0644\u0645\u0634\u062a\u0642\u0627\u062a \u0641\u064a \u062d\u0642\u0644 __grad__. \u064a\u0645\u0643\u0646 \u0645\u0633\u062d \u0645\u0634\u062a\u0642\u0627\u062a \u0627\u0644\u062a\u064a\u0646\u0633\u0648\u0631 __t__ \u0628\u0627\u0633\u062a\u062f\u0639\u0627\u0621 \u0637\u0631\u064a\u0642\u0629 __t.grad.zero_()__. \u0644\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0637\u0631\u064a\u0642\u0629 __detach__ \u0644\u0642\u0637\u0639 \u0627\u0644\u062d\u0633\u0627\u0628\u0627\u062a \u063a\u064a\u0631 \u0627\u0644\u0636\u0631\u0648\u0631\u064a\u0629 \u0644\u0644\u0645\u0634\u062a\u0642\u0627\u062a\u060c \u0648\u0627\u0644\u062a\u064a \u062a\u0646\u0634\u0626 \u0646\u0633\u062e\u0629 \u0645\u0646 \u0627\u0644\u062a\u064a\u0646\u0633\u0648\u0631\u060c \u0645\u0639 \u0625\u0632\u0627\u0644\u0629 \u0639\u0644\u0627\u0645\u0629 __requires_grad__\u060c \u0648\u064a\u062a\u0648\u0642\u0641 \u062a\u062a\u0628\u0639 \u0645\u062d\u0631\u0643 autograd.</div>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">* __torch.numpy__ - \u064a\u0646\u0634\u0626 \u0645\u0635\u0641\u0648\u0641\u0629 \u0628\u064a\u0627\u0646\u0627\u062a NumPy \u0645\u062a\u0639\u062f\u062f\u0629 \u0627\u0644\u0623\u0628\u0639\u0627\u062f \u0645\u0646 \u0627\u0644\u062a\u0646\u0633\u0648\u0631.</div>  \n",
        "\n",
        "<div dir=\"rtl\">* __torch.item__ - \u064a\u0639\u064a\u062f \u0639\u062f\u062f\u064b\u0627\u060c \u0648\u0644\u0643\u0646 \u0641\u0642\u0637 \u0625\u0630\u0627 \u0643\u0627\u0646\u062a \u0631\u062a\u0628\u0629 \u0627\u0644\u062a\u0646\u0633\u0648\u0631 0. \u062e\u0644\u0627\u0641 \u0630\u0644\u0643\u060c \u064a\u0639\u0637\u064a \u062e\u0637\u0623 \u0648\u064a\u062c\u0628 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 torch.numpy.</div>  \n",
        "\n",
        "<div dir=\"rtl\">* __torch.uint8__, __torch.int16__, __torch.int64__, __torch.float32__ - \u062a\u062d\u0648\u064a\u0644 \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0629 \u0625\u0644\u0649 \u0646\u0648\u0639 \u062c\u062f\u064a\u062f\u060c \u0645\u0634\u0627\u0628\u0647 \u0644\u0640 NumPy. \u064a\u062a\u0645 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0627\u0644\u0637\u0631\u064a\u0642\u0629 .to (\u0639\u0644\u0649 \u0633\u0628\u064a\u0644 \u0627\u0644\u0645\u062b\u0627\u0644 <code>t.to(torch.int64))</code> \u0644\u0644\u062a\u062d\u0648\u064a\u0644. \u0628\u0634\u0643\u0644 \u0627\u0641\u062a\u0631\u0627\u0636\u064a\u060c \u064a\u062a\u0645 \u0625\u062c\u0631\u0627\u0621 \u062c\u0645\u064a\u0639 \u0627\u0644\u062d\u0633\u0627\u0628\u0627\u062a \u0639\u0644\u0649 \u0627\u0644\u0631\u0633\u0645 \u0627\u0644\u0628\u064a\u0627\u0646\u064a \u0641\u064a float64\u060c \u0648\u0647\u0646\u0627\u0643 \u0623\u064a\u0636\u064b\u0627 \u0625\u0645\u0643\u0627\u0646\u064a\u0629 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0627\u0644\u062f\u0642\u0629 \u0627\u0644\u0645\u062e\u062a\u0644\u0637\u0629 (\u0634\u064a\u0621 \u0641\u064a float16\u060c \u0648\u0634\u064a\u0621 \u0641\u064a float64)\u060c \u0648\u0644\u0643\u0646 \u062a\u0639\u062a\u0628\u0631 \u0647\u0630\u0647 \u062a\u0642\u0646\u064a\u0629 \u0645\u062a\u0642\u062f\u0645\u0629.</div>  \n",
        "\n",
        "<div dir=\"rtl\">* __torch.ones__, __torch.zeros__, __torch.transpose__, __torch.reshape__ - \u0648\u0627\u062c\u0647\u0629 \u0628\u0631\u0645\u062c\u0629 \u0627\u0644\u062a\u0637\u0628\u064a\u0642\u0627\u062a \u0645\u0634\u0627\u0628\u0647\u0629 \u0644\u0640 NumPy.</div>  \n",
        "\n",
        "<div dir=\"rtl\">* __torch.rand__ - \u0625\u0646\u0634\u0627\u0621 \u062a\u0646\u0633\u0648\u0631 \u0639\u0634\u0648\u0627\u0626\u064a \u0628\u0623\u0631\u0642\u0627\u0645 \u0641\u064a \u0627\u0644\u0646\u0637\u0627\u0642 \u0645\u0646 0 \u0625\u0644\u0649 1. \u064a\u062a\u0645 \u0633\u0631\u062f \u0627\u0644\u0623\u0628\u0639\u0627\u062f \u0639\u0628\u0631 \u0627\u0644\u0641\u0648\u0627\u0635\u0644.</div>  \n",
        "\n",
        "<div dir=\"rtl\">* __torch.t__ - \u0646\u0642\u0644 \u0627\u0644\u062a\u0646\u0633\u0648\u0631\u060c \u0645\u0634\u0627\u0628\u0647 \u0644\u0640 __numpy.transpose__. \u0625\u0630\u0627 \u0643\u0627\u0646 \u0647\u0646\u0627\u0643 \u062a\u0646\u0633\u0648\u0631 X\u060c \u064a\u0645\u0643\u0646 \u0646\u0642\u0644\u0647 \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 <code>X.t()</code>.</div>  \n",
        "\n",
        "<div dir=\"rtl\">* __torch.sum__ - \u062c\u0645\u0639 \u0639\u0646\u0627\u0635\u0631 \u0627\u0644\u062a\u0646\u0633\u0648\u0631 \u0639\u0644\u0649 \u0637\u0648\u0644 \u0627\u0644\u0645\u062d\u0648\u0631 \u0627\u0644\u0645\u062d\u062f\u062f __axis__. \u0625\u0630\u0627 \u062a\u0645 \u0627\u0644\u062c\u0645\u0639 \u0639\u0644\u0649 \u0637\u0648\u0644 \u0627\u0644\u0645\u062d\u0648\u0631 \u0627\u0644\u0623\u062e\u064a\u0631\u060c \u064a\u0645\u0643\u0646 \u062a\u062d\u062f\u064a\u062f \u0627\u0644\u0631\u0642\u0645 -1 \u0628\u062f\u0644\u0627\u064b \u0645\u0646 \u0630\u0644\u0643. \u0644\u0644\u062d\u0641\u0627\u0638 \u0639\u0644\u0649 \u0623\u0628\u0639\u0627\u062f \u0627\u0644\u062a\u0646\u0633\u0648\u0631 \u0627\u0644\u0623\u0635\u0644\u064a\u0629\u060c \u064a\u062c\u0628 \u062a\u0639\u064a\u064a\u0646 \u0639\u0644\u0627\u0645\u0629 __keepdims__.</div>  \n",
        "\n",
        "<div dir=\"rtl\">* __torch.maximum__ - \u064a\u0642\u0648\u0645 \u0628\u0627\u0644\u0645\u0642\u0627\u0631\u0646\u0629 \u0627\u0644\u0639\u0646\u0635\u0631\u064a\u0629 \u0628\u064a\u0646 \u0627\u0644\u062a\u0646\u0633\u0648\u0631\u0627\u062a \u0648\u064a\u0639\u064a\u062f \u0627\u0644\u062d\u062f \u0627\u0644\u0623\u0642\u0635\u0649 \u0645\u0646 \u0627\u0644\u0639\u0646\u0627\u0635\u0631. \u0641\u064a \u0627\u0644\u0645\u0645\u0627\u0631\u0633\u0629 \u0627\u0644\u0639\u0645\u0644\u064a\u0629\u060c \u064a\u062a\u0645 \u0627\u0633\u062a\u062e\u062f\u0627\u0645\u0647 \u0644\u062a\u0646\u0641\u064a\u0630 \u0628\u0639\u0636 \u0648\u0638\u0627\u0626\u0641 \u0627\u0644\u062a\u0646\u0634\u064a\u0637 \u0644\u0644\u0634\u0628\u0643\u0629 \u0627\u0644\u0639\u0635\u0628\u064a\u0629.</div>  \n",
        "\n",
        "<div dir=\"rtl\">* __torch.mm__ - \u0636\u0631\u0628 \u0627\u0644\u062a\u0646\u0633\u0648\u0631\u0627\u062a. \u0628\u0627\u0644\u0646\u0633\u0628\u0629 \u0644\u0645\u0635\u0641\u0648\u0641\u062a\u064a\u0646 \u062b\u0646\u0627\u0626\u064a\u062a\u064a\u0646 \u0627\u0644\u0623\u0628\u0639\u0627\u062f \u0628\u0623\u0628\u0639\u0627\u062f (M, N) \u0648 (N, K)\u060c \u0633\u064a\u0643\u0648\u0646 \u0646\u0627\u062a\u062c \u0647\u0630\u0647 \u0627\u0644\u0637\u0631\u064a\u0642\u0629 \u0645\u0635\u0641\u0648\u0641\u0629 \u062b\u0646\u0627\u0626\u064a\u0629 \u0627\u0644\u0623\u0628\u0639\u0627\u062f \u0628\u0623\u0628\u0639\u0627\u062f (M, K).</div>  \n",
        "\n",
        "<div dir=\"rtl\">* __torch.exp__ - \u064a\u0639\u064a\u062f \u0646\u0641\u0633 \u0648\u0638\u064a\u0641\u0629 __numpy.exp__ - \u0631\u0641\u0639 \u0627\u0644\u062a\u0646\u0633\u0648\u0631 \u0625\u0644\u0649 \u0642\u0648\u0629 \u0627\u0644\u0623\u0633\u0627\u0633 \u0628\u0635\u0648\u0631\u0629 \u0639\u0646\u0635\u0631\u064a\u0629.</div>  \n",
        "\n",
        "<div dir=\"rtl\">* __torch.log__ - \u0639\u0645\u0644\u064a\u0629 \u062a\u0633\u062c\u064a\u0644 \u0639\u0646\u0635\u0631\u064a \u0644\u0644\u062a\u0646\u0633\u0648\u0631 - \u0623\u062e\u0630 \u0627\u0644\u0644\u0648\u063a\u0627\u0631\u064a\u062a\u0645 \u0627\u0644\u0637\u0628\u064a\u0639\u064a\u060c \u0648\u0647\u0648 \u0627\u0644\u0639\u0645\u0644\u064a\u0629 \u0627\u0644\u0639\u0643\u0633\u064a\u0629 \u0644\u0644\u0631\u0641\u0639 \u0644\u0644\u0642\u0648\u0629.</div>  \n",
        "\n",
        "<div dir=\"rtl\">* __torch.flatten__ - \u0645\u0634\u0627\u0628\u0647 \u0644\u0640 NumPy .reshape(-1)\u060c \u0625\u0630\u0627 \u062a\u0645 \u062a\u062d\u062f\u064a\u062f \u0645\u0639\u0644\u0645\u0629 start_dim\u060c \u0641\u0633\u064a\u0628\u062f\u0623 \"\u062a\u0633\u0648\u064a\u0629\" \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0629 \u0628\u062f\u0621\u064b\u0627 \u0645\u0646 \u0627\u0644\u0631\u0642\u0645 \u0627\u0644\u0645\u062d\u062f\u062f. \u0623\u064a \u0644\u062a\u062d\u0648\u064a\u0644 \u0627\u0644\u062a\u0646\u0633\u0648\u0631 t \u0645\u0639 \u0627\u0644\u0634\u0643\u0644 (100, 32, 32, 3) \u0625\u0644\u0649 \u0627\u0644\u0634\u0643\u0644 (100, 3072) \u064a\u0643\u0641\u064a \u0643\u062a\u0627\u0628\u0629 <code>torch.flatten(t, start_dim=1).</code></div>  \n",
        "\n",
        "<div dir=\"rtl\">* __F.one_hot__ - \u0648\u0627\u062d\u062f\u0629 \u0645\u0646 \u0627\u0644\u0637\u0631\u0642 \u0627\u0644\u0639\u062f\u064a\u062f\u0629 \u0644\u0644\u062d\u0635\u0648\u0644 \u0639\u0644\u0649 \u062a\u0631\u0645\u064a\u0632 \u062d\u0627\u0631 \u0644\u0644\u0641\u0626\u0629 \u0641\u064a \u0634\u0643\u0644 \u062a\u0646\u0633\u0648\u0631 PyTorch. \u0639\u0644\u0649 \u0633\u0628\u064a\u0644 \u0627\u0644\u0645\u062b\u0627\u0644\u060c \u0628\u0627\u0644\u0646\u0633\u0628\u0629 \u0644\u062e\u0645\u0633 \u0641\u0626\u0627\u062a\u060c \u0633\u064a\u0643\u0648\u0646 \u0627\u0644\u062a\u0631\u0645\u064a\u0632 \u0627\u0644\u062d\u0627\u0631 \u0644\u0644\u0641\u0626\u0629 \"4\" \u0647\u0648 [0, 0, 0, 1, 0].</div>  \n",
        "\n",
        "<div dir=\"rtl\">* __torch.utils.data.TensorDataset__ - \u0625\u0646\u0634\u0627\u0621 \u062a\u0646\u0633\u0648\u0631\u0627\u062a \u0645\u0631\u062a\u0628\u0637\u0629\u060c \u0645\u062b\u0644 \u0623\u0645\u062b\u0644\u0629 \u0627\u0644\u062a\u062f\u0631\u064a\u0628 \u0648\u0627\u0644\u0639\u0644\u0627\u0645\u0627\u062a \u0627\u0644\u0645\u0642\u0627\u0628\u0644\u0629. \u064a\u062a\u0645 \u062a\u0645\u0631\u064a\u0631 \u0627\u0644\u062a\u0646\u0633\u0648\u0631\u0627\u062a \u0643\u0645\u0639\u0644\u0645\u0627\u062a. \u0637\u0631\u064a\u0642\u0629 \u0645\u0642\u0628\u0648\u0644\u0629 \u0644\u0625\u0646\u0634\u0627\u0621 \u0645\u062c\u0645\u0648\u0639\u0629 \u0628\u064a\u0627\u0646\u0627\u062a \u0639\u0646\u062f\u0645\u0627 \u062a\u0643\u0648\u0646 \u0639\u064a\u0646\u0629 \u0627\u0644\u062a\u062f\u0631\u064a\u0628 \u0635\u063a\u064a\u0631\u0629 \u062a\u0645\u0627\u0645\u064b\u0627 \u0648\u062a\u0646\u0627\u0633\u0628 \u0628\u0627\u0644\u0643\u0627\u0645\u0644 \u0641\u064a \u0627\u0644\u0630\u0627\u0643\u0631\u0629.</div>  \n",
        "\n",
        "<div dir=\"rtl\">* __torch.utils.data.DataLoader__ - \u062a\u0639\u062a\u0645\u062f \u0623\u062f\u0627\u0629 \u062a\u062d\u0645\u064a\u0644 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0641\u064a PyTorch \u0639\u0644\u0649 \u0641\u0626\u0629 DataLoader. \u0625\u0646\u0647\u0627 \u0643\u0627\u0626\u0646 Python \u064a\u062a\u0643\u0631\u0631 \u0639\u0628\u0631 \u0645\u062c\u0645\u0648\u0639\u0629 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a\u060c \u0645\u0639 \u062f\u0639\u0645 \u0645\u062c\u0645\u0648\u0639\u0629 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0628\u0623\u0633\u0644\u0648\u0628 \u0627\u0644\u062e\u0631\u064a\u0637\u0629 \u0648\u0627\u0644\u0645\u0643\u0631\u0631\u061b \u0625\u0639\u062f\u0627\u062f\u0627\u062a \u062a\u0631\u062a\u064a\u0628 \u062a\u062d\u0645\u064a\u0644 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a\u061b \u062a\u0642\u0633\u064a\u0645 \u062a\u0644\u0642\u0627\u0626\u064a \u0625\u0644\u0649 \u062f\u0641\u0639\u0627\u062a \u0635\u063a\u064a\u0631\u0629\u061b \u062a\u062d\u0645\u064a\u0644 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0641\u064a \u0639\u0645\u0644\u064a\u0627\u062a/\u062e\u064a\u0648\u0637 \u0645\u062a\u0639\u062f\u062f\u0629. \u0623\u0643\u062b\u0631 \u0627\u0644\u0645\u0639\u0644\u0645\u0627\u062a \u0641\u0627\u0626\u062f\u0629 \u0641\u064a \u0627\u0644\u0628\u0627\u0646\u064a \u0647\u064a \u062d\u062c\u0645 \u0627\u0644\u062f\u0641\u0639\u0629 \u0627\u0644\u0635\u063a\u064a\u0631\u0629 __batch_size__ \u0648\u0639\u062f\u062f \u0627\u0644\u0639\u0645\u0644\u064a\u0627\u062a \u0627\u0644\u0645\u062a\u0648\u0627\u0632\u064a\u0629 __num_workers__. \u0644\u0645\u0632\u062c \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a (\u0644\u062a\u062d\u0642\u064a\u0642 \u062a\u0642\u0627\u0631\u0628 \u0623\u0641\u0636\u0644)\u060c \u064a\u062c\u0628 \u062a\u0639\u064a\u064a\u0646 \u0639\u0644\u0627\u0645\u0629 __shuffle__ \u0625\u0644\u0649 True.</div>  \n",
        "\n",
        "<div dir=\"rtl\">* __torch.save__ - \u062d\u0641\u0638 \u0645\u0639\u0644\u0645\u0627\u062a \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0639\u0644\u0649 \u0648\u0633\u0627\u0626\u0637 \u0627\u0644\u062a\u062e\u0632\u064a\u0646 \u0627\u0644\u062f\u0627\u0626\u0645\u0629. \u0644\u0630\u0644\u0643 \u064a\u062a\u0645 \u062a\u0645\u0631\u064a\u0631 model.state_dict() \u0643\u0623\u0648\u0644 \u0645\u0639\u0644\u0645\u0629\u060c \u062d\u064a\u062b model \u0647\u0648 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0627\u0644\u0639\u0635\u0628\u064a \u0627\u0644\u0645\u062f\u0631\u0628\u060c \u0648\u0627\u0644\u062b\u0627\u0646\u064a \u0647\u0648 \u0627\u0644\u0645\u0633\u0627\u0631 \u0645\u0639 \u0627\u0633\u0645 \u0627\u0644\u0645\u0644\u0641.</div>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\"><h1>\u0625\u0646\u0634\u0627\u0621 \u0627\u0644\u0646\u0645\u0627\u0630\u062c</h1>:</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\u064a\u062a\u0645 \u0625\u0646\u0634\u0627\u0621 \u0627\u0644\u0646\u0645\u0627\u0630\u062c \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0648\u062d\u062f\u0629 nn\u060c \u062d\u064a\u062b \u062a\u0645 \u062a\u0646\u0641\u064a\u0630 \u0623\u0634\u0647\u0631 \u0643\u062a\u0644 \u0627\u0644\u0634\u0628\u0643\u0627\u062a \u0627\u0644\u0639\u0635\u0628\u064a\u0629 \u0623\u0648 \u0627\u0644\u0637\u0628\u0642\u0627\u062a \u0641\u064a \u0627\u0644\u0648\u062d\u062f\u0629\u060c \u0645\u062b\u0644:</div>\n",
        "\n",
        "<div dir=\"rtl\">* \u0637\u0628\u0642\u0629 \u0627\u0644\u0627\u062a\u0635\u0627\u0644 \u0627\u0644\u0643\u0627\u0645\u0644 Linear</div>  \n",
        "\n",
        "<div dir=\"rtl\">* \u0627\u0644\u0637\u0628\u0642\u0629 \u0627\u0644\u062a\u0644\u0627\u0641\u064a\u0641\u064a\u0629 Conv2d</div>  \n",
        "\n",
        "<div dir=\"rtl\">* \u062a\u062c\u0645\u064a\u0639 MaxPool2d</div>  \n",
        "\n",
        "<div dir=\"rtl\">* \u0627\u0644\u062a\u0637\u0628\u064a\u0639 BatchNorm2d</div>  \n",
        "\n",
        "<div dir=\"rtl\">* \u0645\u062c\u0645\u0648\u0639\u0629 \u0645\u0646 \u0648\u0638\u0627\u0626\u0641 \u0627\u0644\u062a\u0646\u0634\u064a\u0637 ReLU \u0648 Softmax \u0648 Tanh</div>  \n",
        "\n",
        "<div dir=\"rtl\">* \u0637\u0628\u0642\u0627\u062a \u062a\u0646\u0638\u064a\u0645\u060c \u0645\u062b\u0644 Dropout</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "    <h4>\u0641\u064a \u0647\u0630\u0647 \u0627\u0644\u062a\u062c\u0631\u0628\u0629 \u0627\u0644\u0639\u0645\u0644\u064a\u0629\u060c \u0633\u0648\u0641 \u0646\u062f\u0631\u0633 \u0641\u0642\u0637 \u0643\u062a\u0644\u062a\u064a\u0646 \u0645\u0646 \u0643\u062a\u0644 \u0627\u0644\u0634\u0628\u0643\u0629 \u0627\u0644\u0639\u0635\u0628\u064a\u0629 \u0645\u0646 \u0627\u0644\u0642\u0627\u0626\u0645\u0629 \u0623\u0639\u0644\u0627\u0647\u060c \u0648\u0647\u0645\u0627 Linear \u0648 ReLU.</h4>\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\" style=\"margin-top: 20px;\">\n",
        "\n",
        "    \u064a\u0645\u0643\u0646 \u062a\u0639\u064a\u064a\u0646 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0628\u0637\u0631\u064a\u0642\u062a\u064a\u0646:\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\" style=\"margin-top: 20px;\">\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">1. \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 nn.Sequential</div>\n",
        "\n",
        "<div dir=\"rtl\">2. \u0645\u0646 \u062e\u0644\u0627\u0644 \u0648\u0631\u0627\u062b\u0629 \u0627\u0644\u0641\u0626\u0629 nn.Module</div>\n",
        "\n",
        "\n",
        "\n",
        "<div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "    <div dir=\"rtl\">\u0627\u0644\u0637\u0631\u064a\u0642\u0629 \u0627\u0644\u0623\u0648\u0644\u0649 \u0645\u0646\u0627\u0633\u0628\u0629 \u0644\u0625\u0646\u0634\u0627\u0621 \u0646\u0645\u0627\u0630\u062c \u0628\u0633\u064a\u0637\u0629 \u0628\u062f\u0648\u0646 \u062a\u0641\u0631\u0639\u0627\u062a. \u064a\u0645\u0643\u0646 \u062a\u0635\u0648\u0631\u0647\u0627 \u0623\u0633\u0627\u0633\u064b\u0627 \u0643\u062e\u0637 \u062a\u062c\u0645\u064a\u0639\u060c \u062d\u064a\u062b \u064a\u062a\u0645 \u062a\u0645\u0631\u064a\u0631 \u0627\u0644\u062a\u0646\u0633\u0648\u0631 \u0627\u0644\u0645\u062f\u062e\u0644 \u0639\u0628\u0631 \u0633\u0644\u0633\u0644\u0629 \u0645\u0646 \u0627\u0644\u062a\u062d\u0648\u064a\u0644\u0627\u062a \u0627\u0644\u0645\u062a\u0639\u0627\u0642\u0628\u0629 \u0644\u0644\u062d\u0635\u0648\u0644 \u0639\u0644\u0649 \u0627\u0644\u062a\u0646\u0633\u0648\u0631 \u0627\u0644\u0646\u0627\u062a\u062c.</div>\n",
        "\n",
        "    <div dir=\"rtl\">\u0625\u0630\u0627 \u0643\u0627\u0646 \u0645\u0646 \u0627\u0644\u0636\u0631\u0648\u0631\u064a \u062a\u0637\u0628\u064a\u0642 \u0647\u064a\u0627\u0643\u0644 \u0623\u0643\u062b\u0631 \u062a\u0639\u0642\u064a\u062f\u064b\u0627\u060c \u062d\u064a\u062b \u064a\u0645\u0643\u0646 \u0623\u0646 \u062a\u0646\u0642\u0633\u0645 \u0645\u0633\u0627\u0631\u0627\u062a \u062e\u0637 \u0627\u0644\u062a\u062c\u0645\u064a\u0639 \u0625\u0644\u0649 \u0639\u062f\u0629 \u0623\u062c\u0632\u0627\u0621\u060c \u064a\u062a\u0645 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 nn.Module. \u064a\u0633\u0645\u062d \u0647\u0630\u0627 \u0627\u0644\u0646\u0647\u062c \u0628\u062a\u0646\u0641\u064a\u0630 \u0645\u062c\u0645\u0648\u0639\u0629 \u0645\u062a\u0646\u0648\u0639\u0629 \u0645\u0646 \u0627\u0644\u0647\u064a\u0627\u0643\u0644.</div>\n",
        "\n",
        "    <div dir=\"rtl\">\u0644\u0625\u0646\u0634\u0627\u0621 \u0628\u064a\u0631\u0633\u064a\u0628\u062a\u0631\u0648\u0646 \u0645\u062a\u0639\u062f\u062f \u0627\u0644\u0637\u0628\u0642\u0627\u062a \u0628\u0633\u064a\u0637 \u0645\u0639 \u0637\u0628\u0642\u0629 \u0645\u062e\u0641\u064a\u0629 \u0648\u0627\u062d\u062f\u0629 \u0648\u062f\u0627\u0644\u0629 \u063a\u064a\u0631 \u062e\u0637\u064a\u0629\u060c \u064a\u0643\u0641\u064a \u0643\u062a\u0627\u0628\u0629 \u0627\u0644\u0643\u0648\u062f \u0627\u0644\u062a\u0627\u0644\u064a \u0648\u0641\u0642\u064b\u0627 \u0644\u0644\u0637\u0631\u064a\u0642\u0629 \u0627\u0644\u0623\u0648\u0644\u0649:</div>\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "    model = nn.Sequential(\n",
        "\n",
        "      nn.Linear(input_dims, hidden_dims),\n",
        "\n",
        "      nn.ReLU(),\n",
        "\n",
        "      nn.Linear(hidden_dims, num_classes) \n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\u0644\u0625\u0646\u0634\u0627\u0621 \u0628\u064a\u0631\u0633\u064a\u0628\u062a\u0631\u0648\u0646 \u0645\u062a\u0639\u062f\u062f \u0627\u0644\u0637\u0628\u0642\u0627\u062a \u0628\u0633\u064a\u0637 \u0645\u0639 \u0637\u0628\u0642\u0629 \u0645\u062e\u0641\u064a\u0629 \u0648\u0627\u062d\u062f\u0629 \u0648\u062f\u0627\u0644\u0629 \u063a\u064a\u0631 \u062e\u0637\u064a\u0629\u060c \u0648\u0641\u0642\u064b\u0627 \u0644\u0644\u0637\u0631\u064a\u0642\u0629 \u0627\u0644\u062b\u0627\u0646\u064a\u0629\u060c \u064a\u062c\u0628 \u0625\u0646\u0634\u0627\u0621 \u0641\u0626\u0629 (class) \u0648\u0646\u0645\u0648\u0630\u062c (model) \u0643\u0643\u0627\u0626\u0646 \u0645\u0646 \u0647\u0630\u0647 \u0627\u0644\u0641\u0626\u0629.</div>\n",
        "\n",
        "\n",
        "\n",
        "    class MLP(nn.Module):\n",
        "\n",
        "        def __init__(self, input_dims, hidden_dims, num_classes,\n",
        "\n",
        "                     *args, **kwargs):\n",
        "\n",
        "            super(MLP, self).__init__()\n",
        "\n",
        "            self.fc1 = Linear(input_dims, hidden_dims)\n",
        "\n",
        "            self.fc2 = Linear(hidden_dims, num_classes)\n",
        "\n",
        "        \n",
        "\n",
        "        def forward(self, input):\n",
        "\n",
        "             x = self.fc1(input)\n",
        "\n",
        "             x = F.relu(x)\n",
        "\n",
        "             x = self.fc2(x)\n",
        "\n",
        "             return x\n",
        "\n",
        "    \n",
        "\n",
        "    model = MLP(input_dims, hidden_dims, num_classes) \n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\u0641\u064a \u0647\u0630\u0647 \u0627\u0644\u062d\u0627\u0644\u0629\u060c \u064a\u064f\u0633\u0645\u062d \u0628\u062a\u0636\u0645\u064a\u0646 nn.Module \u0648 nn.Sequential \u062f\u0627\u062e\u0644 \u0648\u062d\u062f\u0627\u062a (modules) \u0623\u062e\u0631\u0649\u060c \u0645\u0645\u0627 \u064a\u0633\u0645\u062d \u0628\u0625\u0646\u0634\u0627\u0621 \u0628\u0646\u0649 \u0646\u0645\u0627\u0630\u062c \u0645\u0639\u0642\u062f\u0629 \u062c\u062f\u064b\u0627.</div>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "    <h4>\u062a\u062f\u0631\u064a\u0628 \u0627\u0644\u0646\u0645\u0627\u0630\u062c:</h4>\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\u0642\u0628\u0644 \u062a\u062f\u0631\u064a\u0628 \u0627\u0644\u0646\u0645\u0627\u0630\u062c\u060c \u0645\u0646 \u0627\u0644\u0636\u0631\u0648\u0631\u064a \u0627\u062e\u062a\u064a\u0627\u0631 \u062f\u0627\u0644\u0629 \u062e\u0633\u0627\u0631\u0629 (Loss function) \u0648\u0645\u064f\u062d\u0633\u0651\u0646 (Optimizer). \u062a\u062a\u0648\u0641\u0631 \u062f\u0648\u0627\u0644 \u062e\u0633\u0627\u0631\u0629 \u0645\u062e\u062a\u0644\u0641\u0629 \u0623\u064a\u0636\u064b\u0627 \u0641\u064a \u0648\u062d\u062f\u0629 nn:</div>\n",
        "\n",
        "<div dir=\"rtl\">* __nn.MSELoss__ - \u0645\u062a\u0648\u0633\u0637 \u0627\u0644\u062e\u0637\u0623 \u0627\u0644\u062a\u0631\u0628\u064a\u0639\u064a (y_true - y_pred)**2</div>\n",
        "\n",
        "<div dir=\"rtl\">* __nn.BCEWithLogitsLoss__ - \u0627\u0644\u0627\u0646\u062a\u0631\u0648\u0628\u064a\u0627 \u0627\u0644\u0645\u062a\u0642\u0627\u0637\u0639\u0629 \u0627\u0644\u062b\u0646\u0627\u0626\u064a\u0629 \u0644\u0645\u0647\u0627\u0645 \u0627\u0644\u062a\u0635\u0646\u064a\u0641 \u0627\u0644\u062b\u0646\u0627\u0626\u064a</div>\n",
        "\n",
        "<div dir=\"rtl\">* __nn.CrossEntropyLoss__ - \u0627\u0644\u0627\u0646\u062a\u0631\u0648\u0628\u064a\u0627 \u0627\u0644\u0645\u062a\u0642\u0627\u0637\u0639\u0629 \u0627\u0644\u0641\u0626\u0648\u064a\u0629 \u0644\u0645\u0647\u0627\u0645 \u0627\u0644\u062a\u0635\u0646\u064a\u0641 \u0645\u062a\u0639\u062f\u062f \u0627\u0644\u0641\u0626\u0627\u062a</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\u0643\u0628\u062f\u064a\u0644\u060c \u064a\u0645\u0643\u0646 \u062a\u0646\u0641\u064a\u0630 \u062f\u0627\u0644\u0629 \u062e\u0633\u0627\u0631\u0629 (Loss function) \u064a\u062f\u0648\u064a\u064b\u0627\u060c \u0639\u0644\u0649 \u0633\u0628\u064a\u0644 \u0627\u0644\u0645\u062b\u0627\u0644 \u0644\u0640 MSELoss:</div>\n",
        "\n",
        "\n",
        "\n",
        "    inputs, y = batch\n",
        "\n",
        "    ...\n",
        "\n",
        "    output = model(inputs)\n",
        "\n",
        "    loss = ((output - y)**2).sum()\n",
        "\n",
        "    ...\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "    <h4>\u0627\u0644\u0645\u064f\u062d\u0633\u0651\u0650\u0646\u0627\u062a:</h4>\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\u062a\u062d\u062a\u0648\u064a \u0648\u062d\u062f\u0629 __torch.optim__ \u0639\u0644\u0649 \u0627\u0644\u0645\u064f\u062d\u0633\u0651\u0650\u0646\u0627\u062a. \u0647\u0646\u0627\u0643 \u0627\u0644\u0639\u062f\u064a\u062f \u0645\u0646 \u0627\u0644\u0645\u064f\u062d\u0633\u0651\u0650\u0646\u0627\u062a \u0644\u0644\u062f\u0627\u0644\u0629 \u0627\u0644\u0647\u062f\u0641\u060c \u0648\u0645\u0646 \u0627\u0644\u0643\u0644\u0627\u0633\u064a\u0643\u064a\u064a\u0646 \u0647\u0648 \u0637\u0631\u064a\u0642\u0629 \u0627\u0644\u0646\u0632\u0648\u0644 \u0627\u0644\u0639\u0634\u0648\u0627\u0626\u064a \u0644\u0644\u062a\u062f\u0631\u062c (Stochastic Gradient Descent) \u0623\u0648 SGD. \u0641\u064a \u0645\u064f\u0646\u0634\u0626 \u0627\u0644\u0641\u0626\u0629\u060c \u064a\u062c\u0628 \u062a\u0645\u0631\u064a\u0631 \u0623\u0648\u0632\u0627\u0646 \u0627\u0644\u0646\u0645\u0648\u0630\u062c\u060c \u0628\u0627\u0644\u0625\u0636\u0627\u0641\u0629 \u0625\u0644\u0649 \u062a\u062d\u062f\u064a\u062f \u062e\u0637\u0648\u0629 \u0627\u0644\u062a\u0639\u0644\u0645 \u0623\u0648 learning rate.</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\u0644\u062a\u062d\u0648\u064a\u0644 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0625\u0644\u0649 \u062d\u0627\u0644\u0629 \u0627\u0644\u062a\u062f\u0631\u064a\u0628\u060c \u064a\u062c\u0628 \u0627\u0633\u062a\u062f\u0639\u0627\u0621 \u0627\u0644\u0637\u0631\u064a\u0642\u0629 __train__. \u0628\u0639\u062f \u0630\u0644\u0643\u060c \u064a\u0643\u0648\u0646 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u062c\u0627\u0647\u0632\u064b\u0627 \u0644\u0644\u062a\u062f\u0631\u064a\u0628.</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\u0644\u062a\u062f\u0631\u064a\u0628 \u0646\u0645\u0627\u0630\u062c \u0627\u0644\u0634\u0628\u0643\u0627\u062a \u0627\u0644\u0639\u0635\u0628\u064a\u0629\u060c \u064a\u062a\u0645 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0627\u0644\u0646\u0632\u0648\u0644 \u0627\u0644\u062a\u062f\u0631\u062c\u064a \u0648\u0623\u0646\u0648\u0627\u0639\u0647\u060c \u0648\u0627\u0644\u062a\u064a \u062a\u0633\u062a\u0646\u062f \u0625\u0644\u0649 \u0637\u0631\u064a\u0642\u0629 \u0627\u0644\u0627\u0642\u062a\u0631\u0627\u0628\u0627\u062a \u0627\u0644\u0645\u062a\u062a\u0627\u0644\u064a\u0629.</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\u062e\u0644\u0627\u0644 \u0639\u0635\u0631 \u0648\u0627\u062d\u062f\u060c \u064a\u062a\u0645 \u0627\u062e\u062a\u064a\u0627\u0631 \u0645\u0631\u0648\u0631 \u0627\u0644\u0645\u064f\u0643\u0631\u0631 \u0639\u0628\u0631 \u0645\u062c\u0645\u0648\u0639\u0629 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0628\u0627\u0644\u0643\u0627\u0645\u0644\u060c \u0648\u0641\u064a \u0643\u0644 \u062a\u0643\u0631\u0627\u0631 - \u064a\u062a\u0645 \u062a\u062d\u0633\u064a\u0646 \u0645\u0639\u0644\u0645\u0627\u062a \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0627\u0644\u062f\u0641\u0639\u0629 \u0627\u0644\u062d\u0627\u0644\u064a\u0629 \u0627\u0644\u0635\u063a\u064a\u0631\u0629 (mini-batch). \u064a\u0642\u0648\u0645 PyTorch \u062a\u0644\u0642\u0627\u0626\u064a\u064b\u0627 \u0628\u062d\u0633\u0627\u0628 \u0627\u0644\u0645\u0634\u062a\u0642\u0627\u062a \u0639\u0646\u062f \u0627\u0633\u062a\u062f\u0639\u0627\u0621 \u0627\u0644\u0637\u0631\u064a\u0642\u0629 __backward__\u060c \u0627\u0644\u062a\u064a \u062a\u0645 \u062a\u0637\u0628\u064a\u0642\u0647\u0627 \u0639\u0644\u0649 \u062f\u0627\u0644\u0629 \u0627\u0644\u062e\u0633\u0627\u0631\u0629.</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\u0645\u0639 \u0630\u0644\u0643\u060c \u0639\u0646\u062f \u0627\u0633\u062a\u062f\u0639\u0627\u0621 \u0645\u064f\u062c\u062f\u0651\u064e\u062f\u060c \u0633\u062a\u0636\u0627\u0641 \u0642\u064a\u0645 \u0627\u0644\u0645\u0634\u062a\u0642\u0627\u062a \u0627\u0644\u062c\u062f\u064a\u062f\u0629 \u0625\u0644\u0649 \u0627\u0644\u0645\u0634\u062a\u0642\u0627\u062a \u0627\u0644\u0633\u0627\u0628\u0642\u0629 \u0627\u0644\u0645\u062d\u0633\u0648\u0628\u0629. \u0644\u0630\u0644\u0643\u060c \u0644\u062a\u062c\u0646\u0628 \u0627\u0644\u062a\u0623\u062b\u064a\u0631\u0627\u062a \u063a\u064a\u0631 \u0627\u0644\u0645\u0631\u063a\u0648\u0628 \u0641\u064a\u0647\u0627\u060c \u0645\u0646 \u0627\u0644\u0645\u0639\u062a\u0627\u062f \u062a\u0646\u0638\u064a\u0641 \u0627\u0644\u0642\u064a\u0645 \u0627\u0644\u0633\u0627\u0628\u0642\u0629 \u0644\u0644\u0645\u0634\u062a\u0642\u0627\u062a \u0641\u064a \u0643\u0644 \u062a\u0643\u0631\u0627\u0631 \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0627\u0644\u0637\u0631\u064a\u0642\u0629 __zero_grad__\u060c \u0627\u0644\u062a\u064a \u062a\u0645 \u062a\u0637\u0628\u064a\u0642\u0647\u0627 \u0639\u0644\u0649 \u0645\u062b\u064a\u0644 \u0645\u064f\u062d\u0633\u0651\u0650\u0646 \u0627\u0644\u0641\u0626\u0629.</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\u0644\u062a\u062d\u062f\u064a\u062b \u0645\u0639\u0644\u0645\u0627\u062a \u0627\u0644\u0634\u0628\u0643\u0629 \u0627\u0644\u0639\u0635\u0628\u064a\u0629\u060c \u062a\u064f\u0633\u062a\u062e\u062f\u0645 \u0627\u0644\u0637\u0631\u064a\u0642\u0629 __step__\u060c \u0627\u0644\u062a\u064a \u062a\u0645 \u062a\u0637\u0628\u064a\u0642\u0647\u0627 \u0639\u0644\u0649 \u0645\u062b\u064a\u0644 \u0645\u064f\u062d\u0633\u0651\u0650\u0646 \u0627\u0644\u0641\u0626\u0629.</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "    <h4>\u062a\u062d\u0642\u0642 \u0645\u0646 \u062c\u0648\u062f\u0629 \u0627\u0644\u0646\u0645\u0627\u0630\u062c:</h4>\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\u0644\u062a\u062d\u0648\u064a\u0644 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0625\u0644\u0649 \u062d\u0627\u0644\u0629 \u0627\u0644\u062a\u062d\u0642\u0642\u060c \u064a\u062c\u0628 \u0627\u0633\u062a\u062f\u0639\u0627\u0621 \u0627\u0644\u0637\u0631\u064a\u0642\u0629 __eval__. \u0628\u0639\u062f \u0630\u0644\u0643\u060c \u064a\u0643\u0648\u0646 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u062c\u0627\u0647\u0632\u064b\u0627 \u0644\u0644\u062a\u062d\u0642\u0642.</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\u064a\u062c\u0628 \u0642\u0637\u0639 \u0627\u0644\u062a\u0648\u062a\u0631 \u0627\u0644\u0646\u0627\u062a\u062c \u0639\u0646 \u062a\u0646\u0628\u0624\u0627\u062a \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0639\u0646 \u0627\u0644\u0631\u0633\u0645 \u0627\u0644\u0628\u064a\u0627\u0646\u064a \u0627\u0644\u062d\u0633\u0627\u0628\u064a. \u064a\u062a\u0645 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0627\u0644\u0637\u0631\u064a\u0642\u0629 __detach__\u060c \u0627\u0644\u062a\u064a \u062a\u0645 \u062a\u0637\u0628\u064a\u0642\u0647\u0627 \u0639\u0644\u0649 \u0627\u0644\u062a\u0648\u062a\u0631 \u0627\u0644\u0646\u0627\u062a\u062c \u0639\u0646 \u0627\u0644\u0646\u0645\u0648\u0630\u062c. \u062e\u0644\u0627\u0641 \u0630\u0644\u0643\u060c \u0642\u062f \u062a\u062d\u062f\u062b \u062a\u0633\u0631\u064a\u0628\u0627\u062a \u0641\u064a \u0627\u0644\u0630\u0627\u0643\u0631\u0629. \u062a\u0642\u0648\u0645 \u0627\u0644\u0637\u0631\u064a\u0642\u0629 __numpy__ \u0628\u062a\u062d\u0648\u064a\u0644 \u0627\u0644\u062a\u0648\u062a\u0631 \u0625\u0644\u0649 \u0645\u0635\u0641\u0648\u0641\u0629 NumPy \u0645\u062a\u0639\u062f\u062f\u0629 \u0627\u0644\u0623\u0628\u0639\u0627\u062f.</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\u0628\u0634\u0643\u0644 \u0627\u0641\u062a\u0631\u0627\u0636\u064a\u060c \u064a\u0642\u0648\u0645 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0628\u0625\u062e\u0631\u0627\u062c \u0645\u0627 \u064a\u064f\u0639\u0631\u0641 \u0628\u0640 \"\u0644\u0648\u062c\u064a\u062a\u0633\" (logits) \u0644\u0644\u0641\u0626\u0627\u062a\u060c \u0648\u0644\u064a\u0633 \u0627\u062d\u062a\u0645\u0627\u0644\u0627\u062a\u0647\u0627. \u0644\u0644\u062d\u0635\u0648\u0644 \u0639\u0644\u0649 \u0627\u0644\u0627\u062d\u062a\u0645\u0627\u0644\u0627\u062a\u060c \u064a\u062c\u0628 \u062a\u0637\u0628\u064a\u0642 \u062f\u0627\u0644\u0629 \u0627\u0644\u062a\u0646\u0634\u064a\u0637 __Softmax__. \u0648\u0645\u0639 \u0630\u0644\u0643\u060c \u0641\u064a \u0627\u0644\u0645\u0645\u0627\u0631\u0633\u0629 \u0627\u0644\u0639\u0645\u0644\u064a\u0629\u060c \u0644\u064a\u0633 \u0645\u0646 \u0627\u0644\u0636\u0631\u0648\u0631\u064a \u0630\u0644\u0643\u060c \u062d\u064a\u062b \u0623\u0646 \u0642\u064a\u0645\u0629 \u0627\u0644\u0644\u0648\u062c\u064a\u062a\u0633 \u062a\u062a\u0648\u0627\u0641\u0642 \u0645\u0639 \u0627\u062d\u062a\u0645\u0627\u0644\u0627\u062a \u0627\u0644\u0641\u0626\u0627\u062a\u060c \u0648\u0644\u0644\u062d\u0635\u0648\u0644 \u0639\u0644\u0649 \u0631\u0642\u0645 \u0627\u0644\u0641\u0626\u0629 \u0627\u0644\u0623\u0643\u062b\u0631 \u0627\u062d\u062a\u0645\u0627\u0644\u064b\u0627\u060c \u064a\u0645\u0643\u0646 \u062a\u062e\u0637\u064a \u0647\u0630\u0647 \u0627\u0644\u062e\u0637\u0648\u0629. \u064a\u062a\u0645 \u0627\u0644\u062d\u0635\u0648\u0644 \u0639\u0644\u0649 \u0631\u0642\u0645 \u0627\u0644\u0641\u0626\u0629 \u0625\u0645\u0627 \u0645\u0646 \u062e\u0644\u0627\u0644 \u0627\u0644\u0637\u0631\u064a\u0642\u0629 __argmax__ \u0623\u0648 \u0645\u0646 \u062e\u0644\u0627\u0644 \u0627\u0644\u0637\u0631\u064a\u0642\u0629 __argsort__\u060c \u062d\u064a\u062b \u062a\u0633\u0645\u062d \u0627\u0644\u0623\u062e\u064a\u0631\u0629 \u0628\u062d\u0633\u0627\u0628 \u0645\u0642\u0627\u064a\u064a\u0633 \u0645\u062b\u0644 Accuracy@5 \u0648\u0645\u0642\u0627\u064a\u064a\u0633 \u0627\u0644\u062a\u0631\u062a\u064a\u0628.</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "    <h4>\u0627\u0633\u062a\u064a\u0631\u0627\u062f \u0627\u0644\u0645\u0643\u062a\u0628\u0627\u062a \u0627\u0644\u0636\u0631\u0648\u0631\u064a\u0629</h4>\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        " In[1]:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "811a1f17",
      "metadata": {},
      "outputs": [],
      "source": [
        "    import numpy as np\n",
        "\n",
        "    import torch\n",
        "\n",
        "    import torch.optim as optim\n",
        "\n",
        "    import torch.nn as nn\n",
        "\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "    import pickle\n",
        "\n",
        "    from sklearn.metrics import classification_report\n",
        "\n",
        "    from sklearn.datasets import make_circles, make_moons\n",
        "\n",
        "    from PIL import Image\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    %matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c335435c",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "    <h1>\u0627\u0644\u062c\u0632\u0621 1. \u0645\u0634\u0643\u0644\u0629 \u0627\u0644\u0627\u0646\u062d\u062f\u0627\u0631 \u0648\u0641\u0642\u064b\u0627 \u0644\u0646\u0638\u0631\u064a\u0629 \u0627\u0644\u062a\u0642\u0631\u064a\u0628 \u0627\u0644\u0639\u0627\u0644\u0645\u064a\u0629\u060c \u0627\u0644\u0627\u0634\u062a\u0642\u0627\u0642 \u0627\u0644\u064a\u062f\u0648\u064a</h1>\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\"><h2>\u062a\u0648\u0644\u064a\u062f \u0627\u0644\u0639\u064a\u0646\u0629 \u0648\u062a\u0647\u064a\u0626\u0629 \u0645\u0639\u0644\u0645\u0627\u062a \u0627\u0644\u0634\u0628\u0643\u0629 \u0627\u0644\u0639\u0635\u0628\u064a\u0629</h2> </div>\n",
        "\n",
        "\n",
        "\n",
        " In[2]:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc2400b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "    X = (np.arange(100)/100 - 0.5).repeat(5)\n",
        "\n",
        "    \n",
        "\n",
        "    y = 1/(1+np.exp(-10*X))\n",
        "\n",
        "    yn = np.random.normal(scale=0.05, size=y.size)+y\n",
        "\n",
        "    \n",
        "\n",
        "    plt.plot(X, yn)\n",
        "\n",
        "    plt.plot(X, y, linestyle='--', c='k')\n",
        "\n",
        "    ################################################\n",
        "\n",
        "    tensor_X = torch.Tensor(X.reshape(-1, 1))\n",
        "\n",
        "    tensor_y = torch.Tensor(yn.reshape(-1, 1))\n",
        "\n",
        "    \n",
        "\n",
        "    HIDDEN_SIZE = 64\n",
        "\n",
        "    # \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0435\u0441\u043e\u0432 MLP \u0441 \u043e\u0434\u043d\u0438\u043c \u0441\u043a\u0440\u044b\u0442\u044b\u043c \u0441\u043b\u043e\u0451\u043c\n",
        "\n",
        "    weights_1 = (torch.rand(1, HIDDEN_SIZE)-.5)/10\n",
        "\n",
        "    bias_1 = torch.zeros(HIDDEN_SIZE)\n",
        "\n",
        "    \n",
        "\n",
        "    weights_2 = (torch.rand(HIDDEN_SIZE, 1)-.5)/10\n",
        "\n",
        "    bias_2 = torch.zeros(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2ea756b",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "<div dir=\"rtl\" style=\"margin-top: 70px;\">\n",
        "\n",
        "    <img align=\"right\" src=\"Recources/pic1.png\" height=\"300px\" width=\"580px\"/>  \n",
        "\n",
        "    <hr style=\"margin-top: 70px; margin-bottom: 70px;\"> \n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "<div style=\"height: 80px;\"></div>\n",
        "\n",
        "\n",
        "\n",
        "<br><br><br> <br><br><br> <br><br><br> <br><br>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\" style=\"margin-top: 40px;\">\n",
        "\n",
        "    <h3>\u062a\u062f\u0631\u064a\u0628 \u0627\u0644\u0634\u0628\u0643\u0629 \u0627\u0644\u0639\u0635\u0628\u064a\u0629 \u0644\u0645\u0634\u0643\u0644\u0629 \u0627\u0644\u0627\u0646\u062d\u062f\u0627\u0631</h3>\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "In[3]:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de9482b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "   # Define the non-linearity function\n",
        "\n",
        "relu = lambda x: torch.maximum(x, torch.Tensor([0]))\n",
        "\n",
        "# Forward pass\n",
        "\n",
        "forward = lambda x: (weights_2.t()*relu((weights_1*x) + bias_1)\n",
        "\n",
        "                      ).sum(axis=-1, keepdims=True) + bias_2\n",
        "\n",
        "loss = lambda y, y_: ((y-y_)**2).sum(axis=-1)\n",
        "\n",
        "\n",
        "\n",
        "# Backward pass\n",
        "\n",
        "def backward(X, y, y_pred):\n",
        "\n",
        "    # Derivative of the loss function with respect to y_pred\n",
        "\n",
        "    dL = 2*(y_pred-y)\n",
        "\n",
        "    # Values of the neurons in the hidden layer before applying activation\n",
        "\n",
        "    Ax = (weights_1*X) + bias_1\n",
        "\n",
        "    # Values of the neurons in the hidden layer after applying activation\n",
        "\n",
        "    A = relu(Ax)\n",
        "\n",
        "    # Derivative of the loss function with respect to weight 2\n",
        "\n",
        "    dW2 = torch.mm(A.t(), dL)\n",
        "\n",
        "    # Derivative of the loss function with respect to bias 2\n",
        "\n",
        "    db2 = dL.sum(axis=0)\n",
        "\n",
        "    # Derivative of the loss function with respect to the values of the hidden layer after activation\n",
        "\n",
        "    dA = torch.mm(dL, weights_2.t())\n",
        "\n",
        "    # Derivative of the loss function with respect to the values of the hidden layer before activation\n",
        "\n",
        "    dA[Ax <= 0] = 0\n",
        "\n",
        "    # Derivative of the loss function with respect to weight 1\n",
        "\n",
        "    dW = torch.mm(X.t(), dA)\n",
        "\n",
        "    # Derivative of the loss function with respect to bias 1\n",
        "\n",
        "    db = dA.sum(axis=0)\n",
        "\n",
        "    return dW, db, dW2, db2\n",
        "\n",
        "\n",
        "\n",
        "def optimize(params, grads, lr=0.001):\n",
        "\n",
        "    # Gradient descent over the entire training set\n",
        "\n",
        "    W1, b1, W2, b2 = params\n",
        "\n",
        "    W1 -= lr * grads[0]\n",
        "\n",
        "    W2 -= lr * grads[2]\n",
        "\n",
        "    b1 -= lr * grads[1]\n",
        "\n",
        "    b2 -= lr * grads[3]\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "\n",
        "\n",
        "for i in range(50000): # 50,000 iterations of gradient descent == 50,000 epochs\n",
        "\n",
        "    output = forward(tensor_X)\n",
        "\n",
        "    cur_loss = loss(output, tensor_y)\n",
        "\n",
        "    grads = backward(tensor_X, tensor_y, output)\n",
        "\n",
        "    params = [weights_1, bias_1, weights_2, bias_2]\n",
        "\n",
        "    weights_1, bias_1, weights_2, bias_2 = optimize(params, grads, 1e-4)\n",
        "\n",
        "    if (i + 1) % 10000 == 0:\n",
        "\n",
        "        plt.plot(X, output.numpy(), label=str(i + 1), alpha=0.5)\n",
        "\n",
        "plt.plot(X, y, linestyle='--', c='k', label='real')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.ylim(y.min(), y.max())\n",
        "\n",
        "print(cur_loss.numpy().mean())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3beebbf",
      "metadata": {},
      "source": [
        "0.002543415\n",
        "\n",
        "\n",
        "\n",
        " <img align=\"right\" src=\"Recources/pic2.png\" height=\"300px\" width=\"580px\"/>  \n",
        "\n",
        "\n",
        "\n",
        "<br><br><br> <br><br><br> <br><br><br> <br><br><br><br><br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# \u0627\u0644\u062c\u0632\u0621 2. \u0627\u0644\u062a\u0635\u0646\u064a\u0641 \u0627\u0644\u062b\u0646\u0627\u0626\u064a \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0627\u0644\u062a\u0641\u0627\u0636\u0644 \u0627\u0644\u0622\u0644\u064a \u0641\u064a PyTorch\n",
        "\n",
        "\n",
        "\n",
        "## \u062a\u0648\u0644\u064a\u062f \u0627\u0644\u0639\u064a\u0646\u0629 \u0648\u062a\u0647\u064a\u0626\u0629 \u0645\u0639\u0644\u0645\u0627\u062a \u0627\u0644\u0634\u0628\u0643\u0629 \u0627\u0644\u0639\u0635\u0628\u064a\u0629\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In[4]:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88db7e47",
      "metadata": {},
      "outputs": [],
      "source": [
        "    X = np.random.randint(2, size=(1000, 2))\n",
        "\n",
        "    \n",
        "\n",
        "    y = (X[:, 0] + X[:, 1]) % 2 # XOR\n",
        "\n",
        "    X = X + np.random.normal(0, scale=0.1, size=X.shape)\n",
        "\n",
        "    #X, y = make_circles(n_samples=1000, noise=0.025)\n",
        "\n",
        "    #X, y = make_moons(n_samples=1000, noise=0.025)\n",
        "\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y)\n",
        "\n",
        "    ####################################################\n",
        "\n",
        "    tensor_X = torch.Tensor(X.reshape(-1, 2))\n",
        "\n",
        "    tensor_y = torch.Tensor(y.reshape(-1, 1))\n",
        "\n",
        "    \n",
        "\n",
        "    HIDDEN_SIZE = 16\n",
        "\n",
        "    # Initialize MLP weights with one hidden layer\n",
        "\n",
        "    weights_1 = ((torch.rand(2, HIDDEN_SIZE)-.5)/10).detach().requires_grad_(True)\n",
        "\n",
        "    bias_1 = torch.zeros(HIDDEN_SIZE, requires_grad=True)\n",
        "\n",
        "    \n",
        "\n",
        "    weights_2 = ((torch.rand(HIDDEN_SIZE, 1)-.5)/10).detach().requires_grad_(True)\n",
        "\n",
        "    bias_2 = torch.zeros(1, requires_grad=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24e4449c",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        " <img align=\"right\" src=\"Recources/pic3.png\" height=\"300px\" width=\"580px\"/>  \n",
        "\n",
        "\n",
        "\n",
        "<br><br><br> <br><br><br> <br><br><br> <br><br><br><br><br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## \u062a\u062f\u0631\u064a\u0628 \u0627\u0644\u0634\u0628\u0643\u0629 \u0627\u0644\u0639\u0635\u0628\u064a\u0629 \u0644\u0645\u0647\u0645\u0629 \u0627\u0644\u062a\u0635\u0646\u064a\u0641\n",
        "\n",
        "\n",
        "\n",
        "In[5]:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cb9fed4",
      "metadata": {},
      "outputs": [],
      "source": [
        "    # Define the nonlinearity function\n",
        "\n",
        "def sigmoid(x):\n",
        "\n",
        "    return 1/(1+torch.exp(-x))\n",
        "\n",
        "\n",
        "\n",
        "# Forward pass\n",
        "\n",
        "def forward(x):\n",
        "\n",
        "    hidden = torch.mm(x, weights_1) + bias_1\n",
        "\n",
        "    hidden_nonlin = sigmoid(hidden)\n",
        "\n",
        "    output = (weights_2.t()*hidden_nonlin).sum(axis=-1, keepdims=True) + bias_2\n",
        "\n",
        "    return sigmoid(output)\n",
        "\n",
        "\n",
        "\n",
        "# Log loss\n",
        "\n",
        "def loss(y_true, y_pred):\n",
        "\n",
        "    return -1 * (y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred)).sum()\n",
        "\n",
        "\n",
        "\n",
        "# Set the learning rate\n",
        "\n",
        "lr = 1e-3\n",
        "\n",
        "# Set the number of iterations\n",
        "\n",
        "iters = 10000\n",
        "\n",
        "params = [weights_1, bias_1, weights_2, bias_2]\n",
        "\n",
        "losses = []\n",
        "\n",
        "\n",
        "\n",
        "for i in range(iters):\n",
        "\n",
        "    output = forward(tensor_X)\n",
        "\n",
        "    lossval = loss(tensor_y, output)\n",
        "\n",
        "    lossval.backward()  # This activates autograd\n",
        "\n",
        "    for w in params:\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            w -= w.grad * lr  # Update weights\n",
        "\n",
        "        w.grad.zero_()  # Zero out gradients to prevent accumulation over iterations\n",
        "\n",
        "    losses.append(lossval.item())\n",
        "\n",
        "\n",
        "\n",
        "# Output the loss history by iterations\n",
        "\n",
        "plt.plot(losses)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e74a913a",
      "metadata": {},
      "source": [
        "out[5]: [<matplotlib.lines.Line2D at 0x7ff170558b10>]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " <img align=\"right\" src=\"Recources/pic4.png\" height=\"300px\" width=\"580px\"/>  \n",
        "\n",
        "\n",
        "\n",
        "<br><br><br> <br><br><br> <br><br><br> <br><br><br><br><br>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "    <h2>\u0627\u0644\u062a\u062d\u0642\u0642 \u0645\u0646 \u0646\u062a\u0627\u0626\u062c \u0627\u0644\u062a\u062f\u0631\u064a\u0628</h2>\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "In[6]:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6c27ac7",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_diff = X.max() - X.min()\n",
        "\n",
        "X_left = X.min() - 0.1 * X_diff\n",
        "\n",
        "X_right = X.max() + 0.1 * X_diff\n",
        "\n",
        "grid = np.arange(X_left, X_right, 0.01)\n",
        "\n",
        "grid_width = grid.size\n",
        "\n",
        "surface = []\n",
        "\n",
        "\n",
        "\n",
        "# Creating points along the grid\n",
        "\n",
        "for x1 in grid:\n",
        "\n",
        "    for x2 in grid:\n",
        "\n",
        "        surface.append((x1, x2))\n",
        "\n",
        "surface = np.array(surface)\n",
        "\n",
        "\n",
        "\n",
        "# Getting predictions for all grid points\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    Z = forward(torch.Tensor(surface)).detach().numpy()\n",
        "\n",
        "\n",
        "\n",
        "# Reshaping predictions into a 2D array\n",
        "\n",
        "Z = Z.reshape(grid_width, grid_width)\n",
        "\n",
        "xx = surface[:, 0].reshape(grid_width, grid_width)\n",
        "\n",
        "yy = surface[:, 1].reshape(grid_width, grid_width)\n",
        "\n",
        "\n",
        "\n",
        "# Plotting class separation surfaces\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.5)\n",
        "\n",
        "\n",
        "\n",
        "# Plotting training dataset\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=output.detach().numpy() > 0.5)\n",
        "\n",
        "\n",
        "\n",
        "# Setting the display boundaries for the plot\n",
        "\n",
        "plt.xlim(X_left, X_right)\n",
        "\n",
        "plt.ylim(X_left, X_right)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e84db9fc",
      "metadata": {},
      "source": [
        "out[6]: (-0.4265555623302785, 1.5671068095642362)\n",
        "\n",
        "\n",
        "\n",
        " <img align=\"right\" src=\"Recources/pic5.png\" height=\"300px\" width=\"580px\"/>  \n",
        "\n",
        "\n",
        "\n",
        "<br><br><br> <br><br><br> <br><br><br> <br><br><br><br><br>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "    <h1>\u0627\u0644\u062c\u0632\u0621 3. \u062a\u0635\u0646\u064a\u0641 \u0627\u0644\u0635\u0648\u0631 \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0645\u062c\u0645\u0648\u0639\u0629 \u0628\u064a\u0627\u0646\u0627\u062a CIFAR100</h1>\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "    <h2>\u062a\u062d\u0645\u064a\u0644 \u0648\u0641\u0643 \u0636\u063a\u0637 \u0645\u062c\u0645\u0648\u0639\u0629 \u0628\u064a\u0627\u0646\u0627\u062a CIFAR100</h2>\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In[7]: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf827559",
      "metadata": {},
      "outputs": [],
      "source": [
        "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "\n",
        "    !tar -xvzf cifar-100-python.tar.gz\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c70b7cb",
      "metadata": {},
      "source": [
        "--2022-02-10 21:29:10--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "\n",
        "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
        "\n",
        "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
        "\n",
        "HTTP request sent, awaiting response... 200 OK\n",
        "\n",
        "Length: 169001437 (161M) [application/x-gzip]\n",
        "\n",
        "Saving to: \u2018cifar-100-python.tar.gz\u2019\n",
        "\n",
        "\n",
        "\n",
        "cifar-100-python.ta 100%[===================>] 161.17M  46.1MB/s    in 3.9s    \n",
        "\n",
        "\n",
        "\n",
        "2022-02-10 21:29:14 (41.2 MB/s) - \u2018cifar-100-python.tar.gz\u2019 saved [169001437/169001437]\n",
        "\n",
        "\n",
        "\n",
        "- cifar-100-python/\n",
        "\n",
        "- cifar-100-python/file.txt~\n",
        "\n",
        "- cifar-100-python/train\n",
        "\n",
        "- cifar-100-python/test\n",
        "\n",
        "- cifar-100-python/meta\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "    <h2>\u0642\u0631\u0627\u0621\u0629 \u0628\u064a\u0627\u0646\u0627\u062a \u0627\u0644\u062a\u062f\u0631\u064a\u0628 \u0648\u0627\u0644\u0627\u062e\u062a\u0628\u0627\u0631</h2>\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "In[8]:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7b86f54",
      "metadata": {},
      "outputs": [],
      "source": [
        "    with open('cifar-100-python/train', 'rb') as f:\n",
        "\n",
        "        data_train = pickle.load(f, encoding='latin1')\n",
        "\n",
        "    with open('cifar-100-python/test', 'rb') as f:\n",
        "\n",
        "        data_test = pickle.load(f, encoding='latin1')\n",
        "\n",
        "    \n",
        "\n",
        "    # \u0417\u0434\u0435\u0441\u044c \u0443\u043a\u0430\u0437\u0430\u0442\u044c \u0432\u0430\u0448\u0438 \u043a\u043b\u0430\u0441\u0441\u044b \u043f\u043e \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0443!!!\n",
        "\n",
        "    CLASSES = [0, 55, 58]\n",
        "\n",
        "    \n",
        "\n",
        "    train_X = data_train['data'].reshape(-1, 3, 32, 32)\n",
        "\n",
        "    train_X = np.transpose(train_X, [0, 2, 3, 1]) # NCHW -> NHWC\n",
        "\n",
        "    train_y = np.array(data_train['fine_labels'])\n",
        "\n",
        "    mask = np.isin(train_y, CLASSES)\n",
        "\n",
        "    train_X = train_X[mask].copy()\n",
        "\n",
        "    train_y = train_y[mask].copy()\n",
        "\n",
        "    train_y = np.unique(train_y, return_inverse=1)[1]\n",
        "\n",
        "    del data_train\n",
        "\n",
        "    \n",
        "\n",
        "    test_X = data_test['data'].reshape(-1, 3, 32, 32)\n",
        "\n",
        "    test_X = np.transpose(test_X, [0, 2, 3, 1])\n",
        "\n",
        "    test_y = np.array(data_test['fine_labels'])\n",
        "\n",
        "    mask = np.isin(test_y, CLASSES)\n",
        "\n",
        "    test_X = test_X[mask].copy()\n",
        "\n",
        "    test_y = test_y[mask].copy()\n",
        "\n",
        "    test_y = np.unique(test_y, return_inverse=1)[1]\n",
        "\n",
        "    del data_test\n",
        "\n",
        "    Image.fromarray(train_X[50]).resize((256,256))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb035426",
      "metadata": {},
      "source": [
        "out[8]:\n",
        "\n",
        " <img align=\"left\" src=\"Recources/pic6.png\" height=\"300px\" width=\"380px\"/>  \n",
        "\n",
        "\n",
        "\n",
        "<br><br><br> <br><br><br> <br><br><br> <br><br><br><br><br>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "    <h2>\u0625\u0646\u0634\u0627\u0621 Pytorch DataLoader</h2>\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "In[9]:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c51baa1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "    batch_size = 128\n",
        "\n",
        "    dataloader = {}\n",
        "\n",
        "    for (X, y), part in zip([(train_X, train_y), (test_X, test_y)],\n",
        "\n",
        "                            ['train', 'test']):\n",
        "\n",
        "        tensor_x = torch.Tensor(X)\n",
        "\n",
        "        tensor_y = F.one_hot(torch.Tensor(y).to(torch.int64),\n",
        "\n",
        "                                         num_classes=len(CLASSES))/1.\n",
        "\n",
        "        dataset = TensorDataset(tensor_x, tensor_y) # \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\n",
        "\n",
        "        dataloader[part] = DataLoader(dataset, batch_size=batch_size, shuffle=True) # creating an instance of the DataLoader class\n",
        "\n",
        "    dataloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c30f987",
      "metadata": {},
      "source": [
        "out[9]:{'test': <torch.utils.data.dataloader.DataLoader at 0x7ff2823c59d0>,\n",
        "\n",
        "      'train': <torch.utils.data.dataloader.DataLoader at 0x7ff1706411d0>}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "    <h2>\u0625\u0646\u0634\u0627\u0621 \u0646\u0645\u0648\u0630\u062c \u0645\u062a\u0639\u062f\u062f \u0627\u0644\u0637\u0628\u0642\u0627\u062a Pytorch \u0645\u0639 \u0637\u0628\u0642\u0629 \u0645\u062e\u0641\u064a\u0629 \u0648\u0627\u062d\u062f\u0629</h2>\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "In[10]:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7201fc9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "    class Normalize(nn.Module):\n",
        "\n",
        "        def __init__(self, mean, std):\n",
        "\n",
        "            super(Normalize, self).__init__()\n",
        "\n",
        "            self.mean = torch.tensor(mean)\n",
        "\n",
        "            self.std = torch.tensor(std)\n",
        "\n",
        "    \n",
        "\n",
        "        def forward(self, input):\n",
        "\n",
        "            x = input / 255.0\n",
        "\n",
        "            x = x - self.mean\n",
        "\n",
        "            x = x / self.std\n",
        "\n",
        "            return torch.flatten(x, start_dim=1) # nhwc -> nm\n",
        "\n",
        "    \n",
        "\n",
        "    class Cifar100_MLP(nn.Module):\n",
        "\n",
        "        def __init__(self, hidden_size=32, classes=100):\n",
        "\n",
        "            super(Cifar100_MLP, self).__init__()\n",
        "\n",
        "            # https://blog.jovian.ai/image-classification-of-cifar100-dataset-using-pytorch-8b7145242df1\n",
        "\n",
        "            self.norm = Normalize([0.5074,0.4867,0.4411],[0.2011,0.1987,0.2025])\n",
        "\n",
        "            self.seq = nn.Sequential(\n",
        "\n",
        "                nn.Linear(32*32*3, hidden_size),\n",
        "\n",
        "                nn.ReLU(),\n",
        "\n",
        "                nn.Linear(hidden_size, classes),\n",
        "\n",
        "            )\n",
        "\n",
        "    \n",
        "\n",
        "        def forward(self, input):\n",
        "\n",
        "            x = self.norm(input)\n",
        "\n",
        "            return self.seq(x)\n",
        "\n",
        "    \n",
        "\n",
        "    HIDDEN_SIZE = 10\n",
        "\n",
        "    model = Cifar100_MLP(hidden_size=HIDDEN_SIZE, classes=len(CLASSES))\n",
        "\n",
        "    model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b4898c9",
      "metadata": {},
      "source": [
        "out[10]:{\n",
        "\n",
        "   Cifar100_MLP(\n",
        "\n",
        "  (norm): Normalize()\n",
        "\n",
        "  (seq): Sequential(\n",
        "\n",
        "    (0): Linear(in_features=3072, out_features=10, bias=True)\n",
        "\n",
        "    (1): ReLU()\n",
        "\n",
        "    (2): Linear(in_features=10, out_features=3, bias=True)\n",
        "\n",
        "  )\n",
        "\n",
        ")\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "    <h2>\u0627\u062e\u062a\u064a\u0627\u0631 \u062f\u0627\u0644\u0629 \u0627\u0644\u062e\u0633\u0627\u0631\u0629 \u0648\u0627\u0644\u0645\u064f\u062d\u0633\u0650\u0651\u0646 \u0644\u0627\u0646\u062d\u062f\u0627\u0631 \u0627\u0644\u062a\u062f\u0631\u062c</h2>\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "In[11]:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6406040",
      "metadata": {},
      "outputs": [],
      "source": [
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.005)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0033f14b",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## \u062a\u062f\u0631\u064a\u0628 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u062d\u0633\u0628 \u0627\u0644\u0639\u0635\u0631\n",
        "\n",
        "\n",
        "\n",
        "In[12]:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1f83ce2",
      "metadata": {},
      "outputs": [],
      "source": [
        "    EPOCHS = 250\n",
        "\n",
        "    steps_per_epoch = len(dataloader['train'])\n",
        "\n",
        "    steps_per_epoch_val = len(dataloader['test'])\n",
        "\n",
        "    for epoch in range(EPOCHS):  # pass through the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for i, batch in enumerate(dataloader['train'], 0):\n",
        "\n",
        "            # getting a single minibatch; batch is a two-element list of [inputs, labels]\n",
        "\n",
        "            inputs, labels = batch\n",
        "\n",
        "    \n",
        "\n",
        "            # clearing past gradients from the last iteration\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    \n",
        "\n",
        "            # \u043f\u0440\u044f\u043c\u043e\u0439 + \u043e\u0431\u0440\u0430\u0442\u043d\u044b\u0439 \u043f\u0440\u043e\u0445\u043e\u0434\u044b + \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            #loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "    \n",
        "\n",
        "            # statisticians of iwrm calculation\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / steps_per_epoch:.3f}')\n",
        "\n",
        "        running_loss = 0.0\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad(): # disable automatic differentiation\n",
        "\n",
        "            for i, data in enumerate(dataloader['test'], 0):\n",
        "\n",
        "                inputs, labels = data\n",
        "\n",
        "    \n",
        "\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "        print(f'[{epoch + 1}, {i + 1:5d}] val loss: {running_loss / steps_per_epoch_val:.3f}')\n",
        "\n",
        "    print('\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0437\u0430\u043a\u043e\u043d\u0447\u0435\u043d\u043e')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff33abc2",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- [1,    12] loss: 1.015\n",
        "\n",
        "- [1,     3] val loss: 0.921\n",
        "\n",
        "- [2,    12] loss: 0.860\n",
        "\n",
        "- [2,     3] val loss: 0.837\n",
        "\n",
        "- [3,    12] loss: 0.777\n",
        "\n",
        "- [3,     3] val loss: 0.793\n",
        "\n",
        "- [4,    12] loss: 0.721\n",
        "\n",
        "- [4,     3] val loss: 0.743\n",
        "\n",
        "- [5,    12] loss: 0.674\n",
        "\n",
        "- [5,     3] val loss: 0.727\n",
        "\n",
        "- [6,    12] loss: 0.638\n",
        "\n",
        "- [6,     3] val loss: 0.681\n",
        "\n",
        "- [7,    12] loss: 0.612\n",
        "\n",
        "- [7,     3] val loss: 0.658\n",
        "\n",
        "- [8,    12] loss: 0.589\n",
        "\n",
        "- [8,     3] val loss: 0.667\n",
        "\n",
        "- [9,    12] loss: 0.569\n",
        "\n",
        "- [9,     3] val loss: 0.622\n",
        "\n",
        "- [10,    12] loss: 0.552\n",
        "\n",
        "- [10,     3] val loss: 0.632\n",
        "\n",
        "- [11,    12] loss: 0.537\n",
        "\n",
        "- [11,     3] val loss: 0.604\n",
        "\n",
        "- [12,    12] loss: 0.527\n",
        "\n",
        "- [12,     3] val loss: 0.618\n",
        "\n",
        "- [13,    12] loss: 0.513\n",
        "\n",
        "- [13,     3] val loss: 0.578\n",
        "\n",
        "- [14,    12] loss: 0.502\n",
        "\n",
        "- [14,     3] val loss: 0.626\n",
        "\n",
        "- [15,    12] loss: 0.490\n",
        "\n",
        "- [15,     3] val loss: 0.631\n",
        "\n",
        "- [16,    12] loss: 0.482\n",
        "\n",
        "- [16,     3] val loss: 0.577\n",
        "\n",
        "- [17,    12] loss: 0.475\n",
        "\n",
        "- [17,     3] val loss: 0.558\n",
        "\n",
        "- [18,    12] loss: 0.465\n",
        "\n",
        "- [18,     3] val loss: 0.553\n",
        "\n",
        "- [19,    12] loss: 0.459\n",
        "\n",
        "- [19,     3] val loss: 0.556\n",
        "\n",
        "- [20,    12] loss: 0.450\n",
        "\n",
        "- [20,     3] val loss: 0.542\n",
        "\n",
        "- [21,    12] loss: 0.443\n",
        "\n",
        "- [21,     3] val loss: 0.542\n",
        "\n",
        "- [22,    12] loss: 0.438\n",
        "\n",
        "- [22,     3] val loss: 0.585\n",
        "\n",
        "- [23,    12] loss: 0.428\n",
        "\n",
        "- [23,     3] val loss: 0.546\n",
        "\n",
        "- [24,    12] loss: 0.423\n",
        "\n",
        "- [24,     3] val loss: 0.536\n",
        "\n",
        "- [25,    12] loss: 0.416\n",
        "\n",
        "- [25,     3] val loss: 0.528\n",
        "\n",
        "- [26,    12] loss: 0.409\n",
        "\n",
        "- [26,     3] val loss: 0.537\n",
        "\n",
        "- [27,    12] loss: 0.403\n",
        "\n",
        "- [27,     3] val loss: 0.550\n",
        "\n",
        "- [28,    12] loss: 0.399\n",
        "\n",
        "- [28,     3] val loss: 0.512\n",
        "\n",
        "- [29,    12] loss: 0.391\n",
        "\n",
        "- [29,     3] val loss: 0.512\n",
        "\n",
        "- [30,    12] loss: 0.387\n",
        "\n",
        "- [30,     3] val loss: 0.528\n",
        "\n",
        "- [31,    12] loss: 0.381\n",
        "\n",
        "- [31,     3] val loss: 0.515\n",
        "\n",
        "- [32,    12] loss: 0.378\n",
        "\n",
        "- [32,     3] val loss: 0.551\n",
        "\n",
        "- [33,    12] loss: 0.374\n",
        "\n",
        "- [33,     3] val loss: 0.506\n",
        "\n",
        "- [34,    12] loss: 0.368\n",
        "\n",
        "- [34,     3] val loss: 0.519\n",
        "\n",
        "- [35,    12] loss: 0.364\n",
        "\n",
        "- [35,     3] val loss: 0.535\n",
        "\n",
        "- [36,    12] loss: 0.358\n",
        "\n",
        "- [36,     3] val loss: 0.491\n",
        "\n",
        "- [37,    12] loss: 0.355\n",
        "\n",
        "- [37,     3] val loss: 0.529\n",
        "\n",
        "- [38,    12] loss: 0.349\n",
        "\n",
        "- [38,     3] val loss: 0.535\n",
        "\n",
        "- [39,    12] loss: 0.344\n",
        "\n",
        "- [39,     3] val loss: 0.490\n",
        "\n",
        "- [40,    12] loss: 0.341\n",
        "\n",
        "- [40,     3] val loss: 0.544\n",
        "\n",
        "- [41,    12] loss: 0.337\n",
        "\n",
        "- [41,     3] val loss: 0.512\n",
        "\n",
        "- [42,    12] loss: 0.334\n",
        "\n",
        "- [42,     3] val loss: 0.535\n",
        "\n",
        "- [43,    12] loss: 0.329\n",
        "\n",
        "- [43,     3] val loss: 0.490\n",
        "\n",
        "- [44,    12] loss: 0.323\n",
        "\n",
        "- [44,     3] val loss: 0.504\n",
        "\n",
        "- [45,    12] loss: 0.323\n",
        "\n",
        "- [45,     3] val loss: 0.531\n",
        "\n",
        "- [46,    12] loss: 0.317\n",
        "\n",
        "- [46,     3] val loss: 0.536\n",
        "\n",
        "- [47,    12] loss: 0.315\n",
        "\n",
        "- [47,     3] val loss: 0.509\n",
        "\n",
        "- [48,    12] loss: 0.312\n",
        "\n",
        "- [48,     3] val loss: 0.532\n",
        "\n",
        "- [49,    12] loss: 0.306\n",
        "\n",
        "- [49,     3] val loss: 0.525\n",
        "\n",
        "- [50,    12] loss: 0.303\n",
        "\n",
        "- [50,     3] val loss: 0.538\n",
        "\n",
        "- [51,    12] loss: 0.301\n",
        "\n",
        "- [51,     3] val loss: 0.533\n",
        "\n",
        "- [52,    12] loss: 0.297\n",
        "\n",
        "- [52,     3] val loss: 0.536\n",
        "\n",
        "- [53,    12] loss: 0.292\n",
        "\n",
        "- [53,     3] val loss: 0.507\n",
        "\n",
        "- [54,    12] loss: 0.288\n",
        "\n",
        "- [54,     3] val loss: 0.485\n",
        "\n",
        "- [55,    12] loss: 0.288\n",
        "\n",
        "- [55,     3] val loss: 0.487\n",
        "\n",
        "- [56,    12] loss: 0.284\n",
        "\n",
        "- [56,     3] val loss: 0.514\n",
        "\n",
        "- [57,    12] loss: 0.281\n",
        "\n",
        "- [57,     3] val loss: 0.521\n",
        "\n",
        "- [58,    12] loss: 0.276\n",
        "\n",
        "- [58,     3] val loss: 0.525\n",
        "\n",
        "- [59,    12] loss: 0.274\n",
        "\n",
        "- [59,     3] val loss: 0.519\n",
        "\n",
        "- [60,    12] loss: 0.270\n",
        "\n",
        "- [60,     3] val loss: 0.533\n",
        "\n",
        "- [61,    12] loss: 0.270\n",
        "\n",
        "- [61,     3] val loss: 0.535\n",
        "\n",
        "- [62,    12] loss: 0.265\n",
        "\n",
        "- [62,     3] val loss: 0.557\n",
        "\n",
        "- [63,    12] loss: 0.264\n",
        "\n",
        "- [63,     3] val loss: 0.489\n",
        "\n",
        "- [64,    12] loss: 0.260\n",
        "\n",
        "- [64,     3] val loss: 0.502\n",
        "\n",
        "- [65,    12] loss: 0.257\n",
        "\n",
        "- [65,     3] val loss: 0.550\n",
        "\n",
        "- [66,    12] loss: 0.251\n",
        "\n",
        "- [66,     3] val loss: 0.495\n",
        "\n",
        "- [67,    12] loss: 0.252\n",
        "\n",
        "- [67,     3] val loss: 0.485\n",
        "\n",
        "- [68,    12] loss: 0.247\n",
        "\n",
        "- [68,     3] val loss: 0.507\n",
        "\n",
        "- [69,    12] loss: 0.244\n",
        "\n",
        "- [69,     3] val loss: 0.525\n",
        "\n",
        "- [70,    12] loss: 0.241\n",
        "\n",
        "- [70,     3] val loss: 0.526\n",
        "\n",
        "- [71,    12] loss: 0.240\n",
        "\n",
        "- [71,     3] val loss: 0.517\n",
        "\n",
        "- [72,    12] loss: 0.239\n",
        "\n",
        "- [72,     3] val loss: 0.508\n",
        "\n",
        "- [73,    12] loss: 0.236\n",
        "\n",
        "- [73,     3] val loss: 0.495\n",
        "\n",
        "- [74,    12] loss: 0.232\n",
        "\n",
        "- [74,     3] val loss: 0.519\n",
        "\n",
        "- [75,    12] loss: 0.231\n",
        "\n",
        "- [75,     3] val loss: 0.540\n",
        "\n",
        "- [76,    12] loss: 0.227\n",
        "\n",
        "- [76,     3] val loss: 0.517\n",
        "\n",
        "- [77,    12] loss: 0.226\n",
        "\n",
        "- [77,     3] val loss: 0.479\n",
        "\n",
        "- [78,    12] loss: 0.223\n",
        "\n",
        "- [78,     3] val loss: 0.518\n",
        "\n",
        "- [79,    12] loss: 0.223\n",
        "\n",
        "- [79,     3] val loss: 0.529\n",
        "\n",
        "- [80,    12] loss: 0.218\n",
        "\n",
        "- [80,     3] val loss: 0.544\n",
        "\n",
        "- [81,    12] loss: 0.217\n",
        "\n",
        "- [81,     3] val loss: 0.503\n",
        "\n",
        "- [82,    12] loss: 0.214\n",
        "\n",
        "- [82,     3] val loss: 0.519\n",
        "\n",
        "- [83,    12] loss: 0.210\n",
        "\n",
        "- [83,     3] val loss: 0.486\n",
        "\n",
        "- [84,    12] loss: 0.209\n",
        "\n",
        "- [84,     3] val loss: 0.566\n",
        "\n",
        "- [85,    12] loss: 0.207\n",
        "\n",
        "- [85,     3] val loss: 0.482\n",
        "\n",
        "- [86,    12] loss: 0.205\n",
        "\n",
        "- [86,     3] val loss: 0.557\n",
        "\n",
        "- [87,    12] loss: 0.205\n",
        "\n",
        "- [87,     3] val loss: 0.507\n",
        "\n",
        "- [88,    12] loss: 0.201\n",
        "\n",
        "- [88,     3] val loss: 0.528\n",
        "\n",
        "- [89,    12] loss: 0.199\n",
        "\n",
        "- [89,     3] val loss: 0.484\n",
        "\n",
        "- [90,    12] loss: 0.198\n",
        "\n",
        "- [90,     3] val loss: 0.523\n",
        "\n",
        "- [91,    12] loss: 0.196\n",
        "\n",
        "- [91,     3] val loss: 0.534\n",
        "\n",
        "- [92,    12] loss: 0.193\n",
        "\n",
        "- [92,     3] val loss: 0.514\n",
        "\n",
        "- [93,    12] loss: 0.191\n",
        "\n",
        "- [93,     3] val loss: 0.533\n",
        "\n",
        "- [94,    12] loss: 0.189\n",
        "\n",
        "- [94,     3] val loss: 0.546\n",
        "\n",
        "- [95,    12] loss: 0.187\n",
        "\n",
        "- [95,     3] val loss: 0.561\n",
        "\n",
        "- [96,    12] loss: 0.186\n",
        "\n",
        "- [96,     3] val loss: 0.546\n",
        "\n",
        "- [97,    12] loss: 0.183\n",
        "\n",
        "- [97,     3] val loss: 0.501\n",
        "\n",
        "- [98,    12] loss: 0.182\n",
        "\n",
        "- [98,     3] val loss: 0.538\n",
        "\n",
        "- [99,    12] loss: 0.179\n",
        "\n",
        "- [99,     3] val loss: 0.566\n",
        "\n",
        "- [100,   12] loss: 0.177\n",
        "\n",
        "- [100,     3] val loss: 0.530\n",
        "\n",
        "- [101,    12] loss: 0.176\n",
        "\n",
        "- [101,     3] val loss: 0.547\n",
        "\n",
        "- [102,    12] loss: 0.174\n",
        "\n",
        "- [102,     3] val loss: 0.546\n",
        "\n",
        "- [103,    12] loss: 0.173\n",
        "\n",
        "- [103,     3] val loss: 0.543\n",
        "\n",
        "- [104,    12] loss: 0.170\n",
        "\n",
        "- [104,     3] val loss: 0.507\n",
        "\n",
        "- [105,    12] loss: 0.168\n",
        "\n",
        "- [105,     3] val loss: 0.538\n",
        "\n",
        "- [106,    12] loss: 0.167\n",
        "\n",
        "- [106,     3] val loss: 0.539\n",
        "\n",
        "- [107,    12] loss: 0.166\n",
        "\n",
        "- [107,     3] val loss: 0.546\n",
        "\n",
        "- [108,    12] loss: 0.164\n",
        "\n",
        "- [108,     3] val loss: 0.521\n",
        "\n",
        "- [109,    12] loss: 0.163\n",
        "\n",
        "- [109,     3] val loss: 0.587\n",
        "\n",
        "- [110,    12] loss: 0.159\n",
        "\n",
        "- [110,     3] val loss: 0.541\n",
        "\n",
        "- [111,    12] loss: 0.158\n",
        "\n",
        "- [111,     3] val loss: 0.595\n",
        "\n",
        "- [112,    12] loss: 0.156\n",
        "\n",
        "- [112,     3] val loss: 0.560\n",
        "\n",
        "- [113,    12] loss: 0.155\n",
        "\n",
        "- [113,     3] val loss: 0.522\n",
        "\n",
        "- [114,    12] loss: 0.154\n",
        "\n",
        "- [114,     3] val loss: 0.583\n",
        "\n",
        "- [115,    12] loss: 0.152\n",
        "\n",
        "- [115,     3] val loss: 0.564\n",
        "\n",
        "- [116,    12] loss: 0.152\n",
        "\n",
        "- [116,     3] val loss: 0.485\n",
        "\n",
        "- [117,    12] loss: 0.149\n",
        "\n",
        "- [117,     3] val loss: 0.538\n",
        "\n",
        "- [118,    12] loss: 0.148\n",
        "\n",
        "- [118,     3] val loss: 0.606\n",
        "\n",
        "- [119,    12] loss: 0.147\n",
        "\n",
        "- [119,     3] val loss: 0.526\n",
        "\n",
        "- [120,    12] loss: 0.145\n",
        "\n",
        "- [120,     3] val loss: 0.566\n",
        "\n",
        "- [121,    12] loss: 0.144\n",
        "\n",
        "- [121,     3] val loss: 0.592\n",
        "\n",
        "- [122,    12] loss: 0.142\n",
        "\n",
        "- [122,     3] val loss: 0.530\n",
        "\n",
        "- [123,    12] loss: 0.141\n",
        "\n",
        "- [123,     3] val loss: 0.563\n",
        "\n",
        "- [124,    12] loss: 0.138\n",
        "\n",
        "- [124,     3] val loss: 0.563\n",
        "\n",
        "- [125,    12] loss: 0.137\n",
        "\n",
        "- [125,     3] val loss: 0.519\n",
        "\n",
        "- [126,    12] loss: 0.136\n",
        "\n",
        "- [126,     3] val loss: 0.632\n",
        "\n",
        "- [127,    12] loss: 0.136\n",
        "\n",
        "- [127,     3] val loss: 0.541\n",
        "\n",
        "- [128,    12] loss: 0.136\n",
        "\n",
        "- [128,     3] val loss: 0.572\n",
        "\n",
        "- [129,    12] loss: 0.133\n",
        "\n",
        "- [129,     3] val loss: 0.591\n",
        "\n",
        "- [130,    12] loss: 0.133\n",
        "\n",
        "- [130,     3] val loss: 0.572\n",
        "\n",
        "- [131,    12] loss: 0.130\n",
        "\n",
        "- [131,     3] val loss: 0.543\n",
        "\n",
        "- [132,    12] loss: 0.128\n",
        "\n",
        "- [132,     3] val loss: 0.599\n",
        "\n",
        "- [133,    12] loss: 0.128\n",
        "\n",
        "- [133,     3] val loss: 0.531\n",
        "\n",
        "- [134,    12] loss: 0.126\n",
        "\n",
        "- [134,     3] val loss: 0.601\n",
        "\n",
        "- [135,    12] loss: 0.127\n",
        "\n",
        "- [135,     3] val loss: 0.607\n",
        "\n",
        "- [136,    12] loss: 0.123\n",
        "\n",
        "- [136,     3] val loss: 0.627\n",
        "\n",
        "- [137,    12] loss: 0.123\n",
        "\n",
        "- [137,     3] val loss: 0.574\n",
        "\n",
        "- [138,    12] loss: 0.122\n",
        "\n",
        "- [138,     3] val loss: 0.614\n",
        "\n",
        "- [139,    12] loss: 0.121\n",
        "\n",
        "- [139,     3] val loss: 0.556\n",
        "\n",
        "- [140,    12] loss: 0.119\n",
        "\n",
        "- [140,     3] val loss: 0.589\n",
        "\n",
        "- [141,    12] loss: 0.118\n",
        "\n",
        "- [141,     3] val loss: 0.587\n",
        "\n",
        "- [142,    12] loss: 0.118\n",
        "\n",
        "- [142,     3] val loss: 0.593\n",
        "\n",
        "- [143,    12] loss: 0.118\n",
        "\n",
        "- [143,     3] val loss: 0.606\n",
        "\n",
        "- [144,    12] loss: 0.115\n",
        "\n",
        "- [144,     3] val loss: 0.631\n",
        "\n",
        "- [145,    12] loss: 0.114\n",
        "\n",
        "- [145,     3] val loss: 0.557\n",
        "\n",
        "- [146,    12] loss: 0.113\n",
        "\n",
        "- [146,     3] val loss: 0.640\n",
        "\n",
        "- [147,    12] loss: 0.113\n",
        "\n",
        "- [147,     3] val loss: 0.590\n",
        "\n",
        "- [148,    12] loss: 0.111\n",
        "\n",
        "- [148,     3] val loss: 0.576\n",
        "\n",
        "- [149,    12] loss: 0.111\n",
        "\n",
        "- [149,     3] val loss: 0.596\n",
        "\n",
        "- [150,    12] loss: 0.110\n",
        "\n",
        "- [150,     3] val loss: 0.571\n",
        "\n",
        "- [151,    12] loss: 0.109\n",
        "\n",
        "- [151,     3] val loss: 0.584\n",
        "\n",
        "- [152,    12] loss: 0.107\n",
        "\n",
        "- [152,     3] val loss: 0.596\n",
        "\n",
        "- [153,    12] loss: 0.107\n",
        "\n",
        "- [153,     3] val loss: 0.580\n",
        "\n",
        "- [154,    12] loss: 0.106\n",
        "\n",
        "- [154,     3] val loss: 0.598\n",
        "\n",
        "- [155,    12] loss: 0.105\n",
        "\n",
        "- [155,     3] val loss: 0.624\n",
        "\n",
        "- [156,    12] loss: 0.103\n",
        "\n",
        "- [156,     3] val loss: 0.581\n",
        "\n",
        "- [157,    12] loss: 0.103\n",
        "\n",
        "- [157,     3] val loss: 0.606\n",
        "\n",
        "- [158,    12] loss: 0.102\n",
        "\n",
        "- [158,     3] val loss: 0.569\n",
        "\n",
        "- [159,    12] loss: 0.101\n",
        "\n",
        "- [159,     3] val loss: 0.556\n",
        "\n",
        "- [160,    12] loss: 0.100\n",
        "\n",
        "- [160,     3] val loss: 0.582\n",
        "\n",
        "- [161,    12] loss: 0.100\n",
        "\n",
        "- [161,     3] val loss: 0.614\n",
        "\n",
        "- [162,    12] loss: 0.099\n",
        "\n",
        "- [162,     3] val loss: 0.615\n",
        "\n",
        "- [163,    12] loss: 0.098\n",
        "\n",
        "- [163,     3] val loss: 0.559\n",
        "\n",
        "- [164,    12] loss: 0.097\n",
        "\n",
        "- [164,     3] val loss: 0.590\n",
        "\n",
        "- [165,    12] loss: 0.095\n",
        "\n",
        "- [165,     3] val loss: 0.596\n",
        "\n",
        "- [166,    12] loss: 0.095\n",
        "\n",
        "- [166,     3] val loss: 0.663\n",
        "\n",
        "- [167,    12] loss: 0.094\n",
        "\n",
        "- [167,     3] val loss: 0.666\n",
        "\n",
        "- [168,    12] loss: 0.093\n",
        "\n",
        "- [168,     3] val loss: 0.591\n",
        "\n",
        "- [169,    12] loss: 0.093\n",
        "\n",
        "- [169,     3] val loss: 0.627\n",
        "\n",
        "- [170,    12] loss: 0.091\n",
        "\n",
        "- [170,     3] val loss: 0.598\n",
        "\n",
        "- [171,    12] loss: 0.090\n",
        "\n",
        "- [171,     3] val loss: 0.683\n",
        "\n",
        "- [172,    12] loss: 0.090\n",
        "\n",
        "- [172,     3] val loss: 0.652\n",
        "\n",
        "- [173,    12] loss: 0.090\n",
        "\n",
        "- [173,     3] val loss: 0.601\n",
        "\n",
        "- [174,    12] loss: 0.089\n",
        "\n",
        "- [174,     3] val loss: 0.624\n",
        "\n",
        "- [175,    12] loss: 0.088\n",
        "\n",
        "- [175,     3] val loss: 0.649\n",
        "\n",
        "- [176,    12] loss: 0.088\n",
        "\n",
        "- [176,     3] val loss: 0.624\n",
        "\n",
        "- [177,    12] loss: 0.088\n",
        "\n",
        "- [177,     3] val loss: 0.656\n",
        "\n",
        "- [178,    12] loss: 0.086\n",
        "\n",
        "- [178,     3] val loss: 0.645\n",
        "\n",
        "- [179,    12] loss: 0.085\n",
        "\n",
        "- [179,     3] val loss: 0.614\n",
        "\n",
        "- [180,    12] loss: 0.084\n",
        "\n",
        "- [180,     3] val loss: 0.650\n",
        "\n",
        "- [181,    12] loss: 0.085\n",
        "\n",
        "- [181,     3] val loss: 0.581\n",
        "\n",
        "- [182,    12] loss: 0.083\n",
        "\n",
        "- [182,     3] val loss: 0.594\n",
        "\n",
        "- [183,    12] loss: 0.082\n",
        "\n",
        "- [183,     3] val loss: 0.603\n",
        "\n",
        "- [184,    12] loss: 0.082\n",
        "\n",
        "- [184,     3] val loss: 0.650\n",
        "\n",
        "- [185,    12] loss: 0.082\n",
        "\n",
        "- [185,     3] val loss: 0.647\n",
        "\n",
        "- [186,    12] loss: 0.082\n",
        "\n",
        "- [186,     3] val loss: 0.646\n",
        "\n",
        "- [187,    12] loss: 0.081\n",
        "\n",
        "- [187,     3] val loss: 0.580\n",
        "\n",
        "- [188,    12] loss: 0.079\n",
        "\n",
        "- [188,     3] val loss: 0.676\n",
        "\n",
        "- [189,    12] loss: 0.079\n",
        "\n",
        "- [189,     3] val loss: 0.597\n",
        "\n",
        "- [190,    12] loss: 0.078\n",
        "\n",
        "- [190,     3] val loss: 0.620\n",
        "\n",
        "- [191,    12] loss: 0.077\n",
        "\n",
        "- [191,     3] val loss: 0.655\n",
        "\n",
        "- [192,    12] loss: 0.077\n",
        "\n",
        "- [192,     3] val loss: 0.590\n",
        "\n",
        "- [193,    12] loss: 0.076\n",
        "\n",
        "- [193,     3] val loss: 0.668\n",
        "\n",
        "- [194,    12] loss: 0.075\n",
        "\n",
        "- [194,     3] val loss: 0.686\n",
        "\n",
        "- [195,    12] loss: 0.075\n",
        "\n",
        "- [195,     3] val loss: 0.628\n",
        "\n",
        "- [196,    12] loss: 0.074\n",
        "\n",
        "- [196,     3] val loss: 0.627\n",
        "\n",
        "- [197,    12] loss: 0.074\n",
        "\n",
        "- [197,     3] val loss: 0.625\n",
        "\n",
        "- [198,    12] loss: 0.074\n",
        "\n",
        "- [198,     3] val loss: 0.620\n",
        "\n",
        "- [199,    12] loss: 0.073\n",
        "\n",
        "- [199,     3] val loss: 0.623\n",
        "\n",
        "- [200,    12] loss: 0.072\n",
        "\n",
        "- [200,     3] val loss: 0.600\n",
        "\n",
        "- [201,    12] loss: 0.072\n",
        "\n",
        "- [201,     3] val loss: 0.624\n",
        "\n",
        "- [202,    12] loss: 0.070\n",
        "\n",
        "- [202,     3] val loss: 0.700\n",
        "\n",
        "- [203,    12] loss: 0.070\n",
        "\n",
        "- [203,     3] val loss: 0.709\n",
        "\n",
        "- [204,    12] loss: 0.070\n",
        "\n",
        "- [204,     3] val loss: 0.681\n",
        "\n",
        "- [205,    12] loss: 0.069\n",
        "\n",
        "- [205,     3] val loss: 0.683\n",
        "\n",
        "- [206,    12] loss: 0.069\n",
        "\n",
        "- [206,     3] val loss: 0.689\n",
        "\n",
        "- [207,    12] loss: 0.068\n",
        "\n",
        "- [207,     3] val loss: 0.627\n",
        "\n",
        "- [208,    12] loss: 0.068\n",
        "\n",
        "- [208,     3] val loss: 0.615\n",
        "\n",
        "- [209,    12] loss: 0.068\n",
        "\n",
        "- [209,     3] val loss: 0.684\n",
        "\n",
        "- [210,    12] loss: 0.067\n",
        "\n",
        "- [210,     3] val loss: 0.610\n",
        "\n",
        "- [211,    12] loss: 0.066\n",
        "\n",
        "- [211,     3] val loss: 0.670\n",
        "\n",
        "- [212,    12] loss: 0.067\n",
        "\n",
        "- [212,     3] val loss: 0.685\n",
        "\n",
        "- [213,    12] loss: 0.065\n",
        "\n",
        "- [213,     3] val loss: 0.678\n",
        "\n",
        "- [214,    12] loss: 0.066\n",
        "\n",
        "- [214,     3] val loss: 0.683\n",
        "\n",
        "- [215,    12] loss: 0.065\n",
        "\n",
        "- [215,     3] val loss: 0.622\n",
        "\n",
        "- [216,    12] loss: 0.063\n",
        "\n",
        "- [216,     3] val loss: 0.658\n",
        "\n",
        "- [217,    12] loss: 0.063\n",
        "\n",
        "- [217,     3] val loss: 0.691\n",
        "\n",
        "- [218,    12] loss: 0.063\n",
        "\n",
        "- [218,     3] val loss: 0.661\n",
        "\n",
        "- [219,    12] loss: 0.063\n",
        "\n",
        "- [219,     3] val loss: 0.696\n",
        "\n",
        "- [220,    12] loss: 0.062\n",
        "\n",
        "- [220,     3] val loss: 0.602\n",
        "\n",
        "- [221,    12] loss: 0.061\n",
        "\n",
        "- [221,     3] val loss: 0.665\n",
        "\n",
        "- [222,    12] loss: 0.062\n",
        "\n",
        "- [222,     3] val loss: 0.679\n",
        "\n",
        "- [223,    12] loss: 0.061\n",
        "\n",
        "- [223,     3] val loss: 0.650\n",
        "\n",
        "- [224,    12] loss: 0.061\n",
        "\n",
        "- [224,     3] val loss: 0.694\n",
        "\n",
        "- [225,    12] loss: 0.060\n",
        "\n",
        "- [225,     3] val loss: 0.654\n",
        "\n",
        "- [226,    12] loss: 0.060\n",
        "\n",
        "- [226,     3] val loss: 0.670\n",
        "\n",
        "- [227,    12] loss: 0.059\n",
        "\n",
        "- [227,     3] val loss: 0.687\n",
        "\n",
        "- [228,    12] loss: 0.059\n",
        "\n",
        "- [228,     3] val loss: 0.666\n",
        "\n",
        "- [229,    12] loss: 0.059\n",
        "\n",
        "- [229,     3] val loss: 0.734\n",
        "\n",
        "- [230,    12] loss: 0.058\n",
        "\n",
        "- [230,     3] val loss: 0.729\n",
        "\n",
        "- [231,    12] loss: 0.057\n",
        "\n",
        "- [231,     3] val loss: 0.701\n",
        "\n",
        "- [232,    12] loss: 0.057\n",
        "\n",
        "- [232,     3] val loss: 0.627\n",
        "\n",
        "- [233,    12] loss: 0.056\n",
        "\n",
        "- [233,     3] val loss: 0.642\n",
        "\n",
        "- [234,    12] loss: 0.056\n",
        "\n",
        "- [234,     3] val loss: 0.644\n",
        "\n",
        "- [235,    12] loss: 0.056\n",
        "\n",
        "- [235,     3] val loss: 0.684\n",
        "\n",
        "- [236,    12] loss: 0.056\n",
        "\n",
        "- [236,     3] val loss: 0.684\n",
        "\n",
        "- [237,    12] loss: 0.055\n",
        "\n",
        "- [237,     3] val loss: 0.673\n",
        "\n",
        "- [238,    12] loss: 0.055\n",
        "\n",
        "- [238,     3] val loss: 0.684\n",
        "\n",
        "- [239,    12] loss: 0.054\n",
        "\n",
        "- [239,     3] val loss: 0.723\n",
        "\n",
        "- [240,    12] loss: 0.054\n",
        "\n",
        "- [240,     3] val loss: 0.747\n",
        "\n",
        "- [241,    12] loss: 0.054\n",
        "\n",
        "- [241,     3] val loss: 0.702\n",
        "\n",
        "- [242,    12] loss: 0.053\n",
        "\n",
        "- [242,     3] val loss: 0.742\n",
        "\n",
        "- [243,    12] loss: 0.053\n",
        "\n",
        "- [243,     3] val loss: 0.641\n",
        "\n",
        "- [244,    12] loss: 0.053\n",
        "\n",
        "- [244,     3] val loss: 0.685\n",
        "\n",
        "- [245,    12] loss: 0.052\n",
        "\n",
        "- [245,     3] val loss: 0.615\n",
        "\n",
        "- [246,    12] loss: 0.052\n",
        "\n",
        "- [246,     3] val loss: 0.753\n",
        "\n",
        "- [247,    12] loss: 0.051\n",
        "\n",
        "- [247,     3] val loss: 0.744\n",
        "\n",
        "- [248,    12] loss: 0.052\n",
        "\n",
        "- [248,     3] val loss: 0.696\n",
        "\n",
        "- [249,    12] loss: 0.051\n",
        "\n",
        "- [249,     3] val loss: 0.681\n",
        "\n",
        "- [250,    12] loss: 0.051\n",
        "\n",
        "- [250,     3] val loss: 0.706\n",
        "\n",
        "\n",
        "\n",
        "\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0437\u0430\u043a\u043e\u043d\u0447\u0435\u043d\u043e\n",
        "\n",
        "## \u0627\u0644\u062a\u062d\u0642\u0642 \u0645\u0646 \u062c\u0648\u062f\u0629 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0645\u0646 \u0642\u0628\u0644 \u0641\u0626\u0629 \u0641\u064a \u062a\u062f\u0631\u064a\u0628 \u0648\u0627\u062e\u062a\u0628\u0627\u0631 \u0627\u0644\u0639\u064a\u0646\u0627\u062a\n",
        "\n",
        "\n",
        "\n",
        "In[13]:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "145d08dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "    for part in ['train', 'test']:\n",
        "\n",
        "        y_pred = []\n",
        "\n",
        "        y_true = []\n",
        "\n",
        "        with torch.no_grad(): # disable automatic differentiation\n",
        "\n",
        "            for i, data in enumerate(dataloader[part], 0):\n",
        "\n",
        "                inputs, labels = data\n",
        "\n",
        "    \n",
        "\n",
        "                outputs = model(inputs).detach().numpy()\n",
        "\n",
        "                y_pred.append(outputs)\n",
        "\n",
        "                y_true.append(labels.numpy())\n",
        "\n",
        "            y_true = np.concatenate(y_true)\n",
        "\n",
        "            y_pred = np.concatenate(y_pred)\n",
        "\n",
        "            print(part)\n",
        "\n",
        "            print(classification_report(y_true.argmax(axis=-1), y_pred.argmax(axis=-1),\n",
        "\n",
        "                                        digits=4, target_names=list(map(str, CLASSES))))\n",
        "\n",
        "            print('-'*50)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "998b2445",
      "metadata": {},
      "source": [
        "train\n",
        "\n",
        "\n",
        "\n",
        " precision    recall  f1-score   support\n",
        "\n",
        "\n",
        "\n",
        "           0     1.0000    0.9900    0.9950       500\n",
        "\n",
        "          55     0.9881    1.0000    0.9940       500\n",
        "\n",
        "          58     0.9980    0.9960    0.9970       500\n",
        "\n",
        "\n",
        "\n",
        "    accuracy                         0.9953      1500\n",
        "\n",
        "    macro avg     0.9954    0.9953    0.9953      1500\n",
        "\n",
        "    weighted avg     0.9954    0.9953    0.9953      1500\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "test\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "\n",
        "\n",
        "           0     0.8174    0.9400    0.8744       100\n",
        "\n",
        "          55     0.7158    0.6800    0.6974       100\n",
        "\n",
        "          58     0.6889    0.6200    0.6526       100\n",
        "\n",
        "\n",
        "\n",
        "    accuracy                         0.7467       300\n",
        "\n",
        "    macro avg     0.7407    0.7467    0.7415       300\n",
        "\n",
        "    weighted avg     0.7407    0.7467    0.7415       300\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## \u0627\u0644\u062a\u0635\u0648\u0631 \u0627\u0644\u0645\u0642\u0627\u064a\u064a\u0633\n",
        "\n",
        "In[13]:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6289026e",
      "metadata": {},
      "outputs": [],
      "source": [
        "    weights = list(model.parameters())[0].detach().numpy()\n",
        "\n",
        "    print(weights.shape)\n",
        "\n",
        "    fig, ax = plt.subplots(1, weights.shape[0], figsize=(3*weights.shape[0], 3))\n",
        "\n",
        "    for i, \u03c9 in enumerate(weights):\n",
        "\n",
        "        \u03c9 = \u03c9.reshape(32, 32, 3)\n",
        "\n",
        "        \u03c9 -= np.percentile(\u03c9, 1, axis=[0, 1])\n",
        "\n",
        "        \u03c9 /= np.percentile(\u03c9, 99, axis=[0, 1])\n",
        "\n",
        "        \u03c9 = np.clip(\u03c9, 0, 1) \n",
        "\n",
        "        ax[i].imshow(\u03c9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5cfd6b8",
      "metadata": {},
      "source": [
        "(10, 3072)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\" style=\"margin-top: 70px;\">\n",
        "\n",
        "    <img align=\"right\" src=\"Recources/pic7.png\" height=\"140px\" width=\"620px\"/>  \n",
        "\n",
        "    <hr style=\"margin-top: 70px; margin-bottom: 70px;\"> \n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "<br><br><br> <br><br><br> <br><br><br> <br><br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# \u0648\u0635\u0641  \u0637\u0631\u0642 \u0627\u0644\u0645\u0643\u062a\u0628\u0629\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "    <h3>\u0627\u0644\u0623\u0633\u0627\u0644\u064a\u0628 \u0648\u0627\u0644\u062f\u0648\u0627\u0644 \u0641\u064a \u0645\u0643\u062a\u0628\u0629 NumPy:</h3>\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "(\u0644\u0644\u062d\u0635\u0648\u0644 \u0639\u0644\u0649 \u0645\u0632\u064a\u062f \u0645\u0646 \u0627\u0644\u0645\u0639\u0644\u0648\u0645\u0627\u062a \u060c \u0631\u0627\u062c\u0639 \u0627\u0644\u0648\u062b\u0627\u0626\u0642 https://numpy.org/doc/1.22/reference/index.html)\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">* __np.array__ - \u0625\u0646\u0634\u0627\u0621 \u0645\u0635\u0641\u0648\u0641\u0629 \u0645\u0646 \u0642\u0627\u0626\u0645\u0629 \u0623\u0648 \u0645\u0635\u0641\u0648\u0641\u0629 \u0623\u062e\u0631\u0649</div>\n",
        "\n",
        "<div dir=\"rtl\">* __np.shape__ - \u064a\u0639\u0631\u0636 \u0623\u0628\u0639\u0627\u062f \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0629 \u0645\u062a\u0639\u062f\u062f\u0629 \u0627\u0644\u0623\u0628\u0639\u0627\u062f (\u0623\u064a \u0644\u0645\u0635\u0641\u0648\u0641\u0629 2\u00d72 \u0633\u064a\u0643\u0648\u0646 \u0627\u0644\u0646\u0627\u062a\u062c \u0647\u0648 \u0645\u062c\u0645\u0648\u0639\u0629 (2\u060c 2))</div>\n",
        "\n",
        "<div dir=\"rtl\">* __np.size__ - \u064a\u0639\u0631\u0636 \u0639\u062f\u062f \u0627\u0644\u0639\u0646\u0627\u0635\u0631 \u0641\u064a \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0629 (\u0623\u064a \u0644\u0645\u0635\u0641\u0648\u0641\u0629 2\u00d72 \u0633\u064a\u0643\u0648\u0646 \u0627\u0644\u0646\u0627\u062a\u062c 4)</div>\n",
        "\n",
        "<div dir=\"rtl\">* __np.uint8__\u060c __np.int16__\u060c __np.int64__\u060c __np.float32__ - \u062a\u062d\u0648\u064a\u0644 \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0629 \u0625\u0644\u0649 \u0646\u0648\u0639 \u062c\u062f\u064a\u062f\u060c \u0645\u0639 \u062a\u062e\u0635\u064a\u0635 \u0645\u0633\u0627\u062d\u0629 \u0641\u064a \u0627\u0644\u0630\u0627\u0643\u0631\u0629 \u0644\u0644\u0645\u0635\u0641\u0648\u0641\u0629 \u0627\u0644\u062c\u062f\u064a\u062f\u0629 \u0645\u0646 \u0627\u0644\u0646\u0648\u0639 \u0627\u0644\u0645\u062e\u062a\u0627\u0631. \u0627\u0644\u0639\u062f\u062f \u0628\u0639\u062f \u0627\u0644\u0646\u0648\u0639 \u064a\u062d\u062f\u062f \u0639\u062f\u062f \u0627\u0644\u0628\u062a\u0627\u062a \u0627\u0644\u0645\u0633\u062a\u062e\u062f\u0645\u0629 \u0644\u062a\u062e\u0632\u064a\u0646 \u0639\u0646\u0635\u0631 \u0648\u0627\u062d\u062f \u0645\u0646 \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0629. \u063a\u0627\u0644\u0628\u064b\u0627 \u0645\u0627 \u064a\u0633\u062a\u062e\u062f\u0645 \u0627\u0644\u0646\u0648\u0639 \u0627\u0644\u0627\u0642\u062a\u0635\u0627\u062f\u064a uint8 - \u0648\u0647\u0648 \u0646\u0648\u0639 \u0628\u064a\u0627\u0646\u0627\u062a \u0635\u062d\u064a\u062d \u063a\u064a\u0631 \u0645\u0648\u0642\u0651\u0639 \u0645\u0646 8 \u0628\u062a (\u0646\u0637\u0627\u0642 \u0627\u0644\u0623\u0639\u062f\u0627\u062f 0-255) \u0644\u062a\u062e\u0632\u064a\u0646 \u0627\u0644\u0635\u0648\u0631</div>\n",
        "\n",
        "<div dir=\"rtl\">* __np.ones__\u060c __np.zeros__ - \u0625\u0646\u0634\u0627\u0621 \u0645\u0635\u0641\u0648\u0641\u0627\u062a \u0645\u064f\u0639\u0628\u0623\u0629 \u0645\u0633\u0628\u0642\u064b\u0627 \u0625\u0645\u0627 \u0628\u0627\u0644\u0623\u062d\u0627\u062f\u064a\u0629 \u0623\u0648 \u0628\u0627\u0644\u0635\u0641\u0631. \u064a\u062a\u0645 \u062a\u0645\u0631\u064a\u0631 \u0642\u0627\u0626\u0645\u0629 \u0623\u0648 \u0645\u062c\u0645\u0648\u0639\u0629 \u0643\u0648\u0633\u064a\u0637 \u0644\u062a\u062d\u062f\u064a\u062f \u0627\u0644\u0623\u0628\u0639\u0627\u062f \u0627\u0644\u0645\u0637\u0644\u0648\u0628\u0629. \u0639\u0644\u0649 \u0633\u0628\u064a\u0644 \u0627\u0644\u0645\u062b\u0627\u0644 <code>np.ones((10,))</code> \u0633\u062a\u0642\u0648\u0645 \u0628\u0625\u0646\u0634\u0627\u0621 \u0645\u062a\u062c\u0647 \u0645\u0643\u0648\u0646 \u0645\u0646 10 \u0623\u0639\u062f\u0627\u062f \u0623\u062d\u0627\u062f\u064a\u0629. \u0648 <code>np.zeros((32, 32, 3))</code> \u0633\u062a\u0642\u0648\u0645 \u0628\u0625\u0646\u0634\u0627\u0621 \u0645\u0635\u0641\u0648\u0641\u0629 \u062b\u0646\u0627\u0626\u064a\u0629 \u0627\u0644\u0623\u0628\u0639\u0627\u062f \u0628\u062f\u0642\u0629 32\u00d732 \u0628\u0643\u0633\u0644 \u0645\u0639 3 \u0642\u0646\u0648\u0627\u062a. \u064a\u064f\u0633\u062a\u062e\u062f\u0645 \u0639\u0645\u0644\u064a\u064b\u0627 \u0644\u0644\u062a\u062d\u0642\u0642 \u0645\u0646 \u0628\u0646\u064a\u0629 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0641\u064a \u0627\u0644\u0627\u062a\u062c\u0627\u0647 \u0627\u0644\u0645\u0628\u0627\u0634\u0631</div>\n",
        "\n",
        "<div dir=\"rtl\">* __np.arange__ - \u0625\u0646\u0634\u0627\u0621 \u0645\u0635\u0641\u0648\u0641\u0629 \u0645\u064f\u0639\u0628\u0623\u0629 \u0645\u0633\u0628\u0642\u064b\u0627 \u0628\u0634\u0643\u0644 \u0645\u062a\u0632\u0627\u064a\u062f \u0645\u0646 \u062e\u0644\u0627\u0644 \u0627\u0644\u0645\u062a\u062a\u0627\u0644\u064a\u0629 \u0627\u0644\u062d\u0633\u0627\u0628\u064a\u0629 \u0645\u0646 \u0627\u0644\u0648\u0633\u064a\u0637 \u0627\u0644\u0623\u0648\u0644 \u0625\u0644\u0649 \u0627\u0644\u062b\u0627\u0646\u064a (\u063a\u064a\u0631 \u0645\u0634\u0645\u0648\u0644) \u0628\u062e\u0637\u0648\u0629 \u062a\u062d\u062f\u062f \u0628\u0627\u0644\u0648\u0633\u064a\u0637 \u0627\u0644\u062b\u0627\u0644\u062b. \u064a\u0645\u0643\u0646 \u0627\u0633\u062a\u0628\u0639\u0627\u062f \u0627\u0644\u0648\u0633\u064a\u0637\u064a\u0646 \u0627\u0644\u0623\u0648\u0644 \u0648\u0627\u0644\u062b\u0627\u0644\u062b\u060c \u0648\u0641\u064a \u0647\u0630\u0647 \u0627\u0644\u062d\u0627\u0644\u0629 \u064a\u062a\u0645 \u0627\u0644\u062d\u0635\u0648\u0644 \u0639\u0644\u0649 \u0643\u062a\u0627\u0628\u0629 \u0645\u0636\u063a\u0648\u0637\u0629 <code>np.arange(3)</code> => [0\u060c 1\u060c 2]</div>\n",
        "\n",
        "<div dir=\"rtl\">* __np.repeat__ - \u062a\u0643\u0631\u0627\u0631 \u0639\u0646\u0627\u0635\u0631 \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0629 \u0628\u0639\u062f\u062f \u0645\u062d\u062f\u062f \u064a\u062d\u062f\u062f \u0628\u0627\u0644\u0648\u0633\u064a\u0637 \u0627\u0644\u0623\u0648\u0644. \u0648\u0628\u0627\u0644\u062a\u0627\u0644\u064a\u060c \u0628\u0627\u0644\u0646\u0633\u0628\u0629 \u0644\u0645\u0635\u0641\u0648\u0641\u0629 <code>arr = [0\u060c 1]</code>\u060c <code>arr.repeat(2)</code> \u0633\u062a\u0639\u064a\u062f [0\u060c 0\u060c 1\u060c 1]</div>\n",
        "\n",
        "<div dir=\"rtl\">* __np.exp__ - \u062a\u0637\u0628\u064a\u0642 \u0639\u0645\u0644\u064a\u0629 \u0627\u0644\u0642\u0648\u0629 \u0639\u0644\u0649 \u0643\u0644 \u0639\u0646\u0635\u0631 \u0641\u064a \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0629</div>\n",
        "\n",
        "<div dir=\"rtl\">* __np.random.normal__ - \u062a\u0648\u0644\u064a\u062f \u0645\u0635\u0641\u0648\u0641\u0629 \u0645\u064f\u0639\u0628\u0623\u0629 \u0628\u0642\u064a\u0645 \u0639\u0634\u0648\u0627\u0626\u064a\u0629 \u0637\u0628\u064a\u0639\u064a\u0629 \u0645\u0639 \u0627\u0646\u062d\u0631\u0627\u0641 \u0645\u0639\u064a\u0627\u0631\u064a \u062a\u062d\u062f\u062f\u0647 \u0627\u0644\u0648\u0633\u064a\u0637 scale \u0648\u0645\u062a\u0648\u0633\u0637 \u064a\u0633\u0627\u0648\u064a \u0627\u0644\u0648\u0633\u064a\u0637 mean. \u064a\u062a\u0645 \u062a\u062d\u062f\u064a\u062f \u0639\u062f\u062f \u0627\u0644\u0639\u0646\u0627\u0635\u0631 \u0641\u064a \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0629 \u0628\u0648\u0627\u0633\u0637\u0629 \u0639\u062f\u062f \u0623\u0648 \u0642\u0627\u0626\u0645\u0629 \u064a\u062a\u0645 \u062a\u0645\u0631\u064a\u0631\u0647\u0627 \u0644\u0644\u0648\u0633\u064a\u0637 size.</div>\n",
        "\n",
        "<div dir=\"rtl\">* __np.random.randint__ - \u062a\u0648\u0644\u064a\u062f \u0645\u0635\u0641\u0648\u0641\u0629 \u0645\u064f\u0639\u0628\u0623\u0629 \u0628\u0623\u0639\u062f\u0627\u062f \u0635\u062d\u064a\u062d\u0629 \u0639\u0634\u0648\u0627\u0626\u064a\u0629 \u0636\u0645\u0646 \u0646\u0637\u0627\u0642 \u064a\u062d\u062f\u062f \u0628\u0637\u0631\u064a\u0642\u0629 \u0645\u0645\u0627\u062b\u0644\u0629 \u0644\u0640 __np.arange__. \u064a\u062a\u0645 \u062a\u062d\u062f\u064a\u062f \u0639\u062f\u062f \u0627\u0644\u0639\u0646\u0627\u0635\u0631 \u0641\u064a \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0629 \u0628\u0648\u0627\u0633\u0637\u0629 \u0639\u062f\u062f \u0623\u0648 \u0642\u0627\u0626\u0645\u0629 \u064a\u062a\u0645 \u062a\u0645\u0631\u064a\u0631\u0647\u0627 \u0644\u0644\u0648\u0633\u064a\u0637 size.</div>\n",
        "\n",
        "<div dir=\"rtl\">* __np.reshape__ - \u062a\u063a\u064a\u064a\u0631 \u0623\u0628\u0639\u0627\u062f \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0629 \u0645\u062a\u0639\u062f\u062f\u0629 \u0627\u0644\u0623\u0628\u0639\u0627\u062f \u0648\u0641\u0642\u064b\u0627 \u0644\u0639\u062f\u062f \u0627\u0644\u0639\u0646\u0627\u0635\u0631. \u064a\u062a\u0645 \u062a\u0645\u0631\u064a\u0631 \u0645\u0635\u0641\u0648\u0641\u0629 \u0645\u062a\u0639\u062f\u062f\u0629 \u0627\u0644\u0623\u0628\u0639\u0627\u062f \u0643\u0648\u0633\u064a\u0637\u060c \u0628\u0627\u0644\u0625\u0636\u0627\u0641\u0629 \u0625\u0644\u0649 \u0642\u0627\u0626\u0645\u0629 \u0623\u0648 \u0645\u062c\u0645\u0648\u0639\u0629 \u0645\u0639 \u0627\u0644\u0623\u0628\u0639\u0627\u062f \u0627\u0644\u062c\u062f\u064a\u062f\u0629. \u0639\u0644\u0649 \u0633\u0628\u064a\u0644 \u0627\u0644\u0645\u062b\u0627\u0644 `np.reshape([0\u060c 1\u060c 2\u060c 3]\u060c (2\u060c2))` \u0633\u062a\u0642\u0648\u0645 \u0628\u0625\u0646\u0634\u0627\u0621 \u0645\u0635\u0641\u0648\u0641\u0629 \u062b\u0646\u0627\u0626\u064a\u0629 \u0627\u0644\u0623\u0628\u0639\u0627\u062f \u0628\u062d\u062c\u0645 2\u00d72. \u0641\u064a \u0647\u0630\u0647 \u0627\u0644\u062d\u0627\u0644\u0629\u060c \u0644\u0627 \u064a\u062a\u0645 \u062a\u062e\u0635\u064a\u0635 \u0645\u0635\u0641\u0648\u0641\u0629 \u062c\u062f\u064a\u062f\u0629 \u0641\u064a \u0627\u0644\u0630\u0627\u0643\u0631\u0629\u060c \u0628\u0644 \u064a\u062a\u0645 \u062a\u063a\u064a\u064a\u0631 \u0637\u0631\u064a\u0642\u0629 \u0627\u0644\u062a\u062c\u0648\u0627\u0644 \u0641\u064a\u0647\u0627. \u064a\u0645\u0643\u0646 \u0623\u064a\u0636\u064b\u0627 \u0627\u0633\u062a\u062f\u0639\u0627\u0621 \u0627\u0644\u062f\u0627\u0644\u0629 \u0628\u0627\u0644\u0637\u0631\u064a\u0642\u0629 \u0627\u0644\u062a\u0627\u0644\u064a\u0629: `arr.reshape(2\u060c 2)`. \u0644\u0627\u062d\u0638 \u0639\u062f\u0645 \u0648\u062c\u0648\u062f \u0623\u0642\u0648\u0627\u0633 \u0625\u0636\u0627\u0641\u064a\u0629. \u0625\u0630\u0627 \u062a\u0645 \u0627\u0633\u062a\u0628\u062f\u0627\u0644 \u0631\u0642\u0645 \u0645\u062d\u062f\u062f \u0628\u0640 -1\u060c \u0633\u064a\u062a\u0645 \u062d\u0633\u0627\u0628 \u0627\u0644\u0623\u0628\u0639\u0627\u062f \u062a\u0644\u0642\u0627\u0626\u064a\u064b\u0627. \u064a\u064f\u0633\u062a\u062e\u062f\u0645 \u0639\u0645\u0644\u064a\u064b\u0627 \u0644\u062a\u0633\u0648\u064a\u0629 \u0627\u0644\u0635\u0648\u0631 \u0641\u064a \u0634\u0643\u0644 \u0645\u0635\u0641\u0648\u0641\u0629 \u0623\u062d\u0627\u062f\u064a\u0629 \u0627\u0644\u0628\u0639\u062f: `X.reshape(-1\u060c 3072)` # [100\u060c 32\u060c 32\u060c 3] -> [100\u060c 3072]</div>\n",
        "\n",
        "<div dir=\"rtl\">* __np.transpose__ - \u0625\u0639\u0627\u062f\u0629 \u062a\u0633\u0645\u064a\u0629 \u0645\u062d\u0627\u0648\u0631 \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0629 \u0645\u062a\u0639\u062f\u062f\u0629 \u0627\u0644\u0623\u0628\u0639\u0627\u062f. \u0641\u064a \u0627\u0644\u0639\u0645\u0644 \u0645\u0639 \u0627\u0644\u0635\u0648\u0631\u060c \u064a\u0648\u062c\u062f \u062a\u0646\u0633\u064a\u0642\u0627\u0646 \u0647\u0645\u0627 NHWC \u0648 NCHW (N - \u0639\u062f\u062f \u0627\u0644\u0635\u0648\u0631 \u0641\u064a \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0629\u060c C - \u0639\u062f\u062f \u0627\u0644\u0642\u0646\u0648\u0627\u062a\u060c H - \u0627\u0644\u0627\u0631\u062a\u0641\u0627\u0639\u060c W - \u0627\u0644\u0639\u0631\u0636). \u064a\u062a\u0645 \u062a\u0645\u0631\u064a\u0631 \u0645\u0635\u0641\u0648\u0641\u0629 \u0645\u062a\u0639\u062f\u062f\u0629 \u0627\u0644\u0623\u0628\u0639\u0627\u062f \u0643\u0648\u0633\u064a\u0637\u060c \u0628\u0627\u0644\u0625\u0636\u0627\u0641\u0629 \u0625\u0644\u0649 \u0642\u0627\u0626\u0645\u0629 \u0623\u0648 \u0645\u062c\u0645\u0648\u0639\u0629 \u0645\u0639 \u0627\u0644\u062a\u0631\u062a\u064a\u0628 \u0627\u0644\u062c\u062f\u064a\u062f \u0644\u0644\u0645\u062d\u0627\u0648\u0631. \u0639\u0644\u0649 \u0633\u0628\u064a\u0644 \u0627\u0644\u0645\u062b\u0627\u0644 `np.transpose([[0\u060c 1\u060c 2\u060c 3]]\u060c (1\u060c0))` \u0633\u062a\u0642\u0648\u0645 \u0628\u0625\u0646\u0634\u0627\u0621 \u0645\u062a\u062c\u0647 \u0639\u0645\u0648\u062f\u064a \u062b\u0646\u0627\u0626\u064a \u0627\u0644\u0623\u0628\u0639\u0627\u062f [[[0]\u060c [1]\u060c [2]\u060c [3]]. \u0644\u0627\u062d\u0638 \u0623\u0646 \u0639\u062f \u0627\u0644\u0645\u062d\u0627\u0648\u0631 \u064a\u0628\u062f\u0623 \u0645\u0646 0. \u064a\u064f\u0633\u062a\u062e\u062f\u0645 \u0639\u0645\u0644\u064a\u064b\u0627 \u0644\u062a\u062d\u0648\u064a\u0644 NHWC \u0625\u0644\u0649 NCHW \u0648\u0627\u0644\u0639\u0643\u0633. \u0641\u064a \u0627\u0644\u062d\u0627\u0644\u0629 \u0627\u0644\u0623\u0648\u0644\u0649\u060c \u062a\u0628\u0642\u0649 \u0627\u0644\u0645\u062d\u0648\u0631 0 N \u0641\u064a \u0645\u0643\u0627\u0646\u0647 \u0627\u0644\u0623\u0648\u0644\u060c \u0628\u064a\u0646\u0645\u0627 \u064a\u062a\u0645 \u062a\u062d\u0631\u064a\u0643 \u0627\u0644\u0645\u062d\u0648\u0631\u064a\u0646 \u0627\u0644\u0623\u0648\u0644 \u0648\u0627\u0644\u062b\u0627\u0646\u064a H \u0648 W \u0625\u0644\u0649 \u0627\u0644\u064a\u0645\u064a\u0646 \u0628\u0645\u0643\u0627\u0646 \u0648\u0627\u062d\u062f\u060c \u0648\u064a\u064f\u0648\u0636\u0639 \u0627\u0644\u0645\u062d\u0648\u0631 \u0627\u0644\u062b\u0627\u0644\u062b - C \u0641\u064a \u0627\u0644\u0645\u0643\u0627\u0646 \u0627\u0644\u062b\u0627\u0646\u064a. \u0623\u064a \u0633\u0646\u062d\u0635\u0644 \u0639\u0644\u0649 \u0627\u0644\u062a\u0631\u062a\u064a\u0628 \u0627\u0644\u062a\u0627\u0644\u064a: [0\u060c 3\u060c 1\u060c 2]</div>\n",
        "\n",
        "<div dir=\"rtl\">* __np.isin__ - \u0645\u0634\u0627\u0628\u0647 \u0644\u0645\u0634\u063a\u0644 SQL IN\u060c \u0641\u062d\u0635 \u0627\u0644\u0639\u0646\u0627\u0635\u0631 \u0648\u0627\u062d\u062f\u064b\u0627 \u062a\u0644\u0648 \u0627\u0644\u0622\u062e\u0631 \u0625\u0630\u0627 \u0643\u0627\u0646\u062a \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0629 \u062a\u0646\u062a\u0645\u064a \u0625\u0644\u0649 \u0645\u062c\u0645\u0648\u0639\u0629. `np.isin([0\u060c 2\u060c 1]\u060c [2\u060c 3])` \u0633\u062a\u0639\u064a\u062f [False\u060c True\u060c False]</div>\n",
        "\n",
        "<div dir=\"rtl\">* __\u0627\u0644\u0641\u0647\u0631\u0633\u0629__ - \u0627\u062e\u062a\u064a\u0627\u0631 \u0645\u0635\u0641\u0648\u0641\u0629 \u0641\u0631\u0639\u064a\u0629 \u0623\u0648 \u0634\u0631\u064a\u062d\u0629 \u0645\u0646 \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0629 \u064a\u062a\u0645 \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0627\u0644\u0623\u0642\u0648\u0627\u0633 \u0627\u0644\u0645\u0631\u0628\u0639\u0629 []. \u0625\u0630\u0627 \u0643\u0627\u0646 `arr = np.array([2\u060c 1\u060c 0])`\u060c \u0641\u0625\u0646 `arr[0]` \u0633\u062a\u0639\u064a\u062f \u0627\u0644\u0639\u0646\u0635\u0631 \u0627\u0644\u0623\u0648\u0644. `arr[[0\u060c 1]]` - \u0627\u0644\u0648\u0635\u0648\u0644 \u0628\u0648\u0627\u0633\u0637\u0629 \u0627\u0644\u0641\u0647\u0631\u0633\u060c `arr[[True\u060c False\u060c True]]` - \u0627\u0644\u0648\u0635\u0648\u0644 \u0628\u0648\u0627\u0633\u0637\u0629 \u0627\u0644\u0642\u0646\u0627\u0639 \u0627\u0644\u0628\u0648\u0644\u064a. \u0644\u0627\u062d\u0638 \u0623\u0646 \u0627\u0644\u0648\u0635\u0648\u0644 \u0628\u0648\u0627\u0633\u0637\u0629 \u0627\u0644\u0641\u0647\u0631\u0633 \u0644\u064a\u0633 \u0628\u0627\u0644\u0636\u0631\u0648\u0631\u0629 \u0623\u0646 \u064a\u062a\u0648\u0627\u0641\u0642 \u0645\u0639 \u0623\u0628\u0639\u0627\u062f \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0629\u060c \u0639\u0644\u0649 \u0639\u0643\u0633 \u0627\u0644\u0648\u0635\u0648\u0644 \u0628\u0648\u0627\u0633\u0637\u0629 \u0627\u0644\u0642\u0646\u0627\u0639. \u0645\u0646 \u0627\u0644\u0639\u0645\u0644\u064a \u062a\u0633\u062c\u064a\u0644 \u0642\u064a\u0645 \u0627\u0644\u0642\u0646\u0627\u0639 \u0641\u064a \u0645\u062a\u063a\u064a\u0631 \u0645\u0646\u0641\u0635\u0644. \u0644\u0627\u062e\u062a\u064a\u0627\u0631 \u0639\u0645\u0648\u062f \u0645\u0639\u064a\u0646 \u0641\u064a \u0645\u0635\u0641\u0648\u0641\u0629 \u0645\u062a\u0639\u062f\u062f\u0629 \u0627\u0644\u0623\u0628\u0639\u0627\u062f\u060c \u064a\u062a\u0645 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0628\u0646\u0627\u0621 \u0627\u0644\u062c\u0645\u0644\u0629 \u0627\u0644\u0634\u0631\u0627\u0626\u062d\u064a [:\u060c k]\u060c \u062d\u064a\u062b k \u0647\u0648 \u0631\u0642\u0645 \u0627\u0644\u0639\u0645\u0648\u062f. \u0625\u0630\u0627 \u0643\u0627\u0646\u062a k \u062a\u0633\u0627\u0648\u064a -1\u060c \u0641\u0625\u0646 \u0627\u0644\u0639\u0645\u0648\u062f \u0627\u0644\u0623\u062e\u064a\u0631 \u0623\u0648 \u0627\u0644\u0639\u0646\u0635\u0631 \u064a\u062a\u0645 \u0627\u0633\u062a\u062e\u062f\u0627\u0645\u0647. \u0639\u0644\u0649 \u0633\u0628\u064a\u0644 \u0627\u0644\u0645\u062b\u0627\u0644\u060c \u0628\u0627\u0644\u0646\u0633\u0628\u0629 \u0644\u0644\u0645\u0635\u0641\u0648\u0641\u0629 `arr = np.array([[0\u060c 1]\u060c [2\u060c 3]\u060c [4\u060c 5])` \u0627\u0644\u062a\u0639\u0628\u064a\u0631 `arr[:, 0]` \u0633\u064a\u0639\u064a\u062f \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0629 [0\u060c 2\u060c 4]. \u0646\u0638\u0631\u064b\u0627 \u0644\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0641\u0647\u0631\u0633 \u0627\u0644\u0634\u0631\u0627\u0626\u062d (\u0628\u0646\u0627\u0621 \u0627\u0644\u062c\u0645\u0644\u0629 \u0627\u0644\u0642\u064a\u0627\u0633\u064a \u0641\u064a Python)\u060c \u064a\u0645\u0643\u0646 \u0623\u064a\u0636\u064b\u0627 \u062a\u0646\u0641\u064a\u0630 \u0634\u0631\u0627\u0626\u062d \u0644\u0644\u0645\u0635\u0641\u0648\u0641\u0627\u062a \u0645\u062a\u0639\u062f\u062f\u0629 \u0627\u0644\u0623\u0628\u0639\u0627\u062f. \u0628\u0627\u0644\u0646\u0633\u0628\u0629 \u0644\u0644\u0645\u062b\u0627\u0644 \u0627\u0644\u0633\u0627\u0628\u0642\u060c `arr[1:2\u060c 0:1]` \u0633\u062a\u0639\u064a\u062f [[2]]</div>\n",
        "\n",
        "<div dir=\"rtl\">* __np.unique__ - \u0645\u0634\u0627\u0628\u0647 \u0644\u0640 SELECT DISTINCT \u0641\u064a SQL. \u0639\u0646\u062f \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0627\u0644\u0645\u0639\u0644\u0645\u0627\u062a \u0627\u0644\u0627\u0641\u062a\u0631\u0627\u0636\u064a\u0629\u060c \u064a\u0639\u064a\u062f \u0645\u0635\u0641\u0648\u0641\u0629 \u0641\u0631\u0639\u064a\u0629 \u0623\u062d\u0627\u062f\u064a\u0629 \u0627\u0644\u0628\u0639\u062f \u062a\u062d\u062a\u0648\u064a \u0639\u0644\u0649 \u0639\u0646\u0627\u0635\u0631 \u0641\u0631\u064a\u062f\u0629. \u0625\u0630\u0627 \u062a\u0645 \u062a\u062d\u062f\u064a\u062f \u0627\u0644\u0639\u0644\u0645 __return_inverse__\u060c \u0641\u0633\u064a\u062a\u0645 \u0625\u0631\u062c\u0627\u0639 \u0645\u0635\u0641\u0648\u0641\u0629 \u0628\u0623\u0631\u0642\u0627\u0645 \u0627\u0644\u0641\u0647\u0631\u0633 \u0644\u0644\u0639\u0646\u0627\u0635\u0631 \u0627\u0644\u0641\u0631\u064a\u062f\u0629. \u0641\u064a \u0627\u0644\u0623\u0633\u0627\u0633\u060c \u064a\u062a\u0645 \u062a\u0646\u0641\u064a\u0630 \u0627\u0644\u062a\u0631\u0645\u064a\u0632 \u0627\u0644\u062a\u0633\u0645\u064a\u0627\u062a\u064a</div>\n",
        "\n",
        "<div dir=\"rtl\">* __np.concatenate__ - \u062f\u0645\u062c \u0645\u0635\u0641\u0648\u0641\u0629 \u0645\u062a\u0639\u062f\u062f\u0629 \u0627\u0644\u0623\u0628\u0639\u0627\u062f \u0639\u0644\u0649 \u0637\u0648\u0644 \u0627\u0644\u0645\u062d\u0648\u0631 \u0627\u0644\u0645\u062d\u062f\u062f. \u064a\u062a\u0645 \u062a\u062d\u062f\u064a\u062f \u0631\u0642\u0645 \u0627\u0644\u0645\u062d\u0648\u0631 \u0645\u0646 \u062e\u0644\u0627\u0644 \u0627\u0644\u0648\u0633\u064a\u0637 __axis__. \u0639\u0644\u0649 \u0633\u0628\u064a\u0644 \u0627\u0644\u0645\u062b\u0627\u0644\u060c \u064a\u0645\u0643\u0646 \u0627\u0633\u062a\u062e\u062f\u0627\u0645\u0647 \u0644\u062f\u0645\u062c \u0645\u064a\u0632\u0627\u062a \u0645\u062a\u0639\u062f\u062f\u0629 \u0623\u0648 \u0645\u062c\u0645\u0648\u0639\u0627\u062a \u0628\u064a\u0627\u0646\u0627\u062a \u0645\u062a\u0639\u062f\u062f\u0629. \u0641\u064a \u0633\u064a\u0627\u0642 \u0627\u0644\u0635\u0648\u0631\u060c \u064a\u0645\u0643\u0646 \u0627\u0633\u062a\u062e\u062f\u0627\u0645\u0647 \u0644\u062f\u0645\u062c \u0623\u0648 \u0644\u0635\u0642 \u0635\u0648\u0631 \u0645\u062a\u0639\u062f\u062f\u0629 \u0641\u064a \u0635\u0648\u0631\u0629 \u0648\u0627\u062d\u062f\u0629 \u0628\u0634\u0643\u0644 \u0639\u0645\u0648\u062f\u064a \u0623\u0648 \u0623\u0641\u0642\u064a. \u0641\u064a \u0633\u064a\u0627\u0642 \u0627\u0644\u0635\u0648\u062a - \u062f\u0645\u062c \u0645\u0633\u0627\u0631\u064a\u0646 \u0635\u0648\u062a\u064a\u064a\u0646.</div>\n",
        "\n",
        "<div dir=\"rtl\">* __np.max__\u060c __np.min__ - \u062a\u0639\u064a\u062f \u0627\u0644\u0639\u0646\u0627\u0635\u0631 \u0627\u0644\u0642\u0635\u0648\u0649 \u0648\u0627\u0644\u062f\u0646\u064a\u0627 \u0644\u0644\u0645\u0635\u0641\u0648\u0641\u0629 \u0639\u0644\u0649 \u0637\u0648\u0644 \u0627\u0644\u0645\u062d\u0648\u0631 \u0627\u0644\u0645\u062d\u062f\u062f\u060c \u0639\u0644\u0649 \u0627\u0644\u062a\u0648\u0627\u0644\u064a. \u0625\u0630\u0627 \u0644\u0645 \u064a\u062a\u0645 \u062a\u062d\u062f\u064a\u062f \u0631\u0642\u0645 \u0627\u0644\u0645\u062d\u0648\u0631\u060c \u064a\u062a\u0645 \u0625\u0631\u062c\u0627\u0639 \u0631\u0642\u0645. \u064a\u062a\u0645 \u062a\u062d\u062f\u064a\u062f \u0631\u0642\u0645 \u0627\u0644\u0645\u062d\u0648\u0631 \u0645\u0646 \u062e\u0644\u0627\u0644 \u0627\u0644\u0648\u0633\u064a\u0637 __axis__. \u0625\u0630\u0627 \u062a\u0645 \u062a\u062d\u062f\u064a\u062f -1\u060c \u0641\u064a\u0641\u062a\u0631\u0636 \u0623\u0646 \u064a\u062a\u0645 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0627\u0644\u0631\u0642\u0645 \u0627\u0644\u0623\u062e\u064a\u0631 \u0644\u0644\u0645\u062d\u0648\u0631. \u064a\u0645\u0643\u0646 \u0623\u064a\u0636\u064b\u0627 \u0627\u0633\u062a\u062f\u0639\u0627\u0621 \u0627\u0644\u062f\u0627\u0644\u0629 \u0643\u0637\u0631\u064a\u0642\u0629 \u0644\u0644\u0645\u0635\u0641\u0648\u0641\u0629 \u0645\u062a\u0639\u062f\u062f\u0629 \u0627\u0644\u0623\u0628\u0639\u0627\u062f: `arr.max()`</div>\n",
        "\n",
        "<div dir=\"rtl\">* __np.argmax__ - \u064a\u0639\u064a\u062f \u0641\u0647\u0631\u0633 \u0627\u0644\u0639\u0646\u0635\u0631 \u0627\u0644\u0623\u0642\u0635\u0649 \u0644\u0644\u0645\u0635\u0641\u0648\u0641\u0629 \u0639\u0644\u0649 \u0637\u0648\u0644 \u0627\u0644\u0645\u062d\u0648\u0631 \u0627\u0644\u0645\u062d\u062f\u062f. \u0625\u0630\u0627 \u0644\u0645 \u064a\u062a\u0645 \u062a\u062d\u062f\u064a\u062f \u0631\u0642\u0645 \u0627\u0644\u0645\u062d\u0648\u0631\u060c \u064a\u062a\u0645 \u0625\u0631\u062c\u0627\u0639 \u0623\u0648\u0644 \u0641\u0647\u0631\u0633 \u064a\u062a\u0648\u0627\u0641\u0642 \u0645\u0639 \u0627\u0644\u0642\u064a\u0645\u0629 \u0627\u0644\u0642\u0635\u0648\u0649 \u0641\u064a \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0629\u060c \u0623\u064a \u0631\u0642\u0645 \u0648\u0627\u062d\u062f. \u064a\u062a\u0645 \u062a\u062d\u062f\u064a\u062f \u0631\u0642\u0645 \u0627\u0644\u0645\u062d\u0648\u0631 \u0645\u0646 \u062e\u0644\u0627\u0644 \u0627\u0644\u0648\u0633\u064a\u0637 __axis__. \u0625\u0630\u0627 \u062a\u0645 \u062a\u062d\u062f\u064a\u062f -1\u060c \u0641\u064a\u0641\u062a\u0631\u0636 \u0623\u0646 \u064a\u062a\u0645 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0627\u0644\u0631\u0642\u0645 \u0627\u0644\u0623\u062e\u064a\u0631 \u0644\u0644\u0645\u062d\u0648\u0631. \u0641\u064a \u0627\u0644\u0645\u0645\u0627\u0631\u0633\u0629 \u0627\u0644\u0639\u0645\u0644\u064a\u0629\u060c \u064a\u062a\u0645 \u0627\u0633\u062a\u062e\u062f\u0627\u0645\u0647 \u0644\u062d\u0633\u0627\u0628 \u0645\u0642\u064a\u0627\u0633 \u062f\u0642\u0629 \u0646\u0645\u0648\u0630\u062c \u0627\u0644\u0625\u062c\u0627\u0628\u0627\u062a \u0627\u0644\u0635\u062d\u064a\u062d\u0629 (Accuracy). \u064a\u0645\u0643\u0646 \u0623\u064a\u0636\u064b\u0627 \u0627\u0633\u062a\u062f\u0639\u0627\u0621 \u0627\u0644\u062f\u0627\u0644\u0629 \u0643\u0637\u0631\u064a\u0642\u0629 \u0644\u0644\u0645\u0635\u0641\u0648\u0641\u0629 \u0645\u062a\u0639\u062f\u062f\u0629 \u0627\u0644\u0623\u0628\u0639\u0627\u062f: `arr.argmax(axis=-1)`</div>\n",
        "\n",
        "`\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\"><h3>\u0637\u0631\u0642 \u0648\u0648\u0638\u0627\u0626\u0641 Pickle</h3></div>\n",
        "\n",
        "\n",
        "\n",
        "(documation: https://docs.python.org/3/library/pickle.html)\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">* __pickle.dump__ - \u062a\u0633\u0644\u0633\u0644 \u0647\u064a\u0643\u0644 \u0628\u064a\u0627\u0646\u0627\u062a Python. \u064a\u062a\u0645 \u062a\u0645\u0631\u064a\u0631 \u0627\u0644\u0647\u064a\u0643\u0644 \u0646\u0641\u0633\u0647 \u0643\u0645\u0639\u0627\u0645\u0644 \u0623\u0648\u0644\u060c \u0648FileObject \u0643\u0645\u0639\u0627\u0645\u0644 \u062b\u0627\u0646\u064a. \u064a\u062c\u0628 \u0623\u0646 \u064a\u062a\u0645 \u0641\u062a\u062d FileObject \u0641\u064a \u0648\u0636\u0639 \u0643\u062a\u0627\u0628\u0629 \u0627\u0644\u0628\u0627\u064a\u062a\u0627\u062a (wb). \u064a\u0645\u0643\u0646 \u062a\u062d\u062f\u064a\u062f \u062a\u0631\u0645\u064a\u0632 \u0627\u0644\u0628\u0627\u064a\u062a (big endian/ little endian). \u0647\u0630\u0627 \u064a\u0633\u0645\u062d \u0628\u062a\u062e\u0632\u064a\u0646 \u0647\u064a\u0627\u0643\u0644 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0627\u0644\u0642\u064a\u0627\u0633\u064a\u0629\u060c \u0628\u0645\u0627 \u0641\u064a \u0630\u0644\u0643 \u0645\u0635\u0641\u0648\u0641\u0627\u062a NumPy\u060c \u0628\u0634\u0643\u0644 \u062f\u0627\u0626\u0645 \u0639\u0644\u0649 \u0627\u0644\u0648\u0633\u0627\u0626\u0637.</div>\n",
        "\n",
        "<div dir=\"rtl\">* __pickle.load__ - \u0641\u0643 \u062a\u0633\u0644\u0633\u0644 \u0647\u064a\u0643\u0644 \u0628\u064a\u0627\u0646\u0627\u062a Python. \u064a\u062a\u0645 \u062a\u0645\u0631\u064a\u0631 FileObject \u0643\u0645\u0639\u0627\u0645\u0644 \u0623\u0648\u0644. \u064a\u062c\u0628 \u0623\u0646 \u064a\u062a\u0645 \u0641\u062a\u062d FileObject \u0641\u064a \u0648\u0636\u0639 \u0642\u0631\u0627\u0621\u0629 \u0627\u0644\u0628\u0627\u064a\u062a\u0627\u062a (rb). \u064a\u0645\u0643\u0646 \u062a\u062d\u062f\u064a\u062f \u062a\u0631\u0645\u064a\u0632 \u0627\u0644\u0628\u0627\u064a\u062a (big endian/ little endian). \u0647\u0630\u0627 \u064a\u0633\u0645\u062d \u0628\u062a\u062d\u0645\u064a\u0644 \u0627\u0644\u0647\u064a\u0627\u0643\u0644 \u0627\u0644\u062a\u064a \u062a\u0645 \u062a\u062e\u0632\u064a\u0646\u0647\u0627 \u0633\u0627\u0628\u0642\u0627\u064b\u060c \u0645\u0645\u0627 \u064a\u0645\u0643\u0646 \u0623\u0646 \u064a\u0643\u0648\u0646 \u0645\u0641\u064a\u062f\u0627\u064b \u0625\u0630\u0627 \u0643\u0627\u0646 \u0625\u0639\u062f\u0627\u062f\u0647\u0627 \u064a\u0633\u062a\u063a\u0631\u0642 \u0648\u0642\u062a\u0627\u064b \u0637\u0648\u064a\u0644\u0627\u064b (\u0645\u062b\u0644 \u0625\u0639\u062f\u0627\u062f \u0645\u0639\u0644\u0645\u0627\u062a \u0646\u0645\u0648\u0630\u062c \u0627\u0644\u062a\u0639\u0644\u0645 \u0627\u0644\u0639\u0645\u064a\u0642).</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">### \u0637\u0631\u0642 \u0648\u0648\u0638\u0627\u0626\u0641 Sklearn</div>\n",
        "\n",
        "<div dir=\"rtl\">(\u0627\u0644\u062a\u0648\u062b\u064a\u0642: https://scikit-learn.org/stable/modules/classes.html)</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">* __datasets.make_circles__\u060c __datasets.make_moons__ - \u0625\u0646\u0634\u0627\u0621 \u0645\u062c\u0645\u0648\u0639\u0629 \u0628\u064a\u0627\u0646\u0627\u062a \u062a\u062f\u0631\u064a\u0628 \u0627\u0635\u0637\u0646\u0627\u0639\u064a\u0629 \u0644\u0645\u0647\u0627\u0645 \u0627\u0644\u062a\u0635\u0646\u064a\u0641. \u062a\u064f\u0631\u062c\u0639 X \u0643\u0645\u0635\u0641\u0648\u0641\u0629 \u062b\u0646\u0627\u0626\u064a\u0629 \u0627\u0644\u0623\u0628\u0639\u0627\u062f \u0628\u0639\u062f\u062f \u0627\u0644\u0623\u0645\u062b\u0644\u0629 \u0648\u0627\u0644\u0645\u064a\u0632\u0627\u062a (2 \u0645\u064a\u0632\u0627\u062a)\u060c \u0628\u0627\u0644\u0625\u0636\u0627\u0641\u0629 \u0625\u0644\u0649 \u0645\u0635\u0641\u0648\u0641\u0629 \u0623\u062d\u0627\u062f\u064a\u0629 \u0627\u0644\u0623\u0628\u0639\u0627\u062f \u0628\u062a\u0633\u0645\u064a\u0627\u062a \u0627\u0644\u0641\u0626\u0627\u062a (0 \u0623\u0648 1).</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">* __metrics.classification_report__ - \u062a\u0646\u0634\u0626 \u062a\u0642\u0631\u064a\u0631\u0627\u064b \u0646\u0635\u064a\u0627\u064b \u064a\u0639\u0631\u0636 \u0627\u0644\u0645\u0642\u0627\u064a\u064a\u0633 \u0627\u0644\u0631\u0626\u064a\u0633\u064a\u0629 \u0644\u0644\u062a\u0635\u0646\u064a\u0641 (\u062f\u0642\u0629 \u0627\u0644\u0625\u062c\u0627\u0628\u0629\u060c \u0627\u0644\u0627\u0633\u062a\u0631\u062c\u0627\u0639\u060c \u0627\u0644\u062f\u0642\u0629\u060c \u0645\u0642\u064a\u0627\u0633 f1). \u064a\u064f\u0645\u0631\u0631 \u0645\u0635\u0641\u0648\u0641\u0629 \u0627\u0644\u0639\u0644\u0627\u0645\u0627\u062a \u0627\u0644\u062d\u0642\u064a\u0642\u064a\u0629 \u0643\u0645\u0639\u0627\u0645\u0644 \u0623\u0648\u0644\u060c \u0648\u0645\u0635\u0641\u0648\u0641\u0629 \u0627\u0644\u0639\u0644\u0627\u0645\u0627\u062a \u0627\u0644\u0645\u062a\u0648\u0642\u0639\u0629 \u0645\u0646 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0643\u0645\u0639\u0627\u0645\u0644 \u062b\u0627\u0646\u064a. \u0627\u0644\u0645\u0639\u0627\u0645\u0644\u0627\u062a \u0627\u0644\u0625\u0636\u0627\u0641\u064a\u0629 \u0627\u0644\u0645\u0641\u064a\u062f\u0629: digits - \u0639\u062f\u062f \u0627\u0644\u0623\u0631\u0642\u0627\u0645 \u0628\u0639\u062f \u0627\u0644\u0641\u0627\u0635\u0644\u0629 (\u0627\u0641\u062a\u0631\u0627\u0636\u064a 2)\u060c output_dict - \u062a\u064f\u0631\u062c\u0639 \u0642\u0627\u0645\u0648\u0633\u0627\u064b \u064a\u062d\u062a\u0648\u064a \u0639\u0644\u0649 \u0627\u0644\u0645\u0642\u0627\u064a\u064a\u0633 \u0627\u0644\u0645\u062d\u0633\u0648\u0628\u0629 \u0628\u062f\u0644\u0627\u064b \u0645\u0646 \u0633\u0644\u0633\u0644\u0629 \u0646\u0635\u064a\u0629\u060c sample_weight - \u062a\u062d\u0633\u0628 \u0627\u0644\u0645\u0642\u0627\u064a\u064a\u0633 \u0627\u0644\u0645\u0631\u062c\u062d\u0629 \u0628\u0646\u0627\u0621\u064b \u0639\u0644\u0649 \u0648\u0632\u0646 \u0643\u0644 \u0645\u062b\u0627\u0644.</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">* __metrics.confusion_matrix__ - \u062a\u062d\u0633\u0628 \u0645\u0635\u0641\u0648\u0641\u0629 \u0627\u0644\u0627\u0631\u062a\u0628\u0627\u0643 \u0644\u0644\u0646\u0645\u0648\u0630\u062c \u0644\u062a\u0642\u064a\u064a\u0645 \u062f\u0642\u0629 \u0627\u0644\u062a\u0635\u0646\u064a\u0641. \u062a\u062d\u062a\u0648\u064a \u0645\u0635\u0641\u0648\u0641\u0629 \u0627\u0644\u0627\u0631\u062a\u0628\u0627\u0643 \u0644\u0644\u0646\u0645\u0648\u0630\u062c \u0627\u0644\u0645\u062b\u0627\u0644\u064a \u0639\u0644\u0649 \u0642\u064a\u0645 \u0641\u0642\u0637 \u0639\u0644\u0649 \u0627\u0644\u0642\u0637\u0631 \u0627\u0644\u0631\u0626\u064a\u0633\u064a. \u064a\u0645\u0643\u0646 \u0627\u0633\u062a\u062e\u062f\u0627\u0645\u0647\u0627 \u0644\u062d\u0633\u0627\u0628 \u062c\u0645\u064a\u0639 \u0645\u0642\u0627\u064a\u064a\u0633 \u0627\u0644\u062a\u0635\u0646\u064a\u0641 \u0627\u0644\u062a\u0642\u0644\u064a\u062f\u064a\u0629 (\u062f\u0642\u0629 \u0627\u0644\u0625\u062c\u0627\u0628\u0629\u060c \u0627\u0644\u0627\u0633\u062a\u0631\u062c\u0627\u0639\u060c \u0627\u0644\u062f\u0642\u0629\u060c \u0627\u0644\u0646\u0648\u0639\u064a\u0629\u060c \u0645\u0642\u064a\u0627\u0633 f1).</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\"><h3>\u0637\u0631\u0642 \u0648\u0648\u0638\u0627\u0626\u0641 PIL</h3></div>\n",
        "\n",
        "\n",
        "\n",
        "(documation: https://pillow.readthedocs.io/en/stable/)\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">* __Image.fromarray__ - \u064a\u0646\u0634\u0626 \u0643\u0627\u0626\u0646 Image \u0645\u0646 \u0645\u0635\u0641\u0648\u0641\u0629 \u062b\u0646\u0627\u0626\u064a\u0629 \u0627\u0644\u0623\u0628\u0639\u0627\u062f \u0623\u0648 \u0645\u0635\u0641\u0648\u0641\u0629 \u062b\u0646\u0627\u0626\u064a\u0629 \u0627\u0644\u0623\u0628\u0639\u0627\u062f \u062a\u062d\u062a\u0648\u064a \u0639\u0644\u0649 \u0642\u0646\u0648\u0627\u062a. \u063a\u0627\u0644\u0628\u064b\u0627 \u0645\u0627 \u064a\u0638\u0647\u0631 \u062e\u0637\u0623 \u0625\u0630\u0627 \u0644\u0645 \u064a\u0643\u0646 \u0646\u0648\u0639 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a uint8. \u0642\u062f \u064a\u0638\u0647\u0631 \u062e\u0637\u0623 \u0623\u064a\u0636\u064b\u0627 \u0639\u0646\u062f \u0645\u062d\u0627\u0648\u0644\u0629 \u0625\u0646\u0634\u0627\u0621 \u0635\u0648\u0631\u0629 \u0628\u0627\u0644\u0623\u0628\u064a\u0636 \u0648\u0627\u0644\u0623\u0633\u0648\u062f \u0645\u0646 \u0635\u0648\u0631\u0629 \u0628\u0623\u0628\u0639\u0627\u062f (W, H, 1). \u0644\u0627\u0633\u062a\u0631\u062c\u0627\u0639 \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0629 \u0645\u0646 \u0643\u0627\u0626\u0646 Image\u060c \u064a\u0645\u0643\u0646 \u062a\u062d\u0648\u064a\u0644\u0647\u0627 \u0625\u0644\u0649 \u0645\u0635\u0641\u0648\u0641\u0629 NumPy\u060c \u0639\u0644\u0649 \u0633\u0628\u064a\u0644 \u0627\u0644\u0645\u062b\u0627\u0644 np.array(img).</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">* __Image.resize__ - \u064a\u063a\u064a\u0651\u0631 \u062f\u0642\u0629 \u0627\u0644\u0635\u0648\u0631\u0629 \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0627\u0644\u0627\u0633\u062a\u064a\u0641\u0627\u0621. \u064a\u064f\u062d\u062f\u062f \u0643\u0623\u0648\u0644 \u0645\u0639\u0627\u0645\u0644 \u0642\u0627\u0626\u0645\u0629 \u062a\u062d\u062a\u0648\u064a \u0639\u0644\u0649 \u0627\u0644\u0639\u0631\u0636 \u0648\u0627\u0644\u0627\u0631\u062a\u0641\u0627\u0639 \u0627\u0644\u062c\u062f\u064a\u062f\u064a\u0646 \u0644\u0644\u0635\u0648\u0631\u0629. \u064a\u0645\u0643\u0646 \u0623\u064a\u0636\u064b\u0627 \u062a\u062d\u062f\u064a\u062f \u0646\u0648\u0639 \u0627\u0644\u0627\u0633\u062a\u064a\u0641\u0627\u0621 \u0645\u0646 \u062e\u0644\u0627\u0644 \u0645\u0639\u0627\u0645\u0644 resample. \u0627\u0644\u0642\u064a\u0645 \u0627\u0644\u0645\u062f\u0639\u0648\u0645\u0629: PIL.Image.NEAREST\u060c PIL.Image.BOX\u060c PIL.Image.BILINEAR\u060c PIL.Image.HAMMING\u060c PIL.Image.BICUBIC\u060c PIL.Image.LANCZOS. \u064a\u0633\u062a\u062e\u062f\u0645 \u0627\u0633\u062a\u064a\u0641\u0627\u0621 \u0628\u064a\u0643\u0648\u0628\u064a\u0643\u064a \u0628\u0634\u0643\u0644 \u0627\u0641\u062a\u0631\u0627\u0636\u064a.</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">* __Image.convert__ - \u064a\u062d\u0648\u0644 \u0627\u0644\u0635\u0648\u0631\u0629 \u0645\u0646 \u0635\u064a\u063a\u0629 \u0623\u0644\u0648\u0627\u0646 \u0625\u0644\u0649 \u0623\u062e\u0631\u0649. \u062a\u064f\u062d\u062f\u062f \u0635\u064a\u063a\u0629 \u0627\u0644\u0623\u0644\u0648\u0627\u0646 \u0627\u0644\u062c\u062f\u064a\u062f\u0629 \u0643\u0633\u0644\u0633\u0644\u0629 \u0646\u0635\u064a\u0629\u060c \u062d\u064a\u062b L \u0644\u0644\u0623\u0628\u064a\u0636 \u0648\u0627\u0644\u0623\u0633\u0648\u062f\u060c LA \u0644\u0644\u0623\u0628\u064a\u0636 \u0648\u0627\u0644\u0623\u0633\u0648\u062f \u0645\u0639 \u0627\u0644\u0634\u0641\u0627\u0641\u064a\u0629\u060c RGB \u0644\u0644\u0635\u064a\u063a\u0629 \u0627\u0644\u0642\u064a\u0627\u0633\u064a\u0629 \u0630\u0627\u062a 3 \u0642\u0646\u0648\u0627\u062a\u060c RGBA \u0644\u0644\u0635\u064a\u063a\u0629 \u0627\u0644\u0642\u064a\u0627\u0633\u064a\u0629 \u0645\u0639 3 \u0642\u0646\u0648\u0627\u062a \u0644\u0648\u0646 \u0648\u0642\u0646\u0627\u0629 \u0634\u0641\u0627\u0641\u064a\u0629\u060c HSV \u0644\u062a\u0645\u062b\u064a\u0644 \u0623\u0644\u0648\u0627\u0646 \u0628\u062f\u064a\u0644\u060c \u0648\u0647\u0643\u0630\u0627.</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">* __Image.open__ - \u064a\u0642\u0631\u0623 \u0627\u0644\u0635\u0648\u0631\u0629 \u0645\u0646 \u0627\u0644\u0645\u0633\u0627\u0631 \u0627\u0644\u0645\u062d\u062f\u062f \u0643\u0635\u064a\u063a\u0629 \u0646\u0635\u064a\u0629 \u0623\u0648 \u0643\u0627\u0626\u0646 \u0645\u0644\u0641. \u0639\u0646\u062f \u0625\u0646\u0634\u0627\u0621 \u0645\u062c\u0645\u0648\u0639\u0629 \u0628\u064a\u0627\u0646\u0627\u062a\u060c \u0642\u062f \u064a\u062d\u062f\u062f \u0627\u0644\u062a\u0646\u0633\u064a\u0642 \u0628\u0634\u0643\u0644 \u063a\u064a\u0631 \u0635\u062d\u064a\u062d (\u0645\u062b\u0644 L \u0628\u062f\u0644\u0627\u064b \u0645\u0646 RGB)\u060c \u0644\u0630\u0627 \u064a\u064f\u0641\u0636\u0644 \u062a\u062d\u0648\u064a\u0644 \u0627\u0644\u062a\u0646\u0633\u064a\u0642 \u0645\u0628\u0627\u0634\u0631\u0629\u064b \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u062f\u0627\u0644\u0629 convert \u0628\u0639\u062f \u0627\u0644\u0641\u062a\u062d.</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">* __Image.save__ - \u064a\u062d\u0641\u0638 \u0627\u0644\u0635\u0648\u0631\u0629 \u0641\u064a \u0627\u0644\u0645\u0633\u0627\u0631 \u0627\u0644\u0645\u062d\u062f\u062f \u0643\u0635\u064a\u063a\u0629 \u0646\u0635\u064a\u0629 \u0623\u0648 \u0643\u0627\u0626\u0646 \u0645\u0644\u0641. \u0625\u0630\u0627 \u0643\u0627\u0646 \u0627\u0644\u0645\u0633\u0627\u0631 \u0643\u0627\u0626\u0646 \u0645\u0644\u0641\u060c \u0641\u064a\u062c\u0628 \u0623\u064a\u0636\u064b\u0627 \u062a\u062d\u062f\u064a\u062f \u0635\u064a\u063a\u0629 \u0627\u0644\u0635\u0648\u0631\u0629 \u0641\u064a \u0645\u0639\u0627\u0645\u0644 format\u060c \u0645\u062b\u0644 'PNG' \u0623\u0648 'JPEG'.</div>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\"><h3> Matplotlib \u0637\u0631\u0642 \u0648\u0648\u0638\u0627\u0626\u0641 PIL</h3></div>\n",
        "\n",
        "\n",
        "\n",
        "(documation: https://matplotlib.org/stable/api/index.html)\n",
        "\n",
        "\n",
        "\n",
        "\u0627\u0644\u0627\u062e\u062a\u0635\u0627\u0631\u0627\u062a \u0627\u0644\u0645\u0642\u0628\u0648\u0644\u0629:\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "* matplotlib.pyplot - plt\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "\u0637\u064f\u0631\u0642:\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">* __plt.plot__ - \u064a\u0631\u0633\u0645 \u0631\u0633\u0645\u064b\u0627 \u0628\u064a\u0627\u0646\u064a\u064b\u0627 \u062d\u0633\u0628 \u0627\u0644\u0646\u0642\u0627\u0637 \u0648\u064a\u0642\u0648\u0645 \u0628\u062a\u0648\u0635\u064a\u0644\u0647\u0627 \u0628\u062e\u0637. \u064a\u064f\u0645\u0631\u0631 \u0643\u0623\u0648\u0644 \u0648\u0633\u064a\u0637 \u0625\u062d\u062f\u0627\u062b\u064a\u0627\u062a x\u060c \u0648\u0643\u062b\u0627\u0646\u064a \u0648\u0633\u064a\u0637 \u0625\u062d\u062f\u0627\u062b\u064a\u0627\u062a y. \u0625\u0630\u0627 \u0644\u0645 \u064a\u062a\u0645 \u062a\u0645\u0631\u064a\u0631 \u0627\u0644\u0648\u0633\u064a\u0637 \u0627\u0644\u062b\u0627\u0646\u064a\u060c \u0633\u064a\u062a\u0645 \u0627\u0639\u062a\u0628\u0627\u0631 \u0625\u062d\u062f\u0627\u062b\u064a\u0627\u062a x \u0643\u0640 y\u060c \u0648\u0633\u064a\u062a\u0645 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0639\u062f\u062f \u0639\u0646\u0627\u0635\u0631 \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0629 \u0643\u0625\u062d\u062f\u0627\u062b\u064a\u0627\u062a x. \u0647\u0646\u0627\u0643 \u0628\u0639\u0636 \u0627\u0644\u0645\u0639\u0627\u0645\u0644\u0627\u062a \u0627\u0644\u0625\u0636\u0627\u0641\u064a\u0629 \u0627\u0644\u0645\u0641\u064a\u062f\u0629: linestyle - \u0646\u0648\u0639 \u0627\u0644\u062e\u0637 \u0627\u0644\u0645\u0639\u0631\u0648\u0636 ('--'\u060c '-'\u060c '-.'\u060c \u0625\u0644\u062e)\u060c color - \u0644\u0648\u0646 \u0627\u0644\u062e\u0637 ('k' - \u0623\u0633\u0648\u062f\u060c 'r' - \u0623\u062d\u0645\u0631\u060c 'white' - \u0623\u0628\u064a\u0636\u060c \u0625\u0644\u062e)\u060c alpha - \u0634\u0641\u0627\u0641\u064a\u0629 \u0627\u0644\u062e\u0637\u060c \u0642\u064a\u0645\u0629 \u0645\u0646 0 (\u0627\u0644\u062e\u0637 \u063a\u064a\u0631 \u0645\u0631\u0626\u064a) \u0625\u0644\u0649 1 (\u0628\u062f\u0648\u0646 \u0634\u0641\u0627\u0641\u064a\u0629)\u060c label - \u0627\u0644\u062a\u0633\u0645\u064a\u0629 \u0627\u0644\u0646\u0635\u064a\u0629 \u0644\u0647\u0630\u0627 \u0627\u0644\u0631\u0633\u0645.</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">* __plt.scatter__ - \u064a\u0631\u0633\u0645 \u0627\u0644\u0631\u0633\u0645 \u0627\u0644\u0628\u064a\u0627\u0646\u064a \u062d\u0633\u0628 \u0627\u0644\u0646\u0642\u0627\u0637 \u062f\u0648\u0646 \u062a\u0648\u0635\u064a\u0644\u0647\u0627 \u0628\u062e\u0637\u0648\u0637. \u064a\u064f\u0645\u0631\u0631 \u0643\u0623\u0648\u0644 \u0648\u0633\u064a\u0637 \u0625\u062d\u062f\u0627\u062b\u064a\u0627\u062a x\u060c \u0648\u0643\u062b\u0627\u0646\u064a \u0648\u0633\u064a\u0637 \u0625\u062d\u062f\u0627\u062b\u064a\u0627\u062a y. \u0625\u0630\u0627 \u0644\u0645 \u064a\u062a\u0645 \u062a\u0645\u0631\u064a\u0631 \u0627\u0644\u0648\u0633\u064a\u0637 \u0627\u0644\u062b\u0627\u0646\u064a\u060c \u0633\u064a\u062a\u0645 \u0627\u0639\u062a\u0628\u0627\u0631 \u0625\u062d\u062f\u0627\u062b\u064a\u0627\u062a x \u0643\u0640 y\u060c \u0648\u0633\u064a\u062a\u0645 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0639\u062f\u062f \u0639\u0646\u0627\u0635\u0631 \u0627\u0644\u0645\u0635\u0641\u0648\u0641\u0629 \u0643\u0625\u062d\u062f\u0627\u062b\u064a\u0627\u062a x. \u0647\u0646\u0627\u0643 \u0628\u0639\u0636 \u0627\u0644\u0645\u0639\u0627\u0645\u0644\u0627\u062a \u0627\u0644\u0625\u0636\u0627\u0641\u064a\u0629 \u0627\u0644\u0645\u0641\u064a\u062f\u0629: s - \u062d\u062c\u0645 \u0627\u0644\u0646\u0642\u0627\u0637\u060c color - \u0644\u0648\u0646 \u0627\u0644\u0646\u0642\u0627\u0637 ('k' - \u0623\u0633\u0648\u062f\u060c 'r' - \u0623\u062d\u0645\u0631\u060c 'white' - \u0623\u0628\u064a\u0636\u060c \u0625\u0644\u062e)\u060c alpha - \u0634\u0641\u0627\u0641\u064a\u0629 \u0627\u0644\u0646\u0642\u0627\u0637\u060c \u0642\u064a\u0645\u0629 \u0645\u0646 0 (\u0627\u0644\u0646\u0642\u0637\u0629 \u063a\u064a\u0631 \u0645\u0631\u0626\u064a\u0629) \u0625\u0644\u0649 1 (\u0628\u062f\u0648\u0646 \u0634\u0641\u0627\u0641\u064a\u0629)\u060c label - \u0627\u0644\u062a\u0633\u0645\u064a\u0629 \u0627\u0644\u0646\u0635\u064a\u0629 \u0644\u0647\u0630\u0627 \u0627\u0644\u0631\u0633\u0645.</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">* __plt.contourf__ - \u064a\u0631\u0633\u0645 \u062e\u0637\u0648\u0637 \u0643\u0641\u0627\u0641 \u0645\u0645\u0644\u0648\u0621\u0629 \u0644\u062a\u062d\u062f\u064a\u062f \u0627\u0644\u062d\u062f\u0648\u062f \u0627\u0644\u0641\u0627\u0635\u0644\u0629.</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">* __plt.show__ - \u0639\u0631\u0636 \u0625\u062c\u0628\u0627\u0631\u064a \u0644\u0644\u0631\u0633\u0645 \u0627\u0644\u0628\u064a\u0627\u0646\u064a\u060c \u0648\u064a\u0645\u0643\u0646 \u0627\u0633\u062a\u062e\u062f\u0627\u0645\u0647 \u0644\u0639\u0631\u0636 \u0639\u062f\u0629 \u0631\u0633\u0648\u0645 \u0628\u064a\u0627\u0646\u064a\u0629 \u0641\u064a \u0646\u0641\u0633 \u0627\u0644\u0643\u0648\u062f.</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">* __plt.legend__ - \u064a\u0639\u0631\u0636 \u0627\u0644\u062a\u0633\u0645\u064a\u0627\u062a \u0627\u0644\u062a\u064a \u062a\u0645 \u062a\u062d\u062f\u064a\u062f\u0647\u0627 \u0645\u0633\u0628\u0642\u064b\u0627 \u0644\u0644\u0631\u0633\u0648\u0645 \u0627\u0644\u0628\u064a\u0627\u0646\u064a\u0629.</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">* __plt.xlim__ - \u064a\u062d\u062f\u0651\u062f \u0646\u0637\u0627\u0642 \u0625\u062d\u062f\u0627\u062b\u064a\u0627\u062a x \u0645\u0646 \u0627\u0644\u0648\u0633\u064a\u0637 \u0627\u0644\u0623\u0648\u0644 \u0625\u0644\u0649 \u0627\u0644\u0648\u0633\u064a\u0637 \u0627\u0644\u062b\u0627\u0646\u064a. \u0627\u0641\u062a\u0631\u0627\u0636\u064a\u064b\u0627\u060c \u064a\u062a\u0645 \u062a\u0639\u064a\u064a\u0646 \u0646\u0637\u0627\u0642 \u0627\u0644\u0645\u062d\u0648\u0631 \u0627\u0644\u0623\u0641\u0642\u064a \u062a\u0644\u0642\u0627\u0626\u064a\u064b\u0627 \u0628\u0646\u0627\u0621\u064b \u0639\u0644\u0649 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0627\u0644\u0645\u0633\u062a\u062e\u062f\u0645\u0629. \u064a\u064f\u0633\u062a\u062e\u062f\u0645 \u0647\u0630\u0627 \u0627\u0644\u0623\u0633\u0644\u0648\u0628 \u0644\u062a\u062d\u062f\u064a\u062f \u0646\u0637\u0627\u0642 \u0627\u0644\u0642\u064a\u0645 \u0639\u0644\u0649 \u0627\u0644\u0645\u062d\u0648\u0631 \u0627\u0644\u0623\u0641\u0642\u064a \u064a\u062f\u0648\u064a\u064b\u0627.</div>\n",
        "\n",
        "\n",
        "\n",
        "<div dir=\"rtl\">* __plt.ylim__ - \u0645\u0634\u0627\u0628\u0647\u0629 \u0644\u0640 __plt.xlim__\u060c \u0644\u0643\u0646\u0647\u0627 \u0644\u0644\u0645\u062d\u0648\u0631 \u0627\u0644\u0639\u0645\u0648\u062f\u064a.</div>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}